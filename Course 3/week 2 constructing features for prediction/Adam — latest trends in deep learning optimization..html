<!DOCTYPE html>
<!-- saved from url=(0092)https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Adam — latest trends in deep learning optimization._files/branch-latest.min.js"></script><script async="" src="./Adam — latest trends in deep learning optimization._files/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><script defer="" src="https://cdn.optimizely.com/js/16180790160.js"></script><title>Adam — latest trends in deep learning optimization.</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2018-10-24T05:37:00.020Z"><meta data-rh="true" name="title" content="Adam — latest trends in deep learning optimization."><meta data-rh="true" property="og:title" content="Adam — latest trends in deep learning optimization."><meta data-rh="true" property="twitter:title" content="Adam — latest trends in deep learning optimization."><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/6be9a291375c"><meta data-rh="true" property="al:android:url" content="medium://p/6be9a291375c"><meta data-rh="true" property="al:ios:url" content="medium://p/6be9a291375c"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Adam [1] is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. First published in 2014, Adam was presented at a very prestigious…"><meta data-rh="true" property="og:description" content="Adam is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. First…"><meta data-rh="true" property="twitter:description" content="Adam is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. First…"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://towardsdatascience.com/@bushaev"><meta data-rh="true" name="twitter:creator" content="@Vitalik36"><meta data-rh="true" name="author" content="Vitaly Bushaev"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="16 min read"><meta data-rh="true" name="parsely-post-id" content="6be9a291375c"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Adam — latest trends in deep learning optimization._files/m2.css"><link data-rh="true" rel="author" href="https://towardsdatascience.com/@bushaev"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/6be9a291375c"><link data-rh="true" rel="icon" href="https://miro.medium.com/fit/c/256/256/1*ChFMdf--f5jbm-AYv6VdYA@2x.png"><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*lAMbp2AXAVlmxN_hbsDi3g.jpeg"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fadam-latest-trends-in-deep-learning-optimization-6be9a291375c","dateCreated":"2018-10-22T13:51:43.250Z","datePublished":"2018-10-22T13:51:43.250Z","dateModified":"2018-10-24T05:37:00.020Z","headline":"Adam — latest trends in deep learning optimization.","name":"Adam — latest trends in deep learning optimization.","description":"Adam [1] is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. First published in 2014, Adam was presented at a very prestigious…","identifier":"6be9a291375c","keywords":["Lite:true","Tag:Machine Learning","Tag:Deep Learning","Tag:Optimization","Tag:Towards Data Science","Topic:Machine Learning","Topic:Data Science","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:3"],"author":{"@type":"Person","name":"Vitaly Bushaev","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@bushaev"},"creator":["Vitaly Bushaev"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F330\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fadam-latest-trends-in-deep-learning-optimization-6be9a291375c"}</script><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="https://cdn.optimizely.com/js/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="511" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:absolute}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ag{max-width:1192px}.ah{min-width:0}.ai{width:100%}.aj{height:65px}.am{flex:1 0 auto}.an{fill:rgba(0, 0, 0, 0.9)}.ao{visibility:hidden}.ap{margin-left:16px}.aq{display:none}.as{color:rgba(90, 118, 144, 1)}.at{fill:rgba(102, 138, 170, 1)}.au{font-size:inherit}.av{border:inherit}.aw{font-family:inherit}.ax{letter-spacing:inherit}.ay{font-weight:inherit}.az{padding:0}.ba{margin:0}.bb:hover{cursor:pointer}.bc:hover{color:rgba(84, 108, 131, 1)}.bd:hover{fill:rgba(90, 118, 144, 1)}.be:focus{outline:none}.bf:disabled{cursor:default}.bg:disabled{color:rgba(230, 57, 53, 0.5)}.bh:disabled{fill:rgba(230, 57, 53, 0.5)}.bi{flex:0 0 auto}.bj{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.bk{font-style:normal}.bl{line-height:20px}.bm{font-size:15.8px}.bn{letter-spacing:0px}.bo{color:rgba(0, 0, 0, 0.54)}.bp{fill:rgba(0, 0, 0, 0.54)}.bq{justify-content:flex-end}.br{margin-top:16px}.bs{margin-bottom:16px}.bt{display:inherit}.bu{max-width:210px}.bv{text-overflow:ellipsis}.bw{overflow:hidden}.bx{white-space:nowrap}.by{display:inline-block}.bz{border:none}.ca{outline:none}.cb{font:inherit}.cc{font-size:16px}.cd{opacity:0}.ce{position:relative}.cf{width:0px}.cg{transition:width 140ms ease-in}.ch{color:inherit}.ci{fill:inherit}.cj:hover{color:rgba(0, 0, 0, 0.9)}.ck:hover{fill:rgba(0, 0, 0, 0.9)}.cl:disabled{color:rgba(0, 0, 0, 0.54)}.cm:disabled{fill:rgba(0, 0, 0, 0.54)}.cn{margin-right:10px}.cr{margin-right:16px}.cs{margin:15px 0}.ct{padding:4px 12px}.cu{color:rgba(0, 0, 0, 0.84)}.cv{background:0}.cw{border-color:rgba(0, 0, 0, 0.54)}.cx:hover{color:rgba(0, 0, 0, 0.97)}.cy:hover{fill:rgba(0, 0, 0, 0.97)}.cz:hover{border-color:rgba(0, 0, 0, 0.84)}.da:disabled{fill:rgba(0, 0, 0, 0.76)}.db:disabled{border-color:rgba(0, 0, 0, 0.2)}.dc:disabled{cursor:inherit}.dd:disabled:hover{color:rgba(0, 0, 0, 0.54)}.de:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.df:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.dg{border-radius:4px}.dh{border-width:1px}.di{border-style:solid}.dj{box-sizing:border-box}.dk{text-decoration:none}.dl{padding-bottom:10px}.dm{padding-top:10px}.dn{border-radius:50%}.do{height:32px}.dp{width:32px}.dq{border-top:none}.dr{background-color:rgba(53, 88, 118, 1)}.ds{height:54px}.dt{margin-right:40px}.du{height:36px}.dv{width:100px}.dw{overflow:auto}.dx{flex:0 1 auto}.dy{list-style-type:none}.dz{line-height:40px}.ea{overflow-x:auto}.eb{align-items:flex-start}.ec{margin-top:20px}.ed{padding-top:20px}.ee{height:80px}.ef{height:20px}.eg{margin-right:15px}.eh{margin-left:15px}.ei:first-child{margin-left:0}.ej{min-width:1px}.ek{background-color:rgba(197, 210, 225, 1)}.el{font-weight:300}.em{font-size:15px}.en{color:rgba(197, 210, 225, 1)}.eo{text-transform:uppercase}.ep{letter-spacing:1px}.eq:hover{color:rgba(251, 255, 255, 1)}.er:hover{fill:rgba(233, 241, 250, 1)}.es:disabled{color:rgba(150, 171, 191, 1)}.et:disabled{fill:rgba(150, 171, 191, 1)}.eu{margin-bottom:0px}.ev{height:119px}.ey{padding-left:24px}.ez{padding-right:24px}.fa{margin-left:auto}.fb{margin-right:auto}.fc{max-width:728px}.fd{top:calc(100vh + 100px)}.fe{bottom:calc(100vh + 100px)}.ff{width:10px}.fg{pointer-events:none}.fh{word-break:break-word}.fi{word-wrap:break-word}.fj:after{display:block}.fk:after{content:""}.fl:after{clear:both}.fm{max-width:680px}.fn{max-width:2812px}.ft{clear:both}.fu{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.fv{cursor:zoom-in}.fw{z-index:auto}.fx{transition:opacity 100ms 400ms}.fy{height:100%}.fz{will-change:transform}.ga{transform:translateZ(0)}.gb{margin:auto}.gc{background-color:rgba(0, 0, 0, 0.05)}.gd{padding-bottom:75.03556187766713%}.ge{height:0}.gf{filter:blur(20px)}.gg{transform:scale(1.1)}.gh{visibility:visible}.gi{background:rgba(255, 255, 255, 1)}.gj{line-height:1.23}.gk{letter-spacing:0}.gl{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.gw{margin-bottom:-0.27em}.hc{margin-top:32px}.hd{justify-content:space-between}.hh{height:48px}.hi{width:48px}.hj{margin-left:12px}.hk{margin-bottom:2px}.hm{max-height:20px}.hn{display:-webkit-box}.ho{-webkit-line-clamp:1}.hp{-webkit-box-orient:vertical}.hq:hover{text-decoration:underline}.hr{margin-left:8px}.hs{padding:0px 8px}.ht{border-color:rgba(102, 138, 170, 1)}.hu:hover{border-color:rgba(90, 118, 144, 1)}.hv{line-height:18px}.hw{align-items:flex-end}.ie{padding-right:6px}.if{margin-right:8px}.ig{fill:rgba(0, 0, 0, 0.76)}.ih{margin-right:-6px}.ii{line-height:1.58}.ij{letter-spacing:-0.004em}.ik{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.iv{margin-bottom:-0.46em}.iw{letter-spacing:-0.003em}.jc{background-repeat:repeat-x}.jd{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.je{background-size:1px 1px}.jf{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.jg{line-height:1.12}.jh{letter-spacing:-0.022em}.ji{font-weight:600}.jr{margin-bottom:-0.28em}.jx{max-width:144px}.kd{padding-bottom:35.416666666666664%}.ke{margin-top:10px}.kf{text-align:center}.ki{max-width:443px}.kj{padding-bottom:31.376975169300223%}.kk{max-width:216px}.kl{padding-bottom:48.611111111111114%}.km{font-weight:700}.kn{max-width:1061px}.ko{padding-bottom:22.148916116870875%}.kp{max-width:385px}.kq{padding-bottom:34.02597402597403%}.kr{max-width:518px}.ks{padding-bottom:56.37065637065637%}.kt{list-style-type:decimal}.ku{margin-left:30px}.kv{padding-left:0px}.lb{max-width:195px}.lc{padding-bottom:127.17948717948718%}.ld{max-width:360px}.le{padding-bottom:35.55555555555556%}.lf{padding-bottom:NaN%}.lg{max-width:589px}.lh{padding-bottom:19.69439728353141%}.li{max-width:397px}.lj{padding-bottom:45.84382871536524%}.lk{max-width:800px}.ll{padding-bottom:31.749999999999996%}.lm{max-width:342px}.ln{padding-bottom:98.83040935672514%}.lo{max-width:662px}.lp{padding-bottom:31.419939577039276%}.lq{max-width:586px}.lr{padding-bottom:24.232081911262796%}.ls{max-width:692px}.lt{padding-bottom:12.138728323699423%}.lu{max-width:227px}.lv{padding-bottom:58.14977973568282%}.lw{max-width:539px}.lx{padding-bottom:25.04638218923933%}.ly{max-width:402px}.lz{padding-bottom:14.427860696517412%}.ma{max-width:362px}.mb{padding-bottom:21.546961325966848%}.mc{max-width:364px}.md{padding-bottom:23.626373626373624%}.me{max-width:725px}.mf{padding-bottom:38.62068965517241%}.mg{max-width:224px}.mh{padding-bottom:45.98214285714286%}.mi{will-change:opacity}.mj{position:fixed}.mk{width:188px}.ml{left:50%}.mm{transform:translateX(406px)}.mn{top:calc(65px + 54px + 14px)}.mq{top:calc(65px + 54px + 40px)}.ms{width:131px}.mt{flex-direction:column}.mu{padding-bottom:28px}.mv{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.mw{font-size:18px}.mx{padding-bottom:20px}.my{padding-top:2px}.mz{max-height:120px}.na{-webkit-line-clamp:6}.nb{padding-top:28px}.nc{margin-bottom:19px}.nd{margin-left:-3px}.nj{outline:0}.nk{border:0}.nl{user-select:none}.nm{cursor:pointer}.nn> svg{pointer-events:none}.no:active{border-style:none}.np{-webkit-user-select:none}.nq:focus{fill:rgba(0, 0, 0, 0.54)}.nr:hover{fill:rgba(0, 0, 0, 0.54)}.nz button{text-align:left}.oa{margin-top:40px}.ob{flex-wrap:wrap}.oc{margin-top:25px}.od{margin-bottom:8px}.oe{border-radius:3px}.of{padding:5px 10px}.og{background:rgba(0, 0, 0, 0.05)}.oh{line-height:22px}.oi{margin-top:15px}.oj{flex-direction:row}.ok{max-width:155px}.oq{border:1px solid rgba(0, 0, 0, 0.1)}.or{height:60px}.os{width:60px}.pf:hover{border-color:rgba(0, 0, 0, 0.54)}.pg:active{border-style:solid}.ph{z-index:2}.pj{top:1px}.pp{padding-right:8px}.pq{padding-top:32px}.pr{border-top:1px solid rgba(0, 0, 0, 0.1)}.ps{margin-bottom:25px}.pu{margin-bottom:32px}.pv{min-height:80px}.qa{width:80px}.qb{padding-left:102px}.qd{letter-spacing:0.05em}.qe{margin-bottom:6px}.qf{font-size:28px}.qg{line-height:36px}.qh{max-width:555px}.qi{max-width:450px}.qj{line-height:24px}.ql{max-width:550px}.qm{padding-top:24px}.qn{margin-top:5px}.qo{height:40px}.qp{width:40px}.qq{font-size:12px}.qr{line-height:15px}.qs{padding-top:8px}.qt{padding-top:25px}.qv{color:rgba(0, 0, 0, 0.76)}.qw{opacity:1}.qx{padding:20px}.qy{border:1px solid rgba(102, 138, 170, 1)}.qz{margin-top:64px}.ra{background-color:rgba(0, 0, 0, 0.02)}.rb{padding:60px 0}.rc{background-color:rgba(0, 0, 0, 0.9)}.rt{padding-bottom:48px}.ru{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.rv{margin:0 -12px}.rw{margin:0 12px}.rx{flex:1 1 0}.ry{padding-bottom:12px}.rz:hover{color:rgba(255, 255, 255, 0.99)}.sa:hover{fill:rgba(255, 255, 255, 0.99)}.sb:disabled{color:rgba(255, 255, 255, 0.7)}.sc:disabled{fill:rgba(255, 255, 255, 0.7)}.sd{color:rgba(255, 255, 255, 0.98)}.se{fill:rgba(255, 255, 255, 0.98)}.sf{text-align:inherit}.sg{font-size:21.6px}.sh{letter-spacing:-0.32px}.si{color:rgba(255, 255, 255, 0.7)}.sj{fill:rgba(255, 255, 255, 0.7)}.sk{text-decoration:underline}.sl{padding-bottom:8px}.sm{width:200px}.ss:disabled{color:rgba(3, 168, 124, 0.5)}.st:disabled{fill:rgba(3, 168, 124, 0.5)}.su{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.af{margin:0 64px}.fs{margin-top:32px}.gu{font-size:40px}.gv{margin-top:0.78em}.hb{line-height:48px}.id{margin-left:30px}.it{font-size:21px}.iu{margin-top:2em}.jb{line-height:32px}.jp{font-size:34px}.jq{margin-top:1.95em}.jw{margin-top:0.86em}.kc{margin-top:56px}.la{margin-top:1.05em}.ni{margin-right:5px}.ny{margin-top:5px}.op{margin-right:16px}.po{width:25px}.rq{padding-left:64px}.rr{padding-right:64px}.rs{max-width:1320px}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.ic{margin-left:30px}.kg{margin-left:auto}.kh{text-align:center}.nh{margin-right:5px}.nx{margin-top:5px}.oo{margin-right:16px}.pn{width:25px}.rn{padding-left:64px}.ro{padding-right:64px}.rp{max-width:1080px}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.cq{display:flex}.ib{margin-left:30px}.ng{margin-right:5px}.nw{margin-top:5px}.on{margin-right:16px}.pm{width:15px}.rk{padding-left:48px}.rl{padding-right:48px}.rm{max-width:904px}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.ak{height:56px}.al{display:flex}.ar{display:block}.co{margin-left:10px}.cp{margin-right:10px}.ew{margin-bottom:0px}.ex{height:110px}.hf{margin-top:32px}.hg{flex-direction:column-reverse}.hz{margin-bottom:30px}.ia{margin-left:0px}.nf{margin-left:8px}.nu{margin-top:2px}.nv{margin-right:8px}.om{margin-left:16px}.pl{width:15px}.pt{padding-top:0}.pw{margin-bottom:24px}.px{align-items:center}.py{width:102px}.pz{position:relative}.qc{padding-left:0}.qk{margin-top:24px}.qu{border-top:none}.rd{padding:32px 0}.rh{padding-left:24px}.ri{padding-right:24px}.rj{max-width:728px}.sn{width:140px}.so{margin-bottom:16px}.sp{margin-top:30px}.sq{width:100%}.sr{flex-direction:row}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.z{margin:0 24px}.fo{margin-top:24px}.gm{font-size:30px}.gn{margin-top:0.72em}.gx{line-height:40px}.he{margin-top:32px}.hl{margin-bottom:0px}.hx{margin-bottom:30px}.hy{margin-left:0px}.il{font-size:18px}.im{margin-top:1.56em}.ix{line-height:28px}.jj{margin-top:1.2em}.js{margin-top:0.67em}.jy{margin-top:40px}.kw{margin-top:1.34em}.ne{margin-left:8px}.ns{margin-top:2px}.nt{margin-right:8px}.ol{margin-left:16px}.pk{width:15px}.re{padding-left:24px}.rf{padding-right:24px}.rg{max-width:552px}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ae{margin:0 64px}.fr{margin-top:32px}.gs{font-size:40px}.gt{margin-top:0.78em}.ha{line-height:48px}.ir{font-size:21px}.is{margin-top:2em}.ja{line-height:32px}.jn{font-size:34px}.jo{margin-top:1.95em}.jv{margin-top:0.86em}.kb{margin-top:56px}.kz{margin-top:1.05em}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.ac{margin:0 48px}.fq{margin-top:32px}.gq{font-size:40px}.gr{margin-top:0.78em}.gz{line-height:48px}.ip{font-size:21px}.iq{margin-top:2em}.iz{line-height:32px}.jl{font-size:34px}.jm{margin-top:1.95em}.ju{margin-top:0.86em}.ka{margin-top:56px}.ky{margin-top:1.05em}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ab{margin:0 24px}.fp{margin-top:24px}.go{font-size:30px}.gp{margin-top:0.72em}.gy{line-height:40px}.in{font-size:18px}.io{margin-top:1.56em}.iy{line-height:28px}.jk{margin-top:1.2em}.jt{margin-top:0.67em}.jz{margin-top:40px}.kx{margin-top:1.34em}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="print">.y{display:none}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.mo{transition:opacity 200ms}.ot{transition:border-color 150ms ease}.ou::before{background:
      radial-gradient(circle, rgba(0, 0, 0, 0.84) 60%, transparent 70%)
    }.ov::before{border-radius:50%}.ow::before{content:""}.ox::before{display:block}.oy::before{z-index:0}.oz::before{left:0}.pa::before{height:100%}.pb::before{position:absolute}.pc::before{top:0}.pd::before{width:100%}.pe:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.pi{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (max-width: 1230px)">.mp{display:none}</style><style type="text/css" data-fela-rehydration="511" data-fela-type="RULE" media="all and (max-width: 1198px)">.mr{display:none}</style><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y"><div><div class="r c"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="aj n o ak al"><div class="n o am w"><a href="https://medium.com/?source=post_page-----6be9a291375c----------------------" aria-label="Homepage" rel="noopener"><svg width="35" height="35" viewBox="5 5 35 35" class="an"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a><div class="gh" id="li-general-navbar-open-in-app-button"><div class="ap aq ar"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6be9a291375c&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----6be9a291375c----------------------" class="as at au av aw ax ay az ba bb bc bd be bf bg bh" rel="noopener nofollow">Open in app</a></div></div></div><div class="r bi w"><span class="bj b bk bl bm bn r bo bp"><div class="n o bq"><div class="n f"><div class="by" aria-hidden="true"><div class="n"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="0 0 25 25" class="ap cn r co cp"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></button><input class="bz ca cb cc bl cd ce cf cg" placeholder="Search Towards Data Science" value=""></div></div></div><div class="aq cq"><a href="https://towardsdatascience.com/search?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="ap cr r co cp"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a></div><a href="https://medium.com/me/list/queue?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="cr r g"><path d="M16 6a2 2 0 0 1 2 2v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-5.67-4.13-5.66 4.13a.5.5 0 0 1-.7-.03.48.48 0 0 1-.13-.29H5V8c0-1.1.9-2 2-2h9zM6 8v12.64l5.16-3.67a.49.49 0 0 1 .68 0L17 20.64V8a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z"></path><path d="M21 5v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-.17-.12V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1H8c0-1.1.9-2 2-2h9a2 2 0 0 1 2 2z"></path></svg></a><div class="cr n cp"><div class="by" aria-hidden="true"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm r"><svg width="25" height="25" viewBox="-293 409 25 25" class="cs r"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></div></div><div class="gh" id="li-post-page-navbar-upsell-button"><div class="cr r g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="ct cu q cv cw cx cy cz bb cl da db dc dd de df dg bj b bk bl bm bn dh di dj by dk be" rel="noopener">Upgrade</a></div></div></div><div class="n" aria-hidden="true"><div class="dl dm n o"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><img alt="Ayushverma" class="r dn do dp" src="./Adam — latest trends in deep learning optimization._files/0_fSKu5zWydc5B9tfM" width="32" height="32"></button></div></div></div></span></div></div></div></div></div><div class="dq r dr ar"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="ds bw n o"><div class="dt r bi"><a href="https://towardsdatascience.com/?source=post_page-----6be9a291375c----------------------" rel="noopener"><div class="du dv r"><img alt="Towards Data Science" class="" src="./Adam — latest trends in deep learning optimization._files/1_mG6i4Bh_LgixUYXJgQpYsg@2x.png" width="100" height="36"></div></a></div><div class="dw r dx"><ul class="dy ba dz bx ea n eb g ec ed ee"><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/data-science/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">Data Science</a></span></li><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/machine-learning/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">Machine Learning</a></span></li><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/programming/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">Programming</a></span></li><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/data-visualization/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">Visualization</a></span></li><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/artificial-intelligence/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">AI</a></span></li><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/video/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">Video</a></span></li><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/about-us/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">About</a></span></li><span class="ef ej ek"></span><li class="n o ef eg eh ei"><span class="bj el em bl en eo ep"><a href="https://towardsdatascience.com/contribute/home?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb eq er be bf es et" rel="noopener">Contribute</a></span></li></ul></div></div></div></div></div></div></nav><div class="eu ev r ew ex"></div><article><section class="ey ez fa fb ai fc dj r"></section><span class="r"></span><div><div class="s u fd fe ff fg"></div><div class="fa fb fc ce"><div class="r h g f e"><aside class="sw s t" style="width: 244px;"><div class="sz ta s tb bx ai"><h4 class="bj el em bl bo"><span class="by ta bx bw bv">Top highlight</span></h4></div></aside></div></div><section class="fh fi fj fk fl"><div class="n p"><div class="z ab ac ae af fm ah ai"><figure class="fo fp fq fr fs ft fa fb paragraph-image"><div class="fu fv ce fw ai"><div class="fa fb fn"><div class="gb r ce gc"><div class="gd ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_lAMbp2AXAVlmxN_hbsDi3g.jpeg" width="2812" height="2110" role="presentation"></div><img class="qw sv s t u fy ai gi" width="2812" height="2110" srcset="https://miro.medium.com/max/552/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 276w, https://miro.medium.com/max/1104/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 552w, https://miro.medium.com/max/1280/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 640w, https://miro.medium.com/max/1400/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 700w" sizes="700px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_lAMbp2AXAVlmxN_hbsDi3g(1).jpeg"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/5624/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg" width="2812" height="2110" srcSet="https://miro.medium.com/max/552/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 276w, https://miro.medium.com/max/1104/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 552w, https://miro.medium.com/max/1280/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 640w, https://miro.medium.com/max/1400/1*lAMbp2AXAVlmxN_hbsDi3g.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><div><div id="b83a" class="gj gk cu bk gl b gm gn go gp gq gr gs gt gu gv gw"><h1 class="gl b gm gx go gy gq gz gs ha gu hb cu">Adam — latest trends in deep learning optimization.</h1></div><div class="hc"><div class="n hd he hf hg"><div class="o n"><div><a href="https://towardsdatascience.com/@bushaev?source=post_page-----6be9a291375c----------------------" rel="noopener"><img alt="Vitaly Bushaev" class="r dn hh hi" src="./Adam — latest trends in deep learning optimization._files/1_A7uzFalPvJ0f_0KKrQf34A.jpeg" width="48" height="48"></a></div><div class="hj ai r"><div class="n"><div style="flex:1"><span class="bj b bk bl bm bn r cu q"><div class="hk n o hl"><span class="bj el cc bl bw hm bv hn ho hp cu"><a href="https://towardsdatascience.com/@bushaev?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener">Vitaly Bushaev</a></span><div class="hr r bi h"><button class="hs cv as at ht bc bd hu bb dg bj b bk hv em bn dh di dj by dk be">Follow</button></div></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj el cc bl bw hm bv hn ho hp bo"><div><a class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener" href="https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c?source=post_page-----6be9a291375c----------------------">Oct 22, 2018</a> <!-- -->·<!-- --> <!-- -->16<!-- --> min read</div></span></span></div></div><div class="n hw hx hy hz ia ib ic id y"><div class="n o"><div class="ie r bi"><a href="https://medium.com/p/6be9a291375c/share/twitter?source=post_actions_header---------------------------" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="ie r bi"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="ie r bi"><a href="https://medium.com/p/6be9a291375c/share/facebook?source=post_actions_header---------------------------" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="if r"><div><div class="ig"><div><div class="by" role="tooltip" aria-hidden="true" aria-describedby="1" aria-labelledby="1"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="ih r am"><div class="by" aria-hidden="true"><div class="by" aria-hidden="true"><div class="r bi"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div></div></div></div></div><p id="1ea2" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Adam [1] is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. First published in 2014, Adam was presented at a very prestigious conference for deep learning practitioners —<a href="https://www.iclr.cc/archive/www/doku.php%3Fid=iclr2015:main.html" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow"> ICLR 2015</a>. The paper contained some very promising diagrams, showing huge performance gains in terms of speed of training. However, after a while people started noticing, that in some cases Adam actually finds worse solution than <a class="ch dk jc jd je jf" target="_blank" rel="noopener" href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">stochastic gradient descent</a>. A lot of research has been done to address the problems of Adam.</p><p id="f60b" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">The algorithms leverages the power of adaptive learning rates methods to find individual learning rates for each parameter. It also has advantages of Adagrad [10], which works really well in settings with sparse gradients, but struggles in non-convex optimization of neural networks, and RMSprop [11], which tackles to resolve some of the problems of Adagrad and works really well in on-line settings. Adam has been raising in popularity exponentially according to <a href="https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106" class="ch dk jc jd je jf" target="_blank" rel="noopener">‘A Peek at Trends in Machine Learning’</a> article from <a href="https://medium.com/@karpathy?source=post_header_lockup" class="ch dk jc jd je jf" target="_blank" rel="noopener">Andrej Karpathy</a>.</p><p id="b324" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">In this post, I first introduce Adam algorithm as presented in the original paper, and then walk through latest research around it that demonstrates some potential reasons why the algorithms works worse than classic SGD in some areas and provides several solutions, that narrow the gap between SGD and Adam.</p><h1 id="5f44" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">Adam</h1><p id="7e3b" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv fh" data-selectable-paragraph="">Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. Let’s take a closer look at how it works.</p><p id="2a82" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters. Its name is derived from adaptive <a href="https://en.wikipedia.org/wiki/Moment_(mathematics)" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">moment</a> estimation, and the reason it’s called that is because Adam uses estimations of first and second moments of gradient to adapt the learning rate for each weight of the neural network. Now, what is moment ? N-th moment of a random variable is defined as the <a href="https://en.wikipedia.org/wiki/Expected_value" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">expected value</a> of that variable to the power of n. More formally:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb jx"><div class="gb r ce gc"><div class="kd ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_NIM9yxfs4PciZ_A7w-n0-w.png" width="144" height="51" role="presentation"></div><img class="qw sv s t u fy ai gi" width="144" height="51" srcset="" sizes="144px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_NIM9yxfs4PciZ_A7w-n0-w(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/288/1*NIM9yxfs4PciZ_A7w-n0-w.png" width="144" height="51" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">m — moment, X -random variable.</figcaption></figure><p id="3abf" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">It can be pretty difficult to grasp that idea for the first time, so if you don’t understand it fully, you should still carry on, you’ll be able to understand how algorithms works anyway. Note, that gradient of the cost function of neural network can be considered a random variable, since it usually evaluated on some small random batch of data. <mark class="sx sy nm">The first moment is mean, and the second moment is uncentered variance (meaning we don’t subtract the mean during variance calculation).</mark> We will see later how we use these values, right now, we have to decide on how to get them. To estimates the moments, Adam utilizes exponentially moving averages, computed on the gradient evaluated on a current mini-batch:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb ki"><div class="gb r ce gc"><div class="kj ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_ZhGLUwaaqlJ9C0WK0nbAEA.png" width="443" height="139" role="presentation"></div><img class="qw sv s t u fy ai gi" width="443" height="139" srcset="https://miro.medium.com/max/552/1*ZhGLUwaaqlJ9C0WK0nbAEA.png 276w, https://miro.medium.com/max/886/1*ZhGLUwaaqlJ9C0WK0nbAEA.png 443w" sizes="443px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_ZhGLUwaaqlJ9C0WK0nbAEA(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/886/1*ZhGLUwaaqlJ9C0WK0nbAEA.png" width="443" height="139" srcSet="https://miro.medium.com/max/552/1*ZhGLUwaaqlJ9C0WK0nbAEA.png 276w, https://miro.medium.com/max/886/1*ZhGLUwaaqlJ9C0WK0nbAEA.png 443w" sizes="443px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">Moving averages of gradient and squared gradient.</figcaption></figure><p id="6b43" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Where m and v are moving averages, g is gradient on current mini-batch, and betas — new introduced hyper-parameters of the algorithm. They have really good default values of 0.9 and 0.999 respectively. Almost no one ever changes these values. The vectors of moving averages are initialized with zeros at the first iteration.</p><p id="94f2" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">To see how these values correlate with the moment, defined as in first equation, let’s take look at expected values of our moving averages. Since m and v are <a href="https://en.wikipedia.org/wiki/Estimator" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">estimates</a> of first and second moments, we want to have the following property:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb kk"><div class="gb r ce gc"><div class="kl ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_uoUrVBHcgpdfbB4stoNgYA.png" width="216" height="105" role="presentation"></div><img class="qw sv s t u fy ai gi" width="216" height="105" srcset="" sizes="216px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_uoUrVBHcgpdfbB4stoNgYA(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/432/1*uoUrVBHcgpdfbB4stoNgYA.png" width="216" height="105" role="presentation"/></noscript></div></div></div></figure><p id="a38a" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Expected values of the estimators should equal the parameter we’re trying to estimate, as it happens, the parameter in our case is also the expected value. If these properties held true, that would mean, that we have <strong class="ik km">unbiased estimators</strong>. (To learn more about statistical properties of different estimators, refer to Ian Goodfellow’s <a href="http://www.deeplearningbook.org/contents/ml.html" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Deep Learning book, Chapter 5</a> on machine learning basics). Now, we will see that these do not hold true for the our moving averages. Because we initialize averages with zeros, the estimators are biased towards zero. Let’s prove that for m (the proof for v would be analogous). To prove that we need to formula for m to the very first gradient. Let’s try to unroll a couple values of m to see he pattern we’re going to use:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fu fv ce fw ai"><div class="fa fb kn"><div class="gb r ce gc"><div class="ko ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_ztYpt5ppDEE7lW8LqxMShw.png" width="1061" height="235" role="presentation"></div><img class="qw sv s t u fy ai gi" width="1061" height="235" srcset="https://miro.medium.com/max/552/1*ztYpt5ppDEE7lW8LqxMShw.png 276w, https://miro.medium.com/max/1104/1*ztYpt5ppDEE7lW8LqxMShw.png 552w, https://miro.medium.com/max/1280/1*ztYpt5ppDEE7lW8LqxMShw.png 640w, https://miro.medium.com/max/1400/1*ztYpt5ppDEE7lW8LqxMShw.png 700w" sizes="700px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_ztYpt5ppDEE7lW8LqxMShw(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/2122/1*ztYpt5ppDEE7lW8LqxMShw.png" width="1061" height="235" srcSet="https://miro.medium.com/max/552/1*ztYpt5ppDEE7lW8LqxMShw.png 276w, https://miro.medium.com/max/1104/1*ztYpt5ppDEE7lW8LqxMShw.png 552w, https://miro.medium.com/max/1280/1*ztYpt5ppDEE7lW8LqxMShw.png 640w, https://miro.medium.com/max/1400/1*ztYpt5ppDEE7lW8LqxMShw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="3ccf" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">As you can see, the ‘further’ we go expanding the value of m, the less first values of gradients contribute to the overall value, as they get multiplied by smaller and smaller beta. Capturing this patter, we can rewrite the formula for our moving average:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb kp"><div class="gb r ce gc"><div class="kq ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_5Q1ZlbpP-C4wzJVtVfGn5Q.png" width="385" height="131" role="presentation"></div><img class="qw sv s t u fy ai gi" width="385" height="131" srcset="https://miro.medium.com/max/552/1*5Q1ZlbpP-C4wzJVtVfGn5Q.png 276w, https://miro.medium.com/max/770/1*5Q1ZlbpP-C4wzJVtVfGn5Q.png 385w" sizes="385px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_5Q1ZlbpP-C4wzJVtVfGn5Q(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/770/1*5Q1ZlbpP-C4wzJVtVfGn5Q.png" width="385" height="131" srcSet="https://miro.medium.com/max/552/1*5Q1ZlbpP-C4wzJVtVfGn5Q.png 276w, https://miro.medium.com/max/770/1*5Q1ZlbpP-C4wzJVtVfGn5Q.png 385w" sizes="385px" role="presentation"/></noscript></div></div></div></figure><p id="aaa5" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Now, let’s take a look at the expected value of m, to see how it relates to the true first moment, so we can correct for the discrepancy of the two :</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb kr"><div class="gb r ce gc"><div class="ks ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1__uSts1p9vJ5yat3iTHR5Ug.png" width="518" height="292" role="presentation"></div><img class="qw sv s t u fy ai gi" width="518" height="292" srcset="https://miro.medium.com/max/552/1*_uSts1p9vJ5yat3iTHR5Ug.png 276w, https://miro.medium.com/max/1036/1*_uSts1p9vJ5yat3iTHR5Ug.png 518w" sizes="518px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1__uSts1p9vJ5yat3iTHR5Ug(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1036/1*_uSts1p9vJ5yat3iTHR5Ug.png" width="518" height="292" srcSet="https://miro.medium.com/max/552/1*_uSts1p9vJ5yat3iTHR5Ug.png 276w, https://miro.medium.com/max/1036/1*_uSts1p9vJ5yat3iTHR5Ug.png 518w" sizes="518px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">Bias correction for the first momentum estimator</figcaption></figure><p id="f23c" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">In the first row, we use our new formula for moving average to expand m. Next, we approximate g[i] with g[t]. Now we can take it out of sum, since it does not now depend on i. Because the approximation is taking place, the error C emerge in the formula. In the last line we just use the formula for the sum of a finite geometric series. There are two things we should note from that equation.</p><ol class=""><li id="43a3" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv kt ku kv" data-selectable-paragraph="">We have biased estimator. This is not just true for Adam only, the same holds for algorithms, using moving averages (SGD with momentum, RMSprop, etc.).</li><li id="12fd" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">It won’t have much effect unless it’s the begging of the training, because the value beta to the power of t is quickly going towards zero.</li></ol><p id="9ffb" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Now we need to correct the estimator, so that the expected value is the one we want. This step is usually referred to as bias correction. The final formulas for our estimator will be as follows:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fu fv ce fw ai"><div class="fa fb lb"><div class="gb r ce gc"><div class="lc ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_M86IUMsrHXq4WrS-Bk5boA.png" width="195" height="248" role="presentation"></div><img class="qw sv s t u fy ai gi" width="195" height="248" srcset="" sizes="195px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_M86IUMsrHXq4WrS-Bk5boA(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/390/1*M86IUMsrHXq4WrS-Bk5boA.png" width="195" height="248" role="presentation"/></noscript></div></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">Bias corrected estimators for the first and second moments.</figcaption></figure><p id="c5ad" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">The only thing left to do is to use those moving averages to scale learning rate individually for each parameter. The way it’s done in Adam is very simple, to perform weight update we do the following:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb ld"><div class="gb r ce gc"><div class="le ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_WrfK1bWzKYEH-UdsBHYl5A.png" width="360" height="128" role="presentation"></div><img class="qw sv s t u fy ai gi" width="360" height="128" srcset="https://miro.medium.com/max/552/1*WrfK1bWzKYEH-UdsBHYl5A.png 276w, https://miro.medium.com/max/720/1*WrfK1bWzKYEH-UdsBHYl5A.png 360w" sizes="360px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_WrfK1bWzKYEH-UdsBHYl5A(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/720/1*WrfK1bWzKYEH-UdsBHYl5A.png" width="360" height="128" srcSet="https://miro.medium.com/max/552/1*WrfK1bWzKYEH-UdsBHYl5A.png 276w, https://miro.medium.com/max/720/1*WrfK1bWzKYEH-UdsBHYl5A.png 360w" sizes="360px" role="presentation"/></noscript></div></div></div></figure><p id="8c9c" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Where w is model weights, eta (look like the letter n) is the step size (it can depend on iteration). And that’s it, that’s the update rule for Adam. For some people it can be easier to understand such concepts in code, so here’s possible implementation of Adam in python:</p><figure class="jy jz ka kb kc ft"><div class="gb r ce"><div class="tf ge r"><iframe src="./Adam — latest trends in deep learning optimization._files/1989f43f8b10f499efc335c7d9205236.html" allowfullscreen="" frameborder="0" height="197" width="680" title="Adam.py" class="s t u fy ai" scrolling="auto"></iframe></div></div></figure><p id="e849" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">There are two small variations on Adam that I don’t see much in practice, but they’re implemented in major deep learning frameworks, so it’s worth to briefly mention them.</p><p id="4e2c" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">First one, called <strong class="ik km">Adamax</strong> was introduced by the authors of Adam in the same paper. The idea with Adamax is to look at the value v as the <a href="https://en.wikipedia.org/wiki/Euclidean_distance" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">L2 norm</a> of the individual current and past gradients. We can generalize it to Lp update rule, but it gets pretty unstable for large values of p. But if we use the special case of <a href="https://en.wikipedia.org/wiki/Uniform_norm" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">L-infinity norm</a>, it results in a surprisingly stable and well-performing algorithm. Here’s how to implement Adamax with python:</p><figure class="jy jz ka kb kc ft"><div class="gb r ce"><div class="tg ge r"><iframe src="./Adam — latest trends in deep learning optimization._files/3234ba4c134f5d83c166fffef6eac13c.html" allowfullscreen="" frameborder="0" height="175" width="680" title="Adamax.py" class="s t u fy ai" scrolling="auto"></iframe></div></div></figure><p id="6c04" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Second one is a bit harder to understand, called <strong class="ik km">Nadam </strong>[6].<strong class="ik km"> </strong>Nadam was published by Timothy Dozat in the paper ‘Incorporating Nesterov Momentum into Adam’. As name suggests the idea is to use Nesterov momentum term for the first moving averages. Let’s take a look at update rule of the SGD with momentum:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb lg"><div class="gb r ce gc"><div class="lh ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_ZcprfGi_ppxR5ddYP5LxzQ.png" width="589" height="116" role="presentation"></div><img class="qw sv s t u fy ai gi" width="589" height="116" srcset="https://miro.medium.com/max/552/1*ZcprfGi_ppxR5ddYP5LxzQ.png 276w, https://miro.medium.com/max/1104/1*ZcprfGi_ppxR5ddYP5LxzQ.png 552w, https://miro.medium.com/max/1178/1*ZcprfGi_ppxR5ddYP5LxzQ.png 589w" sizes="589px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_ZcprfGi_ppxR5ddYP5LxzQ(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1178/1*ZcprfGi_ppxR5ddYP5LxzQ.png" width="589" height="116" srcSet="https://miro.medium.com/max/552/1*ZcprfGi_ppxR5ddYP5LxzQ.png 276w, https://miro.medium.com/max/1104/1*ZcprfGi_ppxR5ddYP5LxzQ.png 552w, https://miro.medium.com/max/1178/1*ZcprfGi_ppxR5ddYP5LxzQ.png 589w" sizes="589px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">SGD with momentum update rule</figcaption></figure><p id="4706" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">As shown above, the update rule is equivalent to taking a step in the direction of momentum vector and then taking a step in the direction of gradient. However, the momentum step doesn’t depend on the current gradient , so we can get a higher-quality gradient step direction by updating the parameters with the momentum step before computing the gradient. To achieve that, we modify the update as follows:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb li"><div class="gb r ce gc"><div class="lj ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_a0NNGI-iQ3OtQAUUtCZgIQ.png" width="397" height="182" role="presentation"></div><img class="qw sv s t u fy ai gi" width="397" height="182" srcset="https://miro.medium.com/max/552/1*a0NNGI-iQ3OtQAUUtCZgIQ.png 276w, https://miro.medium.com/max/794/1*a0NNGI-iQ3OtQAUUtCZgIQ.png 397w" sizes="397px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_a0NNGI-iQ3OtQAUUtCZgIQ(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/794/1*a0NNGI-iQ3OtQAUUtCZgIQ.png" width="397" height="182" srcSet="https://miro.medium.com/max/552/1*a0NNGI-iQ3OtQAUUtCZgIQ.png 276w, https://miro.medium.com/max/794/1*a0NNGI-iQ3OtQAUUtCZgIQ.png 397w" sizes="397px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">f — loss function to optimize.</figcaption></figure><p id="1801" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">So, with Nesterov accelerated momentum we first make make a big jump in the direction of the previous accumulated gradient and then measure the gradient where we ended up to make a correction. There’s a great visualization from <a href="http://cs231n.github.io/neural-networks-3/" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">cs231n lecture notes</a>:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fu fv ce fw ai"><div class="fa fb lk"><div class="gb r ce gc"><div class="ll ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_hJSLxZMjYVzgF5A_MoqeVQ.jpeg" width="800" height="254" role="presentation"></div><img class="qw sv s t u fy ai gi" width="800" height="254" srcset="https://miro.medium.com/max/552/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 276w, https://miro.medium.com/max/1104/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 552w, https://miro.medium.com/max/1280/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 640w, https://miro.medium.com/max/1400/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 700w" sizes="700px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_hJSLxZMjYVzgF5A_MoqeVQ(1).jpeg"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1600/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg" width="800" height="254" srcSet="https://miro.medium.com/max/552/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 276w, https://miro.medium.com/max/1104/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 552w, https://miro.medium.com/max/1280/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 640w, https://miro.medium.com/max/1400/1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">sourec: <a href="http://cs231n.github.io/neural-networks-3/" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">cs231n lecture notes</a>.</figcaption></figure><p id="24b0" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">The same method can be incorporated into Adam, by changing the first moving average to a Nesterov accelerated momentum. One computation trick can be applied here: instead of updating the parameters to make momentum step and changing back again, we can achieve the same effect by applying the momentum step of time step t + 1 only once, during the update of the previous time step t instead of t + 1. Using this trick, the implementation of Nadam may look like this:</p><figure class="jy jz ka kb kc ft"><div class="gb r ce"><div class="tf ge r"><iframe src="./Adam — latest trends in deep learning optimization._files/42e8a46b93cc303215116bc651209a7f.html" allowfullscreen="" frameborder="0" height="197" width="680" title="Nadam.py" class="s t u fy ai" scrolling="auto"></iframe></div></div></figure><h1 id="bdd1" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">Properties of Adam</h1><p id="0912" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv fh" data-selectable-paragraph="">Here I list some of the properties of Adam, for proof that these are true refer to the paper.</p><ol class=""><li id="09fe" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv kt ku kv" data-selectable-paragraph="">Actual step size taken by the Adam in each iteration is approximately bounded the step size hyper-parameter. This property add intuitive understanding to previous unintuitive learning rate hyper-parameter.</li><li id="6b14" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">Step size of Adam update rule is invariant to the magnitude of the gradient, which helps a lot when going through areas with tiny gradients (such as saddle points or ravines). In these areas SGD struggles to quickly navigate through them.</li><li id="fc99" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">Adam was designed to combine the advantages of Adagrad, which works well with sparse gradients, and RMSprop, which works well in on-line settings. Having both of these enables us to use Adam for broader range of tasks. Adam can also be looked at as the combination of RMSprop and SGD with momentum.</li></ol><h1 id="9187" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">Problems with Adam</h1><p id="3409" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv fh" data-selectable-paragraph="">When Adam was first introduced, people got very excited about its power. Paper contained some very optimistic charts, showing huge performance gains in terms of speed of training:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb lm"><div class="gb r ce gc"><div class="ln ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_X9gB3l_Wh5owNPCUsaYQVQ.png" width="342" height="338" role="presentation"></div><img class="qw sv s t u fy ai gi" width="342" height="338" srcset="https://miro.medium.com/max/552/1*X9gB3l_Wh5owNPCUsaYQVQ.png 276w, https://miro.medium.com/max/684/1*X9gB3l_Wh5owNPCUsaYQVQ.png 342w" sizes="342px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_X9gB3l_Wh5owNPCUsaYQVQ(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/684/1*X9gB3l_Wh5owNPCUsaYQVQ.png" width="342" height="338" srcSet="https://miro.medium.com/max/552/1*X9gB3l_Wh5owNPCUsaYQVQ.png 276w, https://miro.medium.com/max/684/1*X9gB3l_Wh5owNPCUsaYQVQ.png 342w" sizes="342px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">source: original Adam paper</figcaption></figure><p id="d3dd" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Then, Nadam paper presented diagrams that showed even better results:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb lo"><div class="gb r ce gc"><div class="lp ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_HgfHvTeQpx_hIHfhiDmGXg.png" width="662" height="208" role="presentation"></div><img class="qw sv s t u fy ai gi" width="662" height="208" srcset="https://miro.medium.com/max/552/1*HgfHvTeQpx_hIHfhiDmGXg.png 276w, https://miro.medium.com/max/1104/1*HgfHvTeQpx_hIHfhiDmGXg.png 552w, https://miro.medium.com/max/1280/1*HgfHvTeQpx_hIHfhiDmGXg.png 640w, https://miro.medium.com/max/1324/1*HgfHvTeQpx_hIHfhiDmGXg.png 662w" sizes="662px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_HgfHvTeQpx_hIHfhiDmGXg(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1324/1*HgfHvTeQpx_hIHfhiDmGXg.png" width="662" height="208" srcSet="https://miro.medium.com/max/552/1*HgfHvTeQpx_hIHfhiDmGXg.png 276w, https://miro.medium.com/max/1104/1*HgfHvTeQpx_hIHfhiDmGXg.png 552w, https://miro.medium.com/max/1280/1*HgfHvTeQpx_hIHfhiDmGXg.png 640w, https://miro.medium.com/max/1324/1*HgfHvTeQpx_hIHfhiDmGXg.png 662w" sizes="662px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">source: Nadam paper</figcaption></figure><p id="4c52" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">However, after a while people started noticing that despite superior training time, Adam in some areas does not converge to an optimal solution, so for some tasks (such as image classification on popular <a href="https://www.cs.toronto.edu/~kriz/cifar.html" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">CIFAR datasets</a>) state-of-the-art results are still only achieved by applying SGD with momentum. More than that Wilson et. al [9] showed in their paper ‘The marginal value of adaptive gradient methods in machine learning’ that adaptive methods (such as Adam or <a href="https://arxiv.org/abs/1212.5701" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Adadelta</a>) do not generalize as well as SGD with momentum when tested on a diverse set of deep learning tasks, discouraging people to use popular optimization algorithms. A lot of research has been done since to analyze the poor generalization of Adam trying to get it to close the gap with SGD.</p><p id="a6c8" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Nitish Shirish Keskar and Richard Socher in their paper ‘Improving Generalization Performance by Switching from Adam to SGD’ [5] also showed that by switching to SGD during training training they’ve been able to obtain better generalization power than when using Adam alone. They proposed a simple fix which uses a very simple idea. They’ve noticed that in earlier stages of training Adam still outperforms SGD but later the learning saturates. They proposed simple strategy which they called <strong class="ik km">SWATS </strong>in which they start training deep neural network with Adam but then switch to SGD when certain criteria hits. They managed to achieve results comparable to SGD with momentum.</p><h1 id="a54f" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">On the convergence of Adam</h1><p id="317f" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv fh" data-selectable-paragraph="">One big thing with figuring out what’s wrong with Adam was analyzing it’s convergence. The authors proved that Adam converges to the global minimum in the convex settings in their original paper, however, several papers later found out that their proof contained a few mistakes. Block et. al [7] claimed that they have spotted errors in the original convergence analysis, but still proved that the algorithm converges and provided proof in their paper. Another recent article from Google employees was presented at <a href="https://iclr.cc/Conferences/2018/Schedule?type=Poster" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">ICLR 2018</a> and even won best paper award. To go deeper to their paper I should first describe the framework used by Adam authors for proving that it converges for convex functions.</p><p id="0077" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">In 2003 Martin Zinkevich introduced Online Convex Programming problem [8]. In the presented settings, we have a sequence of convex functions c1, c2, etc (Loss function executed in ith mini-batch in the case of deep learning optimization). The algorithm, that solves the problem (Adam) in each timestamp t chooses a point x[t] (parameters of the model) and then receives the loss function c for the current timestamp. This setting translates to a lot of real world problems, for examples read the introduction of the paper. For understanding how good the algorithm works, the value of regret of the algorithm after T rounds is defined as follows:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb lq"><div class="gb r ce gc"><div class="lr ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_L6RkC6aQ0LTqJt81TF_XMw.png" width="586" height="142" role="presentation"></div><img class="qw sv s t u fy ai gi" width="586" height="142" srcset="https://miro.medium.com/max/552/1*L6RkC6aQ0LTqJt81TF_XMw.png 276w, https://miro.medium.com/max/1104/1*L6RkC6aQ0LTqJt81TF_XMw.png 552w, https://miro.medium.com/max/1172/1*L6RkC6aQ0LTqJt81TF_XMw.png 586w" sizes="586px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_L6RkC6aQ0LTqJt81TF_XMw(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1172/1*L6RkC6aQ0LTqJt81TF_XMw.png" width="586" height="142" srcSet="https://miro.medium.com/max/552/1*L6RkC6aQ0LTqJt81TF_XMw.png 276w, https://miro.medium.com/max/1104/1*L6RkC6aQ0LTqJt81TF_XMw.png 552w, https://miro.medium.com/max/1172/1*L6RkC6aQ0LTqJt81TF_XMw.png 586w" sizes="586px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">Regret of the algorithm in the online convex programming</figcaption></figure><p id="6660" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">where R is regret, c is the loss function on tth mini batch, w is vector of model parameters (weights), and w star is optimal value of weight vector. Our goal is to prove that the regret of algorithm is R(T) = O(T) or less, which means that on average the model converges to an optimal solution. Martin Zinkevich in his paper proved that gradient descent converges to optimal solutions in this setting, using the property of the convex functions:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb ls"><div class="gb r ce gc"><div class="lt ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_BAdCFeu94aDnu7K6FNhwLA.png" width="692" height="84" role="presentation"></div><img class="qw sv s t u fy ai gi" width="692" height="84" srcset="https://miro.medium.com/max/552/1*BAdCFeu94aDnu7K6FNhwLA.png 276w, https://miro.medium.com/max/1104/1*BAdCFeu94aDnu7K6FNhwLA.png 552w, https://miro.medium.com/max/1280/1*BAdCFeu94aDnu7K6FNhwLA.png 640w, https://miro.medium.com/max/1384/1*BAdCFeu94aDnu7K6FNhwLA.png 692w" sizes="692px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_BAdCFeu94aDnu7K6FNhwLA(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1384/1*BAdCFeu94aDnu7K6FNhwLA.png" width="692" height="84" srcSet="https://miro.medium.com/max/552/1*BAdCFeu94aDnu7K6FNhwLA.png 276w, https://miro.medium.com/max/1104/1*BAdCFeu94aDnu7K6FNhwLA.png 552w, https://miro.medium.com/max/1280/1*BAdCFeu94aDnu7K6FNhwLA.png 640w, https://miro.medium.com/max/1384/1*BAdCFeu94aDnu7K6FNhwLA.png 692w" sizes="692px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">Well-known property of convex functions.</figcaption></figure><p id="2a83" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">The same approach and framework used Adam authors to prove that their algorithm converges to an optimal solutions. Reddi et al. [3] spotted several mistakes in their proof, the main one lying in the value, which appears in both Adam and Improving Adam’s proof of convergence papers:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb lu"><div class="gb r ce gc"><div class="lv ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_jyxdh8fuHjuN3Df5BNxknQ.png" width="227" height="132" role="presentation"></div><img class="qw sv s t u fy ai gi" width="227" height="132" srcset="" sizes="227px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_jyxdh8fuHjuN3Df5BNxknQ(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/454/1*jyxdh8fuHjuN3Df5BNxknQ.png" width="227" height="132" role="presentation"/></noscript></div></div></div></figure><p id="022d" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Where V is defined as an abstract function that scales learning rate for parameters which differs for each individual algorithms. For Adam it’s the moving averages of past squared gradients, for Adagrad it’s the sum of all past and current gradients, for SGD it’s just 1. The authors found that in order for proof to work, this value has to be positive. It’s easy to see, that for SGD and Adagrad it’s always positive, however, for Adam(or RMSprop), the value of V can act unexpectedly. They also presented an example in which Adam fails to converge:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb lw"><div class="gb r ce gc"><div class="lx ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_0LekjJcPuqlig4h0DWd2Ng.png" width="539" height="135" role="presentation"></div><img class="qw sv s t u fy ai gi" width="539" height="135" srcset="https://miro.medium.com/max/552/1*0LekjJcPuqlig4h0DWd2Ng.png 276w, https://miro.medium.com/max/1078/1*0LekjJcPuqlig4h0DWd2Ng.png 539w" sizes="539px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_0LekjJcPuqlig4h0DWd2Ng(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1078/1*0LekjJcPuqlig4h0DWd2Ng.png" width="539" height="135" srcSet="https://miro.medium.com/max/552/1*0LekjJcPuqlig4h0DWd2Ng.png 276w, https://miro.medium.com/max/1078/1*0LekjJcPuqlig4h0DWd2Ng.png 539w" sizes="539px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">Adam fails on this sequence</figcaption></figure><p id="b25d" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">For this sequence, it’s easy to see that the optimal solution is x = -1, however, how authors show, Adam converges to highly sub-optimal value of x = 1. The algorithm obtains the large gradient C once every 3 steps, and while the other 2 steps it observes the gradient -1 , which moves the algorithm in the wrong direction. Since values of step size are often decreasing over time, they proposed a fix of keeping the maximum of values V and use it instead of the moving average to update parameters. The resulting algorithm is called <strong class="ik km">Amsgrad. </strong>We can confirm their experiment with <a href="https://github.com/bushaev/adam/blob/master/AdamNonConvergence.ipynb" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">this short notebook I created</a>, which shows different algorithms converge on the function sequence defined above.</p><figure class="jy jz ka kb kc ft"><div class="gb r ce"><div class="tg ge r"><iframe src="./Adam — latest trends in deep learning optimization._files/7a4d9c819ce0ae3edfb8ac247a418ac8.html" allowfullscreen="" frameborder="0" height="175" width="680" title="Amsgrad.py" class="s t u fy ai" scrolling="auto"></iframe></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo">Amsgrad without bias correction</figcaption></figure><p id="e710" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">How much does it help in practice with real-world data ? Sadly, I haven’t seen one case where it would help get better results than Adam. Filip Korzeniowski in <a href="https://fdlm.github.io/post/amsgrad/" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">his post</a> describes experiments with Amsgrad, which show similar results to Adam. Sylvain Gugger and Jeremy Howard <a href="http://www.fast.ai/2018/07/02/adam-weight-decay/" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">in their post</a> show that in their experiments Amsgrad actually performs even worse that Adam. Some reviewers of the paper also pointed out that the issue may lie not in Adam itself but in framework, which I described above, for convergence analysis, which does not allow for much hyper-parameter tuning.</p><h1 id="0884" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">Weight decay with Adam</h1><p id="20ce" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv fh" data-selectable-paragraph="">One paper that actually turned out to help Adam is ‘Fixing Weight Decay Regularization in Adam’ [4] by Ilya Loshchilov and Frank Hutter. This paper contains a lot of contributions and insights into Adam and weight decay. First, they show that despite common belief <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">L2 regularization</a> is not the same as <a href="https://papers.nips.cc/paper/156-comparing-biases-for-minimal-network-construction-with-back-propagation.pdf" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">weight decay</a>, though it is equivalent for stochastic gradient descent. The way weight decay was introduced back in 1988 is:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fu fv ce fw ai"><div class="fa fb ly"><div class="gb r ce gc"><div class="lz ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_FA9xcKWO_htmF8rFNadgQQ.png" width="402" height="58" role="presentation"></div><img class="qw sv s t u fy ai gi" width="402" height="58" srcset="https://miro.medium.com/max/552/1*FA9xcKWO_htmF8rFNadgQQ.png 276w, https://miro.medium.com/max/804/1*FA9xcKWO_htmF8rFNadgQQ.png 402w" sizes="402px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_FA9xcKWO_htmF8rFNadgQQ(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/804/1*FA9xcKWO_htmF8rFNadgQQ.png" width="402" height="58" srcSet="https://miro.medium.com/max/552/1*FA9xcKWO_htmF8rFNadgQQ.png 276w, https://miro.medium.com/max/804/1*FA9xcKWO_htmF8rFNadgQQ.png 402w" sizes="402px" role="presentation"/></noscript></div></div></div></div></figure><p id="5596" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Where lambda is weight decay hyper parameter to tune. I changed notation a little bit to stay consistent with the rest of the post. As defined above, weight decay is applied in the last step, when making the weight update, penalizing large weights. The way it’s been traditionally implemented for SGD is through L2 regularization in which we modify the cost function to contain the L2 norm of the weight vector:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb ma"><div class="gb r ce gc"><div class="mb ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_p7eol4iX8TlWapsR6dk5SA.png" width="362" height="78" role="presentation"></div><img class="qw sv s t u fy ai gi" width="362" height="78" srcset="https://miro.medium.com/max/552/1*p7eol4iX8TlWapsR6dk5SA.png 276w, https://miro.medium.com/max/724/1*p7eol4iX8TlWapsR6dk5SA.png 362w" sizes="362px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_p7eol4iX8TlWapsR6dk5SA(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/724/1*p7eol4iX8TlWapsR6dk5SA.png" width="362" height="78" srcSet="https://miro.medium.com/max/552/1*p7eol4iX8TlWapsR6dk5SA.png 276w, https://miro.medium.com/max/724/1*p7eol4iX8TlWapsR6dk5SA.png 362w" sizes="362px" role="presentation"/></noscript></div></div></div></figure><p id="a0cb" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Historically, stochastic gradient descent methods inherited this way of implementing the weight decay regularization and so did Adam. However, L2 regularization is not equivalent to weight decay for Adam. When using L2 regularization the penalty we use for large weights gets scaled by moving average of the past and current squared gradients and therefore weights with large typical gradient magnitude are regularized by a smaller relative amount than other weights. In contrast, weight decay regularizes all weights by the same factor. To use weight decay with Adam we need to modify the update rule as follows:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb mc"><div class="gb r ce gc"><div class="md ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_NB9-GHeP0-pMBjSw8o_GDA.png" width="364" height="86" role="presentation"></div><img class="qw sv s t u fy ai gi" width="364" height="86" srcset="https://miro.medium.com/max/552/1*NB9-GHeP0-pMBjSw8o_GDA.png 276w, https://miro.medium.com/max/728/1*NB9-GHeP0-pMBjSw8o_GDA.png 364w" sizes="364px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_NB9-GHeP0-pMBjSw8o_GDA(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/728/1*NB9-GHeP0-pMBjSw8o_GDA.png" width="364" height="86" srcSet="https://miro.medium.com/max/552/1*NB9-GHeP0-pMBjSw8o_GDA.png 276w, https://miro.medium.com/max/728/1*NB9-GHeP0-pMBjSw8o_GDA.png 364w" sizes="364px" role="presentation"/></noscript></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">Adam update rule with weight decay</figcaption></figure><p id="d658" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Having show that these types of regularization differ for Adam, authors continue to show how well it works with both of them. The difference in results is shown very well with the diagram from the paper:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fu fv ce fw ai"><div class="fa fb me"><div class="gb r ce gc"><div class="mf ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_DsrkV06bd5UO-H_xySyfIQ.png" width="725" height="280" role="presentation"></div><img class="qw sv s t u fy ai gi" width="725" height="280" srcset="https://miro.medium.com/max/552/1*DsrkV06bd5UO-H_xySyfIQ.png 276w, https://miro.medium.com/max/1104/1*DsrkV06bd5UO-H_xySyfIQ.png 552w, https://miro.medium.com/max/1280/1*DsrkV06bd5UO-H_xySyfIQ.png 640w, https://miro.medium.com/max/1400/1*DsrkV06bd5UO-H_xySyfIQ.png 700w" sizes="700px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_DsrkV06bd5UO-H_xySyfIQ(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/1450/1*DsrkV06bd5UO-H_xySyfIQ.png" width="725" height="280" srcSet="https://miro.medium.com/max/552/1*DsrkV06bd5UO-H_xySyfIQ.png 276w, https://miro.medium.com/max/1104/1*DsrkV06bd5UO-H_xySyfIQ.png 552w, https://miro.medium.com/max/1280/1*DsrkV06bd5UO-H_xySyfIQ.png 640w, https://miro.medium.com/max/1400/1*DsrkV06bd5UO-H_xySyfIQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ke kf fc fa fb kg kh bj el em bl bo" data-selectable-paragraph="">The Top-1 test error of ResNet on CIFAR-10 measured after 100 epochs</figcaption></figure><p id="23a1" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">These diagrams show relation between learning rate and regularization method. The color represent high low the test error is for this pair of hyper parameters. As we can see above not only Adam with weight decay gets much lower test error it actually helps in decoupling learning rate and regularization hyper-parameter. On the left picture we can the that if we change of the parameters, say learning rate, then in order to achieve optimal point again we’d need to change L2 factor as well, showing that these two parameters are interdependent. This dependency contributes to the fact hyper-parameter tuning is a very difficult task sometimes. On the right picture we can see that as long as we stay in some range of optimal values for one the parameter, we can change another one independently.</p><p id="c696" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">Another contribution by the author of the paper shows that optimal value to use for weight decay actually depends on number of iteration during training. To deal with this fact they proposed a simple adaptive formula for setting weight decay:</p><figure class="jy jz ka kb kc ft fa fb paragraph-image"><div class="fa fb mg"><div class="gb r ce gc"><div class="mh ge r"><div class="cd fx s t u fy ai bw fz ga"><img class="s t u fy ai gf gg ao tc" src="./Adam — latest trends in deep learning optimization._files/1_dHuVxZsbg21AXIWAKOtmbw.png" width="224" height="103" role="presentation"></div><img class="qw sv s t u fy ai gi" width="224" height="103" srcset="" sizes="224px" role="presentation" src="./Adam — latest trends in deep learning optimization._files/1_dHuVxZsbg21AXIWAKOtmbw(1).png"><noscript><img class="s t u fy ai" src="https://miro.medium.com/max/448/1*dHuVxZsbg21AXIWAKOtmbw.png" width="224" height="103" role="presentation"/></noscript></div></div></div></figure><p id="35ca" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">where b is batch size, B is the total number of training points per epoch and T is the total number of epochs. This replaces the lambda hyper-parameter lambda by the new one lambda normalized.</p><p id="bd39" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">The authors didn’t even stop there, after fixing weight decay they tried to apply the learning rate schedule with <a href="https://arxiv.org/abs/1608.03983" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">warm restarts</a> with new version of Adam. Warm restarts helped a great deal for stochastic gradient descent, I talk more about it in my post ‘<a href="https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Improving the way we work with learning rate</a>’. But previously Adam was a lot behind SGD. With new weight decay Adam got much better results with restarts, but it’s still not as good as SGDR.</p><h1 id="8204" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">ND-Adam</h1><p id="3ce8" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv fh" data-selectable-paragraph="">One more attempt at fixing Adam, that I haven’t seen much in practice is proposed by Zhang et. al in their paper ‘Normalized Direction-preserving Adam’ [2]. The paper notices two problems with Adam that may cause worse generalization:</p><ol class=""><li id="79a3" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv kt ku kv" data-selectable-paragraph="">The updates of SGD lie in the span of historical gradients, whereas it is not the case for Adam. This difference has also been observed in already mentioned paper [9].</li><li id="b41f" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">Second, while the magnitudes of Adam parameter updates are invariant to descaling of the gradient, the effect of the updates on the same overall network function still varies with the magnitudes of parameters.</li></ol><p id="6fcd" class="ii iw cu bk ik b il im ix in io iy ip iq iz ir is ja it iu jb iv fh" data-selectable-paragraph="">To address these problems the authors propose the algorithm they call Normalized direction-preserving Adam. The algorithms tweaks Adam in the following ways. First, instead of estimating the average gradient magnitude for each individual parameter, it estimates the average squared L2 norm of the gradient vector. Since now V is a scalar value and M is the vector in the same direction as W, the direction of the update is the negative direction of m and thus is in the span of the historical gradients of w. For the second the algorithms before using gradient projects it onto the unit sphere and then after the update, the weights get normalized by their norm. For more details follow their paper.</p><h1 id="4cf5" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">Conclusion</h1><p id="be8d" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv fh" data-selectable-paragraph="">Adam is definitely one of the best optimization algorithms for deep learning and its popularity is growing very fast. While people have noticed some problems with using Adam in certain areas, researches continue to work on solutions to bring Adam results to be on par with SGD with momentum.</p><h1 id="a0fa" class="jg jh cu bk bj ji gm jj go jk jl jm jn jo jp jq jr" data-selectable-paragraph="">References</h1><ol class=""><li id="3d88" class="ii iw cu bk ik b il js ix in jt iy ip ju iz ir jv ja it jw jb iv kt ku kv" data-selectable-paragraph="">Diederik P. Kingma and Jimmy Lei Ba. <a href="https://arxiv.org/abs/1412.6980" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Adam : A method for stochastic optimization</a>. 2014. arXiv:1412.6980v9</li><li id="1385" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">Zijun Zhang et al. <a href="https://arxiv.org/pdf/1709.04546.pdf" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Normalized direction-preserving Adam</a>. 2017. arXiv:1709.04546v2</li><li id="b2b7" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph=""><a href="https://openreview.net/profile?email=sashank%40google.com" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Sashank J. Reddi</a>, <a href="https://openreview.net/profile?email=satyenkale%40google.com" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Satyen Kale</a>, <a href="https://openreview.net/profile?email=sanjivk%40google.com" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Sanjiv Kumar</a>. <a href="https://openreview.net/forum?id=ryQu7f-RZ" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">On the Convergence of Adam and Beyond</a>. 2018.</li><li id="b17e" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph=""><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Loshchilov%2C+I" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Ilya Loshchilov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hutter%2C+F" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Frank Hutter</a>. <a href="https://arxiv.org/abs/1711.05101" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Fixing Weight Decay Regularization in Adam</a>. 2017. arXiv:1711.05101v2</li><li id="c1db" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph=""><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Keskar%2C+N+S" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Nitish Shirish Keskar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Socher%2C+R" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Richard Socher</a>. <a href="https://arxiv.org/abs/1712.07628" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Improving Generalization Performance by Switching from Adam to SGD</a>. 2017 arXiv:1712.07628v1</li><li id="277e" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">Timothy Dozat. <a href="https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Incorporating Nesterov momentum into Adam</a>. 2016.</li><li id="07d9" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph=""><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bock%2C+S" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Sebastian Bock</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goppold%2C+J" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Josef Goppold</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%C3%9F%2C+M" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Martin Weiß</a>. <a href="https://arxiv.org/abs/1804.10587" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">An improvement of the convergence proof of the ADAM-Optimizer</a>. 2018. arXiv:1804.10587v1</li><li id="c8ba" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">Martin Zinkevich. <a href="http://www.cs.cmu.edu/~maz/publications/techconvex.pdf" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Online Convex Programming and Generalized Infinitesimal Gradient Ascent</a>. 2003.</li><li id="210d" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph=""><a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Wilson%2C+A+C" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Ashia C. Wilson</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Roelofs%2C+R" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Rebecca Roelofs</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Stern%2C+M" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Mitchell Stern</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Srebro%2C+N" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Nathan Srebro</a>, <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Recht%2C+B" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Benjamin Recht</a>. <a href="https://arxiv.org/abs/1705.08292" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">The Marginal Value of Adaptive Gradient Methods in Machine Learning</a>. 2017. arXiv:1705.08292v2</li><li id="091c" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">John Duchi, Elad Hazan, and Yoram Singer. <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a>. Journal of Machine Learning Research, 12:2121–2159, 2011.</li><li id="f6d4" class="ii iw cu bk ik b il kw ix in kx iy ip ky iz ir kz ja it la jb iv kt ku kv" data-selectable-paragraph="">Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: <a href="https://www.coursera.org/learn/neural-networks/lecture/YQHki/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude" class="ch dk jc jd je jf" target="_blank" rel="noopener nofollow">Divide the gradient by a running average of its recent magnitude</a>. COURSERA: neural networks for machine learning, 4(2):26–31, 2012.</li></ol></div></div></section></div></article><div class="qw fg mi mj ai mq mo mr" data-test-id="post-sidebar"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="ms n mt"><div class="td"><div class="mu mv r"><a href="https://towardsdatascience.com/?source=post_sidebar--------------------------post_sidebar-" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener"><h2 class="bj ji mw bl cu">Towards Data Science</h2></a><div class="mx my r"><h4 class="bj el cc bl bw mz bv hn na hp bo">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="by" aria-hidden="true"><button class="ct cv as at ht bc bd hu bb dg bj b bk bl bm bn dh di dj by dk be">Follow</button></div></div><div class="nb nc nd n"><div class="n o"><div class="r ce ne nf ng nh ni"><div class=""><button class="az nj nk nl nm nn no np q nq nr"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r ns nt nu nv nw nx ny"><div class="nz"><h4 class="bj el cc bl bo"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm">2.1K </button></h4></div></div></div></div><div class="nc r"></div><div><div class="ig"><div><div class="by" role="tooltip" aria-hidden="true" aria-describedby="2" aria-labelledby="2"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="qw td mi mj mk ml mm mn mo mp"></div><div><div class="oa ft n mt p"><div class="n p"><div class="z ab ac ae af fm ah ai"><div class="n ob"></div><div class="n o ob"></div><div class="oc r"><ul class="az ba"><li class="by dy if od"><a href="https://towardsdatascience.com/tagged/machine-learning" class="oe of dk bo r og oh a b em">Machine Learning</a></li><li class="by dy if od"><a href="https://towardsdatascience.com/tagged/deep-learning" class="oe of dk bo r og oh a b em">Deep Learning</a></li><li class="by dy if od"><a href="https://towardsdatascience.com/tagged/optimization" class="oe of dk bo r og oh a b em">Optimization</a></li><li class="by dy if od"><a href="https://towardsdatascience.com/tagged/towards-data-science" class="oe of dk bo r og oh a b em">Towards Data Science</a></li></ul></div><div class="oi n hd y"><div class="n oj"><div class="ok r"><div class="n o"><div class="r ce ol om on oo op"><div class=""><div class="c oq dn n o or ce os ot ou ov ow ox oy oz pa pb pc pd pe pf"><button class="az nj nk nl nm nn pg np o gi dn n p ph u fy s t ai q nq nr pi"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="r ns nt nu nv nw nx ny"><div class="ce pj nz"><h4 class="bj el cc bl cu"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm">2.1K claps</button></h4></div></div></div></div><div class="r pk pl pm pn po"></div></div><div class="n o"><div class="ie r bi"><a href="https://medium.com/p/6be9a291375c/share/twitter?source=post_actions_footer---------------------------" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="ie r bi"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="ie r bi"><a href="https://medium.com/p/6be9a291375c/share/facebook?source=post_actions_footer---------------------------" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="pp r bi"><div><div class="ig"><div><div class="by" role="tooltip" aria-hidden="true" aria-describedby="3" aria-labelledby="3"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="by" aria-hidden="true"><div class="by" aria-hidden="true"><div class="r bi"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div><div class="pq pr ps oc r pt y"><div class="r g"><div class="pu pv r ce"><span class="r pw al px"><div class="r s py pz"><a href="https://towardsdatascience.com/@bushaev?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Vitaly Bushaev" class="r dn ee qa" src="./Adam — latest trends in deep learning optimization._files/1_A7uzFalPvJ0f_0KKrQf34A(1).jpeg" width="80" height="80"></a></div><span class="r"><div class="qb r qc"><p class="bj el em bl bo eo qd">Written by</p></div><div class="qb qe n qc"><div class="ai n o hd"><h2 class="bj ji qf qg cu"><a href="https://towardsdatascience.com/@bushaev?source=follow_footer--------------------------follow_footer-" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener">Vitaly Bushaev</a></h2><div class="r g"><button class="ct cv as at ht bc bd hu bb dg bj b bk bl bm bn dh di dj by dk be">Follow</button></div></div></div></span></span><div class="qb qh r qc ar"><div class="qi r"><h4 class="bj el mw qj bo">C++, Python Developer</h4></div><div class="aq qk ar"><button class="ct cv as at ht bc bd hu bb dg bj b bk bl bm bn dh di dj by dk be">Follow</button></div></div></div><div class="pq r"></div><div class="pu pv r ce"><span class="r pw al px"><div class="r s py pz"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Towards Data Science" class="dg qa ee" src="./Adam — latest trends in deep learning optimization._files/1_hVxgUA6kP-PgL5TJjuyePg.png" width="80" height="80"></a></div><span class="r"><div class="qb qe n qc"><div class="ai n o hd"><h2 class="bj ji qf qg cu"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener">Towards Data Science</a></h2><div class="r g"><div class="by" aria-hidden="true"><button class="ct cv as at ht bc bd hu bb dg bj b bk bl bm bn dh di dj by dk be">Follow</button></div></div></div></div></span></span><div class="qb ql r qc ar"><div class="qi r"><h4 class="bj el mw qj bo">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="aq qk ar"><div class="by" aria-hidden="true"><button class="ct cv as at ht bc bd hu bb dg bj b bk bl bm bn dh di dj by dk be">Follow</button></div></div></div></div></div><div class="aq ar"><div class="qm r"><div class="n oj"><div class="qn r"><a href="https://towardsdatascience.com/@bushaev?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Vitaly Bushaev" class="r dn qo qp" src="./Adam — latest trends in deep learning optimization._files/1_A7uzFalPvJ0f_0KKrQf34A(2).jpeg" width="40" height="40"></a></div><div class="hj r"><p class="bj el qq qr bo eo qd">Written by</p><div class="n oj"><h2 class="bj ji mw bl cu"><a href="https://towardsdatascience.com/@bushaev?source=follow_footer--------------------------follow_footer-" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener">Vitaly Bushaev</a></h2><div class="hj r"><button class="hs cv as at ht bc bd hu bb dg bj b bk hv em bn dh di dj by dk be">Follow</button></div></div><div class="qs r"><h4 class="bj el cc bl bo">C++, Python Developer</h4></div></div></div><div class="qm r"><div class="n oj"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Towards Data Science" class="dg qp qo" src="./Adam — latest trends in deep learning optimization._files/1_hVxgUA6kP-PgL5TJjuyePg(1).png" width="40" height="40"></a><div class="hj r"><div class="n oj"><h2 class="bj ji mw bl cu"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener">Towards Data Science</a></h2><div class="hj r"><div class="by" aria-hidden="true"><button class="hs cv as at ht bc bd hu bb dg bj b bk hv em bn dh di dj by dk be">Follow</button></div></div></div><div class="qs r"><h4 class="bj el cc bl bo">A Medium publication sharing concepts, ideas, and codes.</h4></div></div></div></div></div></div></div><div class="qt pr r pt qu y"><a href="https://medium.com/p/6be9a291375c/responses/show?source=follow_footer--------------------------follow_footer-" class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm" rel="noopener"><span class="qv qw nm"><div class="qx qy dg r kf ar"><span class="as">See responses (9)</span></div></span></a></div></div></div><div class="qz r ra y"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="th r ti"><div class="sl mv pu r"><h2 class="bj ji tj tk cu">More From Medium</h2></div><div class="eb n oj ob tl tm tn to tp tq tr ts tt tu tv tw tx ty tz"><div class="ua ub uc ud ue uf ug uh ui uj uk ul um un uo up uq ur us ut uu"><div class="ai fy"><div class="r uv"><div class="uw ux tl tm tn uy uz to tp tq va vb tr ts tt vc vd tu tv tw ve vf tx ty tz n ob"><div class="ua ub uc ud ue uf vg vh ui uj vi vj um un vk vl uq ur vm vn uu"><div class="vo r vp f"><h4 class="bj el cc bl bo">More from Towards Data Science</h4></div><div class="bs r vq ti"><a class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm r" rel="noopener" href="https://towardsdatascience.com/10-smooth-python-tricks-for-python-gods-2e4f6180e5e3?source=post_recirc---------0------------------"></a></div></div><div class="ua ub uc ud ue uf vg vh ui uj vi vj um un vk vl uq ur vm vn uu"><div class="bs r"><div class="vr aq h vs"><h4 class="bj el cc bl bo">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/10-smooth-python-tricks-for-python-gods-2e4f6180e5e3?source=post_recirc---------0------------------"><h3 class="cu q gl b bk vt vu vv">10 Smooth Python Tricks For Python Gods</h3></a></div><div class="n o hd"><div class="cn r dx"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="bj b bk bl bm bn r cu q"><div class="eu n o hl"><span class="bj el cc bl bw hm bv hn ho hp cu"><a href="https://towardsdatascience.com/@emmettgb?source=post_recirc---------0------------------" class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener">Emmett Boudreau</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------0------------------" class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj el cc bl bw hm bv hn ho hp bo"><div><a class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener" href="https://towardsdatascience.com/10-smooth-python-tricks-for-python-gods-2e4f6180e5e3?source=post_recirc---------0------------------">Jun 2</a> · 6 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="r ce ne nf ng nh ni"><div class=""><button class="az nj nk nl nm nn no su q nq nr"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r ns nt nu nv nw nx ny"><div class="nz"><h4 class="bj el cc bl bo">2K</h4></div></div></div><div class="vw hj cn ef vx r"></div><div class="ig"><div><div class="by" role="tooltip" aria-hidden="true" aria-describedby="15" aria-labelledby="15"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ua ub uc ud ue uf ug uh ui uj uk ul um un uo up uq ur us ut uu"><div class="ai fy"><div class="r uv"><div class="uw ux tl tm tn uy uz to tp tq va vb tr ts tt vc vd tu tv tw ve vf tx ty tz n ob"><div class="ua ub uc ud ue uf vg vh ui uj vi vj um un vk vl uq ur vm vn uu"><div class="vo r vp f"><h4 class="bj el cc bl bo">More from Towards Data Science</h4></div><div class="bs r vq ti"><a class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm r" rel="noopener" href="https://towardsdatascience.com/how-to-process-a-dataframe-with-billions-of-rows-in-seconds-c8212580f447?source=post_recirc---------1------------------"></a></div></div><div class="ua ub uc ud ue uf vg vh ui uj vi vj um un vk vl uq ur vm vn uu"><div class="bs r"><div class="vr aq h vs"><h4 class="bj el cc bl bo">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/how-to-process-a-dataframe-with-billions-of-rows-in-seconds-c8212580f447?source=post_recirc---------1------------------"><h3 class="cu q gl b bk vt vu vv">How to process a DataFrame with billions of rows in seconds</h3></a></div><div class="n o hd"><div class="cn r dx"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="bj b bk bl bm bn r cu q"><div class="eu n o hl"><span class="bj el cc bl bw hm bv hn ho hp cu"><a href="https://towardsdatascience.com/@romanorac?source=post_recirc---------1------------------" class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener">Roman Orac</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------1------------------" class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj el cc bl bw hm bv hn ho hp bo"><div><a class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener" href="https://towardsdatascience.com/how-to-process-a-dataframe-with-billions-of-rows-in-seconds-c8212580f447?source=post_recirc---------1------------------">Jun 1</a> · 6 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="r ce ne nf ng nh ni"><div class=""><button class="az nj nk nl nm nn no su q nq nr"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r ns nt nu nv nw nx ny"><div class="nz"><h4 class="bj el cc bl bo">2.5K</h4></div></div></div><div class="vw hj cn ef vx r"></div><div class="ig"><div><div class="by" role="tooltip" aria-hidden="true" aria-describedby="16" aria-labelledby="16"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ua ub uc ud ue uf ug uh ui uj uk ul um un uo up uq ur us ut uu"><div class="ai fy"><div class="r uv"><div class="uw ux tl tm tn uy uz to tp tq va vb tr ts tt vc vd tu tv tw ve vf tx ty tz n ob"><div class="ua ub uc ud ue uf vg vh ui uj vi vj um un vk vl uq ur vm vn uu"><div class="vo r vp f"><h4 class="bj el cc bl bo">More from Towards Data Science</h4></div><div class="bs r vq ti"><a class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm r" rel="noopener" href="https://towardsdatascience.com/a-better-way-to-become-a-data-scientist-than-online-courses-2abc343d4d55?source=post_recirc---------2------------------"></a></div></div><div class="ua ub uc ud ue uf vg vh ui uj vi vj um un vk vl uq ur vm vn uu"><div class="bs r"><div class="vr aq h vs"><h4 class="bj el cc bl bo">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/a-better-way-to-become-a-data-scientist-than-online-courses-2abc343d4d55?source=post_recirc---------2------------------"><h3 class="cu q gl b bk vt vu vv">A Better Way To Become A Data Scientist Than Online Courses</h3></a></div><div class="n o hd"><div class="cn r dx"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="bj b bk bl bm bn r cu q"><div class="eu n o hl"><span class="bj el cc bl bw hm bv hn ho hp cu"><a href="https://towardsdatascience.com/@chris.the.data.scientist?source=post_recirc---------2------------------" class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener">Chris</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------2------------------" class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="bj b bk bl bm bn r bo bp"><span class="bj el cc bl bw hm bv hn ho hp bo"><div><a class="ch ci au av aw ax ay az ba bb hq be bf cl cm" rel="noopener" href="https://towardsdatascience.com/a-better-way-to-become-a-data-scientist-than-online-courses-2abc343d4d55?source=post_recirc---------2------------------">May 29</a> · 4 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="r ce ne nf ng nh ni"><div class=""><button class="az nj nk nl nm nn no su q nq nr"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r ns nt nu nv nw nx ny"><div class="nz"><h4 class="bj el cc bl bo">3K</h4></div></div></div><div class="vw hj cn ef vx r"></div><div class="ig"><div><div class="by" role="tooltip" aria-hidden="true" aria-describedby="17" aria-labelledby="17"><button class="ch ci au av aw ax ay az ba bb cj ck be bf cl cm"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="rb r rc rd"><section class="fa fb ai dj r re rf rg rh ri rj rk rl rm rn ro rp rq rr rs"><div class="rt ru pu n hd g"><div class="rv n hd"><div class="rw r rx"><div class="ry r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb rz sa be bf sb sc" rel="noopener"><h4 class="sd se sf bj ji bk qj sg sh r">Discover <!-- -->Medium</h4></a></div><span class="bj b bk bl bm bn r si sj">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb be bf sb sc sk" rel="noopener">Watch</a></span></div><div class="rw r rx"><div class="sl r"><a href="https://medium.com/topics?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb rz sa be bf sb sc" rel="noopener"><h4 class="sd se sf bj ji bk qj sg sh r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="bj b bk bl bm bn r si sj">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb be bf sb sc sk" rel="noopener">Explore</a></span></div><div class="rw r rx"><div class="ry r"><a href="https://medium.com/membership?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb rz sa be bf sb sc" rel="noopener"><h4 class="sd se sf bj ji bk qj sg sh r">Become a member</h4></a></div><span class="bj b bk bl bm bn r si sj">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb be bf sb sc sk" rel="noopener">Upgrade</a></span></div></div></div><div class="n mt"><div class="n o hd"><a href="https://medium.com/?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb rz sa be bf sb sc" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="se"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="bj b bk bl bm bn r si sj"><div class="qs sm n hd sn al"><h4 class="bj el mw qj sd"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb hq be bf sb sc" rel="noopener">About</a></h4><h4 class="bj el mw qj sd"><a href="https://help.medium.com/?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb hq be bf sb sc" rel="noopener">Help</a></h4><h4 class="bj el mw qj sd"><a href="https://medium.com/policy/9db0094a1e0f?source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb hq be bf sb sc" rel="noopener">Legal</a></h4></div></span></div><div class="aq so sp al"><h4 class="bj el mw qj si">Get the Medium app</h4></div><div class="aq so sq al sr"><div class="cr r"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb rz sa be bf sb sc" rel="noopener nofollow"><img alt="A button that says &#39;Download on the App Store&#39;, and if clicked it will lead you to the iOS App store" class="" src="./Adam — latest trends in deep learning optimization._files/1_M2FVPPidy2x386MRAE-EeA.png" width="135" height="41"></a></div><div class="r"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----6be9a291375c----------------------" class="ch ci au av aw ax ay az ba bb rz sa be bf sb sc" rel="noopener nofollow"><img alt="A button that says &#39;Get it on, Google Play&#39;, and if clicked it will lead you to the Google Play store" class="" src="./Adam — latest trends in deep learning optimization._files/1_HyH8oIcJvXp7xzu5oF6dTg.png" width="135" height="41"></a></div></div></div></section></div></div></div><script>window.__BUILD_ID__ = "master-20200605-194009-1dee7aea0e"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200605-194009-1dee7aea0e","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200605-194009-1dee7aea0e"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"master-20200605-194009-1dee7aea0e","commit":"1dee7aea0e59083879be38472994a778715cb6d8"}},"datacenter":"us"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"webpMiroImageIds":["1*y7gegIZOYlsnhWFJwIyDJw","1*ByGRQD1zlYXGS4YBYAoLVA","1*orNowUCqCER-BwaAXOZx0A","1*itOsotYFripvvRKY1itrVQ","1*b_LB1ifqWQ2x3JG_m3MJsg","1*AD7jcqVRun0Hhwmg0-Vqfg","1*0kJoJveqoxkYXEmdM2FZ3A","1*pTq8R2lALVUytg_k4y5CpQ","1*1_tSnwIHb_oPsU9vucJijQ","1*EBN0PWXjvaF2gRdk9fCvzA","1*Uxc2_wlnoVQNMQUhQaLVZg","1*ABOw4ARUQ90kwKfXqeVdXA","1*Ok2A1h7LmAtYjWVG9c8IMA","1*Kw1AUMFyy3AGJ1BbTdeyWg","1*2RZldaiJQXadc5zjscYncg","1*hPIJUpxe2QMvOMNcTlnOlQ","1*MZyvxFpPUvfBUfoNvFhRzg","1*2ROzqt2hXcYs6BuKjG2_nQ","1*NO9eMccT-vPrY8nylJ4PVw","2*6cf2Ep3P-r1vCrc-6Bc-vA","1*hVxgUA6kP-PgL5TJjuyePg","1*VGtACZSU6AxT3ugiNr-WGg","0*8dBf1Vy9mkDdcuwQ","1*5ciI2lDFX8sJanIJa6ppnA","1*Hqtfw2Juvf6Zb9uGimLLMg","1*suPSqLiNrJPCUbtdUwLnGw","1*dIANAeHtMxPlVO9awEN0Jw","1*N2KcM3GCLymsKxnSBXyG_g","1*fgNVzsUlPl9tA8ladAT-KA","1*h-La0GVOrPo6SrFpNWQLtw","1*3IPJZVYg-95RkV8H4DRjvA","1*DgnF3PmTVG14Oz_-8BWX_g","1*x_SKDZtUCcWMH9FB092srw","1*oQ4U4pCo7OUPMXXphwqzTg","1*fjC3jxxcmOwXLwqxraNHfw","1*DXy0NEVftDaLKDVG8dS3YQ","1*KbxEajPgdT9GhcWWGG8JmQ","1*e6oTrX0jQU0lPM_0Tt-oYw","1*oQ4U4pCo7OUPMXXphwqzTg","1*5I_5X9Of8dwLfLEwJWd9jA","1*wYUZRoGUFOj1kdT7R8S0TQ","1*dzn6a448FO7kqbXk2K1qiA","1*YQDEXca6FZq-9L5WXBH-3A","1*YlpXJYsNOORiPqY0Rx6zIQ","1*NM3ybm1NGbyvF_IGL5DPYQ","1*DJ36tR2MtLrCRA1BO62L8Q","1*wV6yqGmy-aGNGWt1paWy5g","0*iWmcqANWK8Ayo6NP","0*nB8YhFfPe8q0sJcJ","0*GN_xV2uMHSeGuotv","0*oT1BmxuFNEyN0XC4","0*Aow5I0AXoM8HG4RA","0*geqIA7abXJGEobLr"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"performanceTags":[],"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":["pandemic","epidemic","coronavirus","covid19","co-vid-19","containment","self-care","flatten-the-curve","public-health","virus","public-health-crisis","quarantine","self-quarantine","zika","corona","disease-prevention","wuhan","chinavirus","outbreak","influenza","socialdistancing","social-distance","flu","vaccines","healthcare","medicine","conspiracy-theories","conspiracy","virality","epidemia","pandemia","salud","corona-e-virus","coronavirus-covid19","covid-19","covid-19-symptoms","covid-19-crisis","covid-19-testing","covid-19-treatment","coronavirus-update","coronavirus-diaries"],"COVID_APPLICABLE_TOPIC_NAMES":["coronavirus"],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":["coronavirus","health"],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4"},"debug":{"requestId":"ada20129-722d-4a80-8e95-32aa6c729ddb","edge":"","originalSpanCarrier":{"ot-tracer-spanid":"7e904582392af4ed","ot-tracer-traceid":"742017a2334079c5","ot-tracer-sampled":"true"}},"session":{"user":{"id":"85164a79d2b9"},"xsrf":"3lhV8CCD98HW","isSpoofed":false},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fadam-latest-trends-in-deep-learning-optimization-6be9a291375c","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register"},"postRead":false},"client":{"isBot":false,"isCustomDomain":true,"isEu":false,"isNativeMedium":false,"isSafariMobile":false,"inAppBrowserName":"","supportsWebp":true},"multiVote":{"clapsPerPost":{}},"tracing":{}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"bane_add_user","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.10":{"name":"coronavirus_topic_recirc","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"covid_19_cdc_banner","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_alternate_onboarding_email_subject","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_apple_sign_in","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_aurora_opt_in_control","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_braintree_integration","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_braintree_webhook","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_cc_trial_member_onboarding_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_cleansweep_cachev2_reads","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_cleansweep_double_writes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_confirm_sign_in","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_cta_meter","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_curation_priority_queue_experiment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_digest_feature_logging","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_digest_tagline","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_end_of_post_cleanup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_filter_expire_processor","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_footer_app_buttons","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_free_corona_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_global_susi_modal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_highlander_member_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_icelandic_truncated_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.57":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_li_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_lite_about_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_lo_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_logged_out_history","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_login_code_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_m2_unviewable_filter","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_miro_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_ml_rank_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"enable_ml_rank_rex_anno","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"enable_more_on_coronavirus","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"enable_mute","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"enable_newsletter_v3","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.92":{"name":"enable_olsen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"enable_open_in_app_regwall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.94":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.95":{"name":"enable_orion","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.96":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.96.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.96.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.97":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.97.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.97.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.98":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.98.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.98.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.99":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.99.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.99.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.100":{"name":"enable_post_page_nav_stickiness_removal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.100.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.100.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.101":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.101.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.101.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.102":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.102.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.102.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.103":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.103.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.103.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.104":{"name":"enable_responses_2","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.104.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.104.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.105":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.105.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.105.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.106":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.106.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.106.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.107":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.107.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.107.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.108":{"name":"enable_sepia_to_olsen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.108.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.108.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.109":{"name":"enable_sisko","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.109.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.109.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.110":{"name":"enable_starspace","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.110.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.110.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.111":{"name":"enable_starspace_digest_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.111.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.111.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.112":{"name":"enable_starspace_ranker_starspace","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.112.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.112.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.113":{"name":"enable_theme_editor","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.113.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.113.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.114":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.114.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.114.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.115":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.115.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.115.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.116":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.116.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.116.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.117":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.117.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.117.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.118":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.118.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.118.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.119":{"name":"enable_utc_fix_on_partner_program_dashboard","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.119.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.119.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.120":{"name":"enable_valencia_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.120.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.120.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.121":{"name":"exclude_curated_in_popular_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.121.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.121.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.122":{"name":"featured_fc_and_ydr","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.122.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.122.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.123":{"name":"filter_low_scoring_users","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.123.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.123.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.124":{"name":"glyph_embed_commands","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.124.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.124.valueType":{"__typename":"VariantFlagString","value":"control"},"ROOT_QUERY.variantFlags.125":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.125.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.125.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.126":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.126.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.126.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.127":{"name":"ios_pub_follow_email_opt_in","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.127.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.127.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.128":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.128.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.128.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.129":{"name":"limit_post_referrers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.129.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.129.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.130":{"name":"make_nav_sticky","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.130.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.130.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.131":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.131.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.131.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.132":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.132.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.132.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.133":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.133.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.133.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.134":{"name":"remove_email_opt_in_on_pub_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.134.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.134.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.135":{"name":"remove_post_post_similarity","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.135.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.135.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.136":{"name":"share_post_linkedin","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.136.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.136.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.137":{"name":"sign_up_with_email_button","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.137.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.137.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.138":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.138.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.138.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.139":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.139.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.139.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.140":{"name":"skip_sign_in_recaptcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.140.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.140.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.141":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.141.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.141.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.142":{"name":"xgboost_auto_suspend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.142.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.142.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.96","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.97","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.98","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.99","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.100","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.101","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.102","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.103","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.104","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.105","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.106","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.107","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.108","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.109","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.110","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.111","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.112","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.113","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.114","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.115","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.116","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.117","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.118","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.119","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.120","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.121","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.122","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.123","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.124","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.125","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.126","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.127","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.128","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.129","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.130","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.131","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.132","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.133","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.134","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.135","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.136","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.137","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.138","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.139","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.140","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.141","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.142","typename":"VariantFlag"}],"viewer":{"type":"id","generated":false,"id":"User:85164a79d2b9","typename":"User"},"meterPost({\"postId\":\"6be9a291375c\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"6be9a291375c\"})":{"type":"id","generated":false,"id":"Post:6be9a291375c","typename":"Post"}},"User:85164a79d2b9":{"id":"85164a79d2b9","username":"ayushverma1321","name":"Ayushverma","imageId":"0*fSKu5zWydc5B9tfM","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"ayushverma1321@gmail.com","unverifiedEmail":"","createdAt":1589189777130,"isEligibleToViewNewResponses":false,"isMembershipTrialEligible":true,"isSuspended":false,"__typename":"User"},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":[]},"maxUnlockCount":4,"unlocksRemaining":4},"Post:6be9a291375c":{"__typename":"Post","id":"6be9a291375c","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fadam-latest-trends-in-deep-learning-optimization-6be9a291375c","canonicalUrl":"","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:6be9a291375c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})","typename":"PostContent"},"creator":{"type":"id","generated":false,"id":"User:55063a62106c","typename":"User"},"firstPublishedAt":1540216303250,"isLocked":false,"isPublished":true,"layerCake":3,"primaryTopic":{"type":"id","generated":false,"id":"data-science","typename":"Topic"},"title":"Adam — latest trends in deep learning optimization.","latestPublishedVersion":"db7a81234a79","visibility":"PUBLIC","isLimitedState":false,"sequence":null,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":15.18301886792453,"readingList":"READING_LIST_NONE","allowResponses":true,"clapCount":2190,"viewerClapCount":null,"voterCount":441,"recommenders":[],"license":"ALL_RIGHTS_RESERVED","tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:optimization","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:towards-data-science","typename":"Tag"}],"topics":[{"type":"id","generated":false,"id":"1eca0103fff3","typename":"Topic"},{"type":"id","generated":false,"id":"ae5d4995e225","typename":"Topic"}],"postResponses":{"type":"id","generated":true,"id":"$Post:6be9a291375c.postResponses","typename":"PostResponses"},"responsesCount":9,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"lockedSource":"LOCKED_POST_SOURCE_NONE","curationEligibleAt":0,"isDistributionAlertDismissed":false,"audioVersionUrl":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1540359420020,"previewContent":{"type":"id","generated":true,"id":"$Post:6be9a291375c.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*lAMbp2AXAVlmxN_hbsDi3g.jpeg","typename":"ImageMetadata"},"isShortform":false,"seoTitle":"","updatedAt":1540359420020,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","customStyleSheet":null,"colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","isAuroraPilot":false,"isAuroraVisible":false,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png","typename":"ImageMetadata"},"isAuroraEligible":false,"viewerIsEditor":false,"__typename":"Collection","name":"Towards Data Science","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"isNewsletterV3Enabled":true,"newsletterV3":{"type":"id","generated":false,"id":"NewsletterV3:d6fe9076899","typename":"NewsletterV3"},"creator":{"type":"id","generated":false,"id":"User:895063a310f4","typename":"User"},"navItems":[{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.6","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.7","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette","typename":"ColorPalette"},"description":"A Medium publication sharing concepts, ideas, and codes.","shortDescription":"A Medium publication sharing concepts, ideas, and codes.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":true,"isUserSubscribedToCollectionEmails":false,"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"viewerIsMuting":false,"ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"tagline":"A Medium publication sharing concepts, ideas, and codes."},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","originalWidth":337,"originalHeight":122,"__typename":"ImageMetadata"},"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png":{"id":"1*hVxgUA6kP-PgL5TJjuyePg.png","__typename":"ImageMetadata"},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","slug":"monthly-edition","__typename":"NewsletterV3","isSubscribed":false,"showPromo":false,"name":"Monthly Edition","description":"Well-written and informative articles that you’ll be excited to read. Find our best content here, including tutorials, hands-on real-world examples, research, and cutting-edge techniques. It’s the best way to learn new things and stay up to date with Towards Data Science. ","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"}},"User:895063a310f4":{"id":"895063a310f4","__typename":"User"},"Collection:7f60cf5620c9.navItems.0":{"title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.1":{"title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.2":{"title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.3":{"title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.4":{"title":"AI","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fartificial-intelligence\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.5":{"title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.6":{"title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.7":{"title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF355876","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF355876","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF4D6C88","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF637F99","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF7791A8","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF8CA2B7","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF9FB3C6","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFB2C3D4","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFC5D2E1","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFD7E2EE","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFE9F1FA","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFBFFFF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum","typename":"ColorSpectrum"},"defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"}},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFEDF4FC","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFE9F2FD","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFE6F1FD","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFE2EFFD","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFDFEEFD","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFDBECFE","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFD7EBFE","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFD4E9FE","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFD0E7FF","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFCCE6FF","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFC8E4FF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF668AAA","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF61809D","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF5A7690","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF546C83","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF4D6275","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF455768","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF3D4C5A","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF34414C","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF2B353E","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF21282F","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF161B1F","point":1,"__typename":"ColorPoint"},"$Post:6be9a291375c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:6be9a291375c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel","typename":"RichText"}},"User:55063a62106c":{"id":"55063a62106c","isSuspended":false,"__typename":"User","allowNotes":true,"name":"Vitaly Bushaev","isFollowing":false,"username":"bushaev","bio":"C++, Python Developer","imageId":"1*A7uzFalPvJ0f_0KKrQf34A.jpeg","mediumMemberAt":0,"isBlocking":false,"isMuting":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"Vitalik36"},"data-science":{"name":"Data Science","slug":"data-science","__typename":"Topic","isFollowing":null},"$Post:6be9a291375c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0":{"name":"95cf","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:6be9a291375c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:6be9a291375c.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_70","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_71","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_72","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_73","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_74","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_75","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_76","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_77","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_78","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_79","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_80","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_81","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_82","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_83","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_84","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_85","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_86","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_87","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_88","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_89","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_90","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_91","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_92","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_93","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_94","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_95","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_96","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:db7a81234a79_97","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:db7a81234a79_0":{"id":"db7a81234a79_0","name":"e0e2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*lAMbp2AXAVlmxN_hbsDi3g.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*lAMbp2AXAVlmxN_hbsDi3g.jpeg":{"id":"1*lAMbp2AXAVlmxN_hbsDi3g.jpeg","originalHeight":2110,"originalWidth":2812,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_1":{"id":"db7a81234a79_1","name":"b83a","type":"H3","href":null,"layout":null,"metadata":null,"text":"Adam — latest trends in deep learning optimization.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_2":{"id":"db7a81234a79_2","name":"1ea2","type":"P","href":null,"layout":null,"metadata":null,"text":"Adam [1] is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. First published in 2014, Adam was presented at a very prestigious conference for deep learning practitioners — ICLR 2015. The paper contained some very promising diagrams, showing huge performance gains in terms of speed of training. However, after a while people started noticing, that in some cases Adam actually finds worse solution than stochastic gradient descent. A lot of research has been done to address the problems of Adam.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_2.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_2.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_2.markups.0":{"type":"A","start":240,"end":250,"href":"https:\u002F\u002Fwww.iclr.cc\u002Farchive\u002Fwww\u002Fdoku.php%3Fid=iclr2015:main.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_2.markups.1":{"type":"A","start":471,"end":498,"href":"https:\u002F\u002Ftowardsdatascience.com\u002Fstochastic-gradient-descent-with-momentum-a84097641a5d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_3":{"id":"db7a81234a79_3","name":"f60b","type":"P","href":null,"layout":null,"metadata":null,"text":"The algorithms leverages the power of adaptive learning rates methods to find individual learning rates for each parameter. It also has advantages of Adagrad [10], which works really well in settings with sparse gradients, but struggles in non-convex optimization of neural networks, and RMSprop [11], which tackles to resolve some of the problems of Adagrad and works really well in on-line settings. Adam has been raising in popularity exponentially according to ‘A Peek at Trends in Machine Learning’ article from Andrej Karpathy.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_3.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_3.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_3.markups.0":{"type":"A","start":465,"end":503,"href":"https:\u002F\u002Fmedium.com\u002F@karpathy\u002Fa-peek-at-trends-in-machine-learning-ab8a1085a106","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_3.markups.1":{"type":"A","start":517,"end":532,"href":"https:\u002F\u002Fmedium.com\u002F@karpathy?source=post_header_lockup","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_4":{"id":"db7a81234a79_4","name":"b324","type":"P","href":null,"layout":null,"metadata":null,"text":"In this post, I first introduce Adam algorithm as presented in the original paper, and then walk through latest research around it that demonstrates some potential reasons why the algorithms works worse than classic SGD in some areas and provides several solutions, that narrow the gap between SGD and Adam.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_5":{"id":"db7a81234a79_5","name":"5f44","type":"H3","href":null,"layout":null,"metadata":null,"text":"Adam","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_6":{"id":"db7a81234a79_6","name":"7e3b","type":"P","href":null,"layout":null,"metadata":null,"text":"Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum. Let’s take a closer look at how it works.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_7":{"id":"db7a81234a79_7","name":"2a82","type":"P","href":null,"layout":null,"metadata":null,"text":"Adam is an adaptive learning rate method, which means, it computes individual learning rates for different parameters. Its name is derived from adaptive moment estimation, and the reason it’s called that is because Adam uses estimations of first and second moments of gradient to adapt the learning rate for each weight of the neural network. Now, what is moment ? N-th moment of a random variable is defined as the expected value of that variable to the power of n. More formally:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_7.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_7.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_7.markups.0":{"type":"A","start":153,"end":159,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMoment_(mathematics)","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_7.markups.1":{"type":"A","start":416,"end":430,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FExpected_value","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_8":{"id":"db7a81234a79_8","name":"6d71","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*NIM9yxfs4PciZ_A7w-n0-w.png","typename":"ImageMetadata"},"text":"m — moment, X -random variable.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*NIM9yxfs4PciZ_A7w-n0-w.png":{"id":"1*NIM9yxfs4PciZ_A7w-n0-w.png","originalHeight":51,"originalWidth":144,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_9":{"id":"db7a81234a79_9","name":"3abf","type":"P","href":null,"layout":null,"metadata":null,"text":"It can be pretty difficult to grasp that idea for the first time, so if you don’t understand it fully, you should still carry on, you’ll be able to understand how algorithms works anyway. Note, that gradient of the cost function of neural network can be considered a random variable, since it usually evaluated on some small random batch of data. The first moment is mean, and the second moment is uncentered variance (meaning we don’t subtract the mean during variance calculation). We will see later how we use these values, right now, we have to decide on how to get them. To estimates the moments, Adam utilizes exponentially moving averages, computed on the gradient evaluated on a current mini-batch:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_10":{"id":"db7a81234a79_10","name":"c8f2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ZhGLUwaaqlJ9C0WK0nbAEA.png","typename":"ImageMetadata"},"text":"Moving averages of gradient and squared gradient.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ZhGLUwaaqlJ9C0WK0nbAEA.png":{"id":"1*ZhGLUwaaqlJ9C0WK0nbAEA.png","originalHeight":139,"originalWidth":443,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_11":{"id":"db7a81234a79_11","name":"6b43","type":"P","href":null,"layout":null,"metadata":null,"text":"Where m and v are moving averages, g is gradient on current mini-batch, and betas — new introduced hyper-parameters of the algorithm. They have really good default values of 0.9 and 0.999 respectively. Almost no one ever changes these values. The vectors of moving averages are initialized with zeros at the first iteration.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_12":{"id":"db7a81234a79_12","name":"94f2","type":"P","href":null,"layout":null,"metadata":null,"text":"To see how these values correlate with the moment, defined as in first equation, let’s take look at expected values of our moving averages. Since m and v are estimates of first and second moments, we want to have the following property:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_12.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_12.markups.0":{"type":"A","start":158,"end":167,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FEstimator","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_13":{"id":"db7a81234a79_13","name":"7e6f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*uoUrVBHcgpdfbB4stoNgYA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*uoUrVBHcgpdfbB4stoNgYA.png":{"id":"1*uoUrVBHcgpdfbB4stoNgYA.png","originalHeight":105,"originalWidth":216,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_14":{"id":"db7a81234a79_14","name":"a38a","type":"P","href":null,"layout":null,"metadata":null,"text":"Expected values of the estimators should equal the parameter we’re trying to estimate, as it happens, the parameter in our case is also the expected value. If these properties held true, that would mean, that we have unbiased estimators. (To learn more about statistical properties of different estimators, refer to Ian Goodfellow’s Deep Learning book, Chapter 5 on machine learning basics). Now, we will see that these do not hold true for the our moving averages. Because we initialize averages with zeros, the estimators are biased towards zero. Let’s prove that for m (the proof for v would be analogous). To prove that we need to formula for m to the very first gradient. Let’s try to unroll a couple values of m to see he pattern we’re going to use:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_14.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_14.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_14.markups.0":{"type":"A","start":333,"end":362,"href":"http:\u002F\u002Fwww.deeplearningbook.org\u002Fcontents\u002Fml.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_14.markups.1":{"type":"STRONG","start":217,"end":236,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_15":{"id":"db7a81234a79_15","name":"abac","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ztYpt5ppDEE7lW8LqxMShw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ztYpt5ppDEE7lW8LqxMShw.png":{"id":"1*ztYpt5ppDEE7lW8LqxMShw.png","originalHeight":235,"originalWidth":1061,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_16":{"id":"db7a81234a79_16","name":"3ccf","type":"P","href":null,"layout":null,"metadata":null,"text":"As you can see, the ‘further’ we go expanding the value of m, the less first values of gradients contribute to the overall value, as they get multiplied by smaller and smaller beta. Capturing this patter, we can rewrite the formula for our moving average:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_17":{"id":"db7a81234a79_17","name":"eda9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*5Q1ZlbpP-C4wzJVtVfGn5Q.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*5Q1ZlbpP-C4wzJVtVfGn5Q.png":{"id":"1*5Q1ZlbpP-C4wzJVtVfGn5Q.png","originalHeight":131,"originalWidth":385,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_18":{"id":"db7a81234a79_18","name":"aaa5","type":"P","href":null,"layout":null,"metadata":null,"text":"Now, let’s take a look at the expected value of m, to see how it relates to the true first moment, so we can correct for the discrepancy of the two :","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_19":{"id":"db7a81234a79_19","name":"ce65","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*_uSts1p9vJ5yat3iTHR5Ug.png","typename":"ImageMetadata"},"text":"Bias correction for the first momentum estimator","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_uSts1p9vJ5yat3iTHR5Ug.png":{"id":"1*_uSts1p9vJ5yat3iTHR5Ug.png","originalHeight":292,"originalWidth":518,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_20":{"id":"db7a81234a79_20","name":"f23c","type":"P","href":null,"layout":null,"metadata":null,"text":"In the first row, we use our new formula for moving average to expand m. Next, we approximate g[i] with g[t]. Now we can take it out of sum, since it does not now depend on i. Because the approximation is taking place, the error C emerge in the formula. In the last line we just use the formula for the sum of a finite geometric series. There are two things we should note from that equation.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_21":{"id":"db7a81234a79_21","name":"43a3","type":"OLI","href":null,"layout":null,"metadata":null,"text":"We have biased estimator. This is not just true for Adam only, the same holds for algorithms, using moving averages (SGD with momentum, RMSprop, etc.).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_22":{"id":"db7a81234a79_22","name":"12fd","type":"OLI","href":null,"layout":null,"metadata":null,"text":"It won’t have much effect unless it’s the begging of the training, because the value beta to the power of t is quickly going towards zero.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_23":{"id":"db7a81234a79_23","name":"9ffb","type":"P","href":null,"layout":null,"metadata":null,"text":"Now we need to correct the estimator, so that the expected value is the one we want. This step is usually referred to as bias correction. The final formulas for our estimator will be as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_24":{"id":"db7a81234a79_24","name":"2d4e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*M86IUMsrHXq4WrS-Bk5boA.png","typename":"ImageMetadata"},"text":"Bias corrected estimators for the first and second moments.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*M86IUMsrHXq4WrS-Bk5boA.png":{"id":"1*M86IUMsrHXq4WrS-Bk5boA.png","originalHeight":248,"originalWidth":195,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_25":{"id":"db7a81234a79_25","name":"c5ad","type":"P","href":null,"layout":null,"metadata":null,"text":"The only thing left to do is to use those moving averages to scale learning rate individually for each parameter. The way it’s done in Adam is very simple, to perform weight update we do the following:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_26":{"id":"db7a81234a79_26","name":"d238","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*WrfK1bWzKYEH-UdsBHYl5A.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*WrfK1bWzKYEH-UdsBHYl5A.png":{"id":"1*WrfK1bWzKYEH-UdsBHYl5A.png","originalHeight":128,"originalWidth":360,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_27":{"id":"db7a81234a79_27","name":"8c9c","type":"P","href":null,"layout":null,"metadata":null,"text":"Where w is model weights, eta (look like the letter n) is the step size (it can depend on iteration). And that’s it, that’s the update rule for Adam. For some people it can be easier to understand such concepts in code, so here’s possible implementation of Adam in python:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_28":{"id":"db7a81234a79_28","name":"1834","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:db7a81234a79_28.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:1989f43f8b10f499efc335c7d9205236":{"id":"1989f43f8b10f499efc335c7d9205236","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Adam.py","__typename":"MediaResource"},"$Paragraph:db7a81234a79_28.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:1989f43f8b10f499efc335c7d9205236","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:db7a81234a79_29":{"id":"db7a81234a79_29","name":"e849","type":"P","href":null,"layout":null,"metadata":null,"text":"There are two small variations on Adam that I don’t see much in practice, but they’re implemented in major deep learning frameworks, so it’s worth to briefly mention them.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_30":{"id":"db7a81234a79_30","name":"4e2c","type":"P","href":null,"layout":null,"metadata":null,"text":"First one, called Adamax was introduced by the authors of Adam in the same paper. The idea with Adamax is to look at the value v as the L2 norm of the individual current and past gradients. We can generalize it to Lp update rule, but it gets pretty unstable for large values of p. But if we use the special case of L-infinity norm, it results in a surprisingly stable and well-performing algorithm. Here’s how to implement Adamax with python:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_30.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_30.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_30.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_30.markups.0":{"type":"A","start":136,"end":143,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FEuclidean_distance","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_30.markups.1":{"type":"A","start":315,"end":330,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FUniform_norm","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_30.markups.2":{"type":"STRONG","start":18,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_31":{"id":"db7a81234a79_31","name":"97d4","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:db7a81234a79_31.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:3234ba4c134f5d83c166fffef6eac13c":{"id":"3234ba4c134f5d83c166fffef6eac13c","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Adamax.py","__typename":"MediaResource"},"$Paragraph:db7a81234a79_31.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:3234ba4c134f5d83c166fffef6eac13c","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:db7a81234a79_32":{"id":"db7a81234a79_32","name":"6c04","type":"P","href":null,"layout":null,"metadata":null,"text":"Second one is a bit harder to understand, called Nadam [6]. Nadam was published by Timothy Dozat in the paper ‘Incorporating Nesterov Momentum into Adam’. As name suggests the idea is to use Nesterov momentum term for the first moving averages. Let’s take a look at update rule of the SGD with momentum:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_32.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_32.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_32.markups.0":{"type":"STRONG","start":49,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_32.markups.1":{"type":"STRONG","start":59,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_33":{"id":"db7a81234a79_33","name":"0091","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*ZcprfGi_ppxR5ddYP5LxzQ.png","typename":"ImageMetadata"},"text":"SGD with momentum update rule","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ZcprfGi_ppxR5ddYP5LxzQ.png":{"id":"1*ZcprfGi_ppxR5ddYP5LxzQ.png","originalHeight":116,"originalWidth":589,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_34":{"id":"db7a81234a79_34","name":"4706","type":"P","href":null,"layout":null,"metadata":null,"text":"As shown above, the update rule is equivalent to taking a step in the direction of momentum vector and then taking a step in the direction of gradient. However, the momentum step doesn’t depend on the current gradient , so we can get a higher-quality gradient step direction by updating the parameters with the momentum step before computing the gradient. To achieve that, we modify the update as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_35":{"id":"db7a81234a79_35","name":"f40e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*a0NNGI-iQ3OtQAUUtCZgIQ.png","typename":"ImageMetadata"},"text":"f — loss function to optimize.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*a0NNGI-iQ3OtQAUUtCZgIQ.png":{"id":"1*a0NNGI-iQ3OtQAUUtCZgIQ.png","originalHeight":182,"originalWidth":397,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_36":{"id":"db7a81234a79_36","name":"1801","type":"P","href":null,"layout":null,"metadata":null,"text":"So, with Nesterov accelerated momentum we first make make a big jump in the direction of the previous accumulated gradient and then measure the gradient where we ended up to make a correction. There’s a great visualization from cs231n lecture notes:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_36.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_36.markups.0":{"type":"A","start":228,"end":248,"href":"http:\u002F\u002Fcs231n.github.io\u002Fneural-networks-3\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_37":{"id":"db7a81234a79_37","name":"f27e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg","typename":"ImageMetadata"},"text":"sourec: cs231n lecture notes.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_37.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg":{"id":"1*hJSLxZMjYVzgF5A_MoqeVQ.jpeg","originalHeight":254,"originalWidth":800,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_37.markups.0":{"type":"A","start":8,"end":28,"href":"http:\u002F\u002Fcs231n.github.io\u002Fneural-networks-3\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_38":{"id":"db7a81234a79_38","name":"24b0","type":"P","href":null,"layout":null,"metadata":null,"text":"The same method can be incorporated into Adam, by changing the first moving average to a Nesterov accelerated momentum. One computation trick can be applied here: instead of updating the parameters to make momentum step and changing back again, we can achieve the same effect by applying the momentum step of time step t + 1 only once, during the update of the previous time step t instead of t + 1. Using this trick, the implementation of Nadam may look like this:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_39":{"id":"db7a81234a79_39","name":"6a6d","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:db7a81234a79_39.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:42e8a46b93cc303215116bc651209a7f":{"id":"42e8a46b93cc303215116bc651209a7f","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Nadam.py","__typename":"MediaResource"},"$Paragraph:db7a81234a79_39.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:42e8a46b93cc303215116bc651209a7f","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:db7a81234a79_40":{"id":"db7a81234a79_40","name":"bdd1","type":"H3","href":null,"layout":null,"metadata":null,"text":"Properties of Adam","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_41":{"id":"db7a81234a79_41","name":"0912","type":"P","href":null,"layout":null,"metadata":null,"text":"Here I list some of the properties of Adam, for proof that these are true refer to the paper.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_42":{"id":"db7a81234a79_42","name":"09fe","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Actual step size taken by the Adam in each iteration is approximately bounded the step size hyper-parameter. This property add intuitive understanding to previous unintuitive learning rate hyper-parameter.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_43":{"id":"db7a81234a79_43","name":"6b14","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Step size of Adam update rule is invariant to the magnitude of the gradient, which helps a lot when going through areas with tiny gradients (such as saddle points or ravines). In these areas SGD struggles to quickly navigate through them.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_44":{"id":"db7a81234a79_44","name":"fc99","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Adam was designed to combine the advantages of Adagrad, which works well with sparse gradients, and RMSprop, which works well in on-line settings. Having both of these enables us to use Adam for broader range of tasks. Adam can also be looked at as the combination of RMSprop and SGD with momentum.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_45":{"id":"db7a81234a79_45","name":"9187","type":"H3","href":null,"layout":null,"metadata":null,"text":"Problems with Adam","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_46":{"id":"db7a81234a79_46","name":"3409","type":"P","href":null,"layout":null,"metadata":null,"text":"When Adam was first introduced, people got very excited about its power. Paper contained some very optimistic charts, showing huge performance gains in terms of speed of training:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_47":{"id":"db7a81234a79_47","name":"49d1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*X9gB3l_Wh5owNPCUsaYQVQ.png","typename":"ImageMetadata"},"text":"source: original Adam paper","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*X9gB3l_Wh5owNPCUsaYQVQ.png":{"id":"1*X9gB3l_Wh5owNPCUsaYQVQ.png","originalHeight":338,"originalWidth":342,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_48":{"id":"db7a81234a79_48","name":"d3dd","type":"P","href":null,"layout":null,"metadata":null,"text":"Then, Nadam paper presented diagrams that showed even better results:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_49":{"id":"db7a81234a79_49","name":"e07c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*HgfHvTeQpx_hIHfhiDmGXg.png","typename":"ImageMetadata"},"text":"source: Nadam paper","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*HgfHvTeQpx_hIHfhiDmGXg.png":{"id":"1*HgfHvTeQpx_hIHfhiDmGXg.png","originalHeight":208,"originalWidth":662,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_50":{"id":"db7a81234a79_50","name":"4c52","type":"P","href":null,"layout":null,"metadata":null,"text":"However, after a while people started noticing that despite superior training time, Adam in some areas does not converge to an optimal solution, so for some tasks (such as image classification on popular CIFAR datasets) state-of-the-art results are still only achieved by applying SGD with momentum. More than that Wilson et. al [9] showed in their paper ‘The marginal value of adaptive gradient methods in machine learning’ that adaptive methods (such as Adam or Adadelta) do not generalize as well as SGD with momentum when tested on a diverse set of deep learning tasks, discouraging people to use popular optimization algorithms. A lot of research has been done since to analyze the poor generalization of Adam trying to get it to close the gap with SGD.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_50.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_50.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_50.markups.0":{"type":"A","start":204,"end":218,"href":"https:\u002F\u002Fwww.cs.toronto.edu\u002F~kriz\u002Fcifar.html","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_50.markups.1":{"type":"A","start":464,"end":472,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1212.5701","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_51":{"id":"db7a81234a79_51","name":"a6c8","type":"P","href":null,"layout":null,"metadata":null,"text":"Nitish Shirish Keskar and Richard Socher in their paper ‘Improving Generalization Performance by Switching from Adam to SGD’ [5] also showed that by switching to SGD during training training they’ve been able to obtain better generalization power than when using Adam alone. They proposed a simple fix which uses a very simple idea. They’ve noticed that in earlier stages of training Adam still outperforms SGD but later the learning saturates. They proposed simple strategy which they called SWATS in which they start training deep neural network with Adam but then switch to SGD when certain criteria hits. They managed to achieve results comparable to SGD with momentum.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_51.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_51.markups.0":{"type":"STRONG","start":493,"end":499,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_52":{"id":"db7a81234a79_52","name":"a54f","type":"H3","href":null,"layout":null,"metadata":null,"text":"On the convergence of Adam","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_53":{"id":"db7a81234a79_53","name":"317f","type":"P","href":null,"layout":null,"metadata":null,"text":"One big thing with figuring out what’s wrong with Adam was analyzing it’s convergence. The authors proved that Adam converges to the global minimum in the convex settings in their original paper, however, several papers later found out that their proof contained a few mistakes. Block et. al [7] claimed that they have spotted errors in the original convergence analysis, but still proved that the algorithm converges and provided proof in their paper. Another recent article from Google employees was presented at ICLR 2018 and even won best paper award. To go deeper to their paper I should first describe the framework used by Adam authors for proving that it converges for convex functions.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_53.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_53.markups.0":{"type":"A","start":515,"end":524,"href":"https:\u002F\u002Ficlr.cc\u002FConferences\u002F2018\u002FSchedule?type=Poster","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_54":{"id":"db7a81234a79_54","name":"0077","type":"P","href":null,"layout":null,"metadata":null,"text":"In 2003 Martin Zinkevich introduced Online Convex Programming problem [8]. In the presented settings, we have a sequence of convex functions c1, c2, etc (Loss function executed in ith mini-batch in the case of deep learning optimization). The algorithm, that solves the problem (Adam) in each timestamp t chooses a point x[t] (parameters of the model) and then receives the loss function c for the current timestamp. This setting translates to a lot of real world problems, for examples read the introduction of the paper. For understanding how good the algorithm works, the value of regret of the algorithm after T rounds is defined as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_55":{"id":"db7a81234a79_55","name":"329a","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*L6RkC6aQ0LTqJt81TF_XMw.png","typename":"ImageMetadata"},"text":"Regret of the algorithm in the online convex programming","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*L6RkC6aQ0LTqJt81TF_XMw.png":{"id":"1*L6RkC6aQ0LTqJt81TF_XMw.png","originalHeight":142,"originalWidth":586,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_56":{"id":"db7a81234a79_56","name":"6660","type":"P","href":null,"layout":null,"metadata":null,"text":"where R is regret, c is the loss function on tth mini batch, w is vector of model parameters (weights), and w star is optimal value of weight vector. Our goal is to prove that the regret of algorithm is R(T) = O(T) or less, which means that on average the model converges to an optimal solution. Martin Zinkevich in his paper proved that gradient descent converges to optimal solutions in this setting, using the property of the convex functions:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_57":{"id":"db7a81234a79_57","name":"10b4","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*BAdCFeu94aDnu7K6FNhwLA.png","typename":"ImageMetadata"},"text":"Well-known property of convex functions.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BAdCFeu94aDnu7K6FNhwLA.png":{"id":"1*BAdCFeu94aDnu7K6FNhwLA.png","originalHeight":84,"originalWidth":692,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_58":{"id":"db7a81234a79_58","name":"2a83","type":"P","href":null,"layout":null,"metadata":null,"text":"The same approach and framework used Adam authors to prove that their algorithm converges to an optimal solutions. Reddi et al. [3] spotted several mistakes in their proof, the main one lying in the value, which appears in both Adam and Improving Adam’s proof of convergence papers:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_59":{"id":"db7a81234a79_59","name":"bd72","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*jyxdh8fuHjuN3Df5BNxknQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jyxdh8fuHjuN3Df5BNxknQ.png":{"id":"1*jyxdh8fuHjuN3Df5BNxknQ.png","originalHeight":132,"originalWidth":227,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_60":{"id":"db7a81234a79_60","name":"022d","type":"P","href":null,"layout":null,"metadata":null,"text":"Where V is defined as an abstract function that scales learning rate for parameters which differs for each individual algorithms. For Adam it’s the moving averages of past squared gradients, for Adagrad it’s the sum of all past and current gradients, for SGD it’s just 1. The authors found that in order for proof to work, this value has to be positive. It’s easy to see, that for SGD and Adagrad it’s always positive, however, for Adam(or RMSprop), the value of V can act unexpectedly. They also presented an example in which Adam fails to converge:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_61":{"id":"db7a81234a79_61","name":"58f1","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*0LekjJcPuqlig4h0DWd2Ng.png","typename":"ImageMetadata"},"text":"Adam fails on this sequence","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*0LekjJcPuqlig4h0DWd2Ng.png":{"id":"1*0LekjJcPuqlig4h0DWd2Ng.png","originalHeight":135,"originalWidth":539,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_62":{"id":"db7a81234a79_62","name":"b25d","type":"P","href":null,"layout":null,"metadata":null,"text":"For this sequence, it’s easy to see that the optimal solution is x = -1, however, how authors show, Adam converges to highly sub-optimal value of x = 1. The algorithm obtains the large gradient C once every 3 steps, and while the other 2 steps it observes the gradient -1 , which moves the algorithm in the wrong direction. Since values of step size are often decreasing over time, they proposed a fix of keeping the maximum of values V and use it instead of the moving average to update parameters. The resulting algorithm is called Amsgrad. We can confirm their experiment with this short notebook I created, which shows different algorithms converge on the function sequence defined above.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_62.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_62.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_62.markups.0":{"type":"A","start":580,"end":609,"href":"https:\u002F\u002Fgithub.com\u002Fbushaev\u002Fadam\u002Fblob\u002Fmaster\u002FAdamNonConvergence.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_62.markups.1":{"type":"STRONG","start":534,"end":543,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_63":{"id":"db7a81234a79_63","name":"d885","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"Amsgrad without bias correction","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:db7a81234a79_63.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:7a4d9c819ce0ae3edfb8ac247a418ac8":{"id":"7a4d9c819ce0ae3edfb8ac247a418ac8","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Amsgrad.py","__typename":"MediaResource"},"$Paragraph:db7a81234a79_63.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:7a4d9c819ce0ae3edfb8ac247a418ac8","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:db7a81234a79_64":{"id":"db7a81234a79_64","name":"e710","type":"P","href":null,"layout":null,"metadata":null,"text":"How much does it help in practice with real-world data ? Sadly, I haven’t seen one case where it would help get better results than Adam. Filip Korzeniowski in his post describes experiments with Amsgrad, which show similar results to Adam. Sylvain Gugger and Jeremy Howard in their post show that in their experiments Amsgrad actually performs even worse that Adam. Some reviewers of the paper also pointed out that the issue may lie not in Adam itself but in framework, which I described above, for convergence analysis, which does not allow for much hyper-parameter tuning.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_64.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_64.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_64.markups.0":{"type":"A","start":160,"end":168,"href":"https:\u002F\u002Ffdlm.github.io\u002Fpost\u002Famsgrad\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_64.markups.1":{"type":"A","start":274,"end":287,"href":"http:\u002F\u002Fwww.fast.ai\u002F2018\u002F07\u002F02\u002Fadam-weight-decay\u002F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_65":{"id":"db7a81234a79_65","name":"0884","type":"H3","href":null,"layout":null,"metadata":null,"text":"Weight decay with Adam","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_66":{"id":"db7a81234a79_66","name":"20ce","type":"P","href":null,"layout":null,"metadata":null,"text":"One paper that actually turned out to help Adam is ‘Fixing Weight Decay Regularization in Adam’ [4] by Ilya Loshchilov and Frank Hutter. This paper contains a lot of contributions and insights into Adam and weight decay. First, they show that despite common belief L2 regularization is not the same as weight decay, though it is equivalent for stochastic gradient descent. The way weight decay was introduced back in 1988 is:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_66.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_66.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_66.markups.0":{"type":"A","start":265,"end":282,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FRegularization_(mathematics)","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_66.markups.1":{"type":"A","start":302,"end":314,"href":"https:\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F156-comparing-biases-for-minimal-network-construction-with-back-propagation.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_67":{"id":"db7a81234a79_67","name":"fd68","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*FA9xcKWO_htmF8rFNadgQQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*FA9xcKWO_htmF8rFNadgQQ.png":{"id":"1*FA9xcKWO_htmF8rFNadgQQ.png","originalHeight":58,"originalWidth":402,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_68":{"id":"db7a81234a79_68","name":"5596","type":"P","href":null,"layout":null,"metadata":null,"text":"Where lambda is weight decay hyper parameter to tune. I changed notation a little bit to stay consistent with the rest of the post. As defined above, weight decay is applied in the last step, when making the weight update, penalizing large weights. The way it’s been traditionally implemented for SGD is through L2 regularization in which we modify the cost function to contain the L2 norm of the weight vector:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_69":{"id":"db7a81234a79_69","name":"aaa8","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*p7eol4iX8TlWapsR6dk5SA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*p7eol4iX8TlWapsR6dk5SA.png":{"id":"1*p7eol4iX8TlWapsR6dk5SA.png","originalHeight":78,"originalWidth":362,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_70":{"id":"db7a81234a79_70","name":"a0cb","type":"P","href":null,"layout":null,"metadata":null,"text":"Historically, stochastic gradient descent methods inherited this way of implementing the weight decay regularization and so did Adam. However, L2 regularization is not equivalent to weight decay for Adam. When using L2 regularization the penalty we use for large weights gets scaled by moving average of the past and current squared gradients and therefore weights with large typical gradient magnitude are regularized by a smaller relative amount than other weights. In contrast, weight decay regularizes all weights by the same factor. To use weight decay with Adam we need to modify the update rule as follows:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_71":{"id":"db7a81234a79_71","name":"34d5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*NB9-GHeP0-pMBjSw8o_GDA.png","typename":"ImageMetadata"},"text":"Adam update rule with weight decay","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*NB9-GHeP0-pMBjSw8o_GDA.png":{"id":"1*NB9-GHeP0-pMBjSw8o_GDA.png","originalHeight":86,"originalWidth":364,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_72":{"id":"db7a81234a79_72","name":"d658","type":"P","href":null,"layout":null,"metadata":null,"text":"Having show that these types of regularization differ for Adam, authors continue to show how well it works with both of them. The difference in results is shown very well with the diagram from the paper:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_73":{"id":"db7a81234a79_73","name":"abb9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*DsrkV06bd5UO-H_xySyfIQ.png","typename":"ImageMetadata"},"text":"The Top-1 test error of ResNet on CIFAR-10 measured after 100 epochs","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*DsrkV06bd5UO-H_xySyfIQ.png":{"id":"1*DsrkV06bd5UO-H_xySyfIQ.png","originalHeight":280,"originalWidth":725,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_74":{"id":"db7a81234a79_74","name":"23a1","type":"P","href":null,"layout":null,"metadata":null,"text":"These diagrams show relation between learning rate and regularization method. The color represent high low the test error is for this pair of hyper parameters. As we can see above not only Adam with weight decay gets much lower test error it actually helps in decoupling learning rate and regularization hyper-parameter. On the left picture we can the that if we change of the parameters, say learning rate, then in order to achieve optimal point again we’d need to change L2 factor as well, showing that these two parameters are interdependent. This dependency contributes to the fact hyper-parameter tuning is a very difficult task sometimes. On the right picture we can see that as long as we stay in some range of optimal values for one the parameter, we can change another one independently.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_75":{"id":"db7a81234a79_75","name":"c696","type":"P","href":null,"layout":null,"metadata":null,"text":"Another contribution by the author of the paper shows that optimal value to use for weight decay actually depends on number of iteration during training. To deal with this fact they proposed a simple adaptive formula for setting weight decay:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_76":{"id":"db7a81234a79_76","name":"d12e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*dHuVxZsbg21AXIWAKOtmbw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*dHuVxZsbg21AXIWAKOtmbw.png":{"id":"1*dHuVxZsbg21AXIWAKOtmbw.png","originalHeight":103,"originalWidth":224,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:db7a81234a79_77":{"id":"db7a81234a79_77","name":"35ca","type":"P","href":null,"layout":null,"metadata":null,"text":"where b is batch size, B is the total number of training points per epoch and T is the total number of epochs. This replaces the lambda hyper-parameter lambda by the new one lambda normalized.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_78":{"id":"db7a81234a79_78","name":"bd39","type":"P","href":null,"layout":null,"metadata":null,"text":"The authors didn’t even stop there, after fixing weight decay they tried to apply the learning rate schedule with warm restarts with new version of Adam. Warm restarts helped a great deal for stochastic gradient descent, I talk more about it in my post ‘Improving the way we work with learning rate’. But previously Adam was a lot behind SGD. With new weight decay Adam got much better results with restarts, but it’s still not as good as SGDR.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_78.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_78.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_78.markups.0":{"type":"A","start":114,"end":127,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1608.03983","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_78.markups.1":{"type":"A","start":254,"end":298,"href":"https:\u002F\u002Ftechburst.io\u002Fimproving-the-way-we-work-with-learning-rate-5e99554f163b","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_79":{"id":"db7a81234a79_79","name":"8204","type":"H3","href":null,"layout":null,"metadata":null,"text":"ND-Adam","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_80":{"id":"db7a81234a79_80","name":"3ce8","type":"P","href":null,"layout":null,"metadata":null,"text":"One more attempt at fixing Adam, that I haven’t seen much in practice is proposed by Zhang et. al in their paper ‘Normalized Direction-preserving Adam’ [2]. The paper notices two problems with Adam that may cause worse generalization:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_81":{"id":"db7a81234a79_81","name":"79a3","type":"OLI","href":null,"layout":null,"metadata":null,"text":"The updates of SGD lie in the span of historical gradients, whereas it is not the case for Adam. This difference has also been observed in already mentioned paper [9].","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_82":{"id":"db7a81234a79_82","name":"b41f","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Second, while the magnitudes of Adam parameter updates are invariant to descaling of the gradient, the effect of the updates on the same overall network function still varies with the magnitudes of parameters.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_83":{"id":"db7a81234a79_83","name":"6fcd","type":"P","href":null,"layout":null,"metadata":null,"text":"To address these problems the authors propose the algorithm they call Normalized direction-preserving Adam. The algorithms tweaks Adam in the following ways. First, instead of estimating the average gradient magnitude for each individual parameter, it estimates the average squared L2 norm of the gradient vector. Since now V is a scalar value and M is the vector in the same direction as W, the direction of the update is the negative direction of m and thus is in the span of the historical gradients of w. For the second the algorithms before using gradient projects it onto the unit sphere and then after the update, the weights get normalized by their norm. For more details follow their paper.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_84":{"id":"db7a81234a79_84","name":"4cf5","type":"H3","href":null,"layout":null,"metadata":null,"text":"Conclusion","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_85":{"id":"db7a81234a79_85","name":"be8d","type":"P","href":null,"layout":null,"metadata":null,"text":"Adam is definitely one of the best optimization algorithms for deep learning and its popularity is growing very fast. While people have noticed some problems with using Adam in certain areas, researches continue to work on solutions to bring Adam results to be on par with SGD with momentum.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_86":{"id":"db7a81234a79_86","name":"a0fa","type":"H3","href":null,"layout":null,"metadata":null,"text":"References","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_87":{"id":"db7a81234a79_87","name":"3d88","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Diederik P. Kingma and Jimmy Lei Ba. Adam : A method for stochastic optimization. 2014. arXiv:1412.6980v9","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_87.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_87.markups.0":{"type":"A","start":37,"end":80,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1412.6980","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_88":{"id":"db7a81234a79_88","name":"1385","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Zijun Zhang et al. Normalized direction-preserving Adam. 2017. arXiv:1709.04546v2","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_88.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_88.markups.0":{"type":"A","start":19,"end":55,"href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1709.04546.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_89":{"id":"db7a81234a79_89","name":"b2b7","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Sashank J. Reddi, Satyen Kale, Sanjiv Kumar. On the Convergence of Adam and Beyond. 2018.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_89.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_89.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_89.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_89.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_89.markups.0":{"type":"A","start":0,"end":16,"href":"https:\u002F\u002Fopenreview.net\u002Fprofile?email=sashank%40google.com","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_89.markups.1":{"type":"A","start":18,"end":29,"href":"https:\u002F\u002Fopenreview.net\u002Fprofile?email=satyenkale%40google.com","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_89.markups.2":{"type":"A","start":31,"end":43,"href":"https:\u002F\u002Fopenreview.net\u002Fprofile?email=sanjivk%40google.com","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_89.markups.3":{"type":"A","start":45,"end":82,"href":"https:\u002F\u002Fopenreview.net\u002Fforum?id=ryQu7f-RZ","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_90":{"id":"db7a81234a79_90","name":"b17e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Ilya Loshchilov, Frank Hutter. Fixing Weight Decay Regularization in Adam. 2017. arXiv:1711.05101v2","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_90.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_90.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_90.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_90.markups.0":{"type":"A","start":0,"end":15,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fcs?searchtype=author&query=Loshchilov%2C+I","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_90.markups.1":{"type":"A","start":17,"end":29,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fcs?searchtype=author&query=Hutter%2C+F","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_90.markups.2":{"type":"A","start":31,"end":73,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1711.05101","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_91":{"id":"db7a81234a79_91","name":"c1db","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Nitish Shirish Keskar, Richard Socher. Improving Generalization Performance by Switching from Adam to SGD. 2017 arXiv:1712.07628v1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_91.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_91.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_91.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_91.markups.0":{"type":"A","start":0,"end":21,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fcs?searchtype=author&query=Keskar%2C+N+S","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_91.markups.1":{"type":"A","start":23,"end":37,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fcs?searchtype=author&query=Socher%2C+R","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_91.markups.2":{"type":"A","start":39,"end":105,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1712.07628","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_92":{"id":"db7a81234a79_92","name":"277e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Timothy Dozat. Incorporating Nesterov momentum into Adam. 2016.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_92.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_92.markups.0":{"type":"A","start":15,"end":56,"href":"https:\u002F\u002Fopenreview.net\u002Fpdf?id=OM0jvwB8jIp57ZJjtNEZ","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_93":{"id":"db7a81234a79_93","name":"07d9","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Sebastian Bock, Josef Goppold, Martin Weiß. An improvement of the convergence proof of the ADAM-Optimizer. 2018. arXiv:1804.10587v1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_93.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_93.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_93.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_93.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_93.markups.0":{"type":"A","start":0,"end":14,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fcs?searchtype=author&query=Bock%2C+S","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_93.markups.1":{"type":"A","start":16,"end":29,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fcs?searchtype=author&query=Goppold%2C+J","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_93.markups.2":{"type":"A","start":31,"end":42,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fcs?searchtype=author&query=Wei%C3%9F%2C+M","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_93.markups.3":{"type":"A","start":44,"end":105,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1804.10587","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_94":{"id":"db7a81234a79_94","name":"c8ba","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Martin Zinkevich. Online Convex Programming and Generalized Infinitesimal Gradient Ascent. 2003.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_94.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_94.markups.0":{"type":"A","start":18,"end":89,"href":"http:\u002F\u002Fwww.cs.cmu.edu\u002F~maz\u002Fpublications\u002Ftechconvex.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_95":{"id":"db7a81234a79_95","name":"210d","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht. The Marginal Value of Adaptive Gradient Methods in Machine Learning. 2017. arXiv:1705.08292v2","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_95.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_95.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_95.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_95.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_95.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_95.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_95.markups.0":{"type":"A","start":0,"end":15,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fstat?searchtype=author&query=Wilson%2C+A+C","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_95.markups.1":{"type":"A","start":17,"end":32,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fstat?searchtype=author&query=Roelofs%2C+R","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_95.markups.2":{"type":"A","start":34,"end":48,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fstat?searchtype=author&query=Stern%2C+M","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_95.markups.3":{"type":"A","start":50,"end":63,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fstat?searchtype=author&query=Srebro%2C+N","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_95.markups.4":{"type":"A","start":65,"end":79,"href":"https:\u002F\u002Farxiv.org\u002Fsearch\u002Fstat?searchtype=author&query=Recht%2C+B","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_95.markups.5":{"type":"A","start":81,"end":148,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1705.08292","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_96":{"id":"db7a81234a79_96","name":"091c","type":"OLI","href":null,"layout":null,"metadata":null,"text":"John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_96.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_96.markups.0":{"type":"A","start":42,"end":118,"href":"http:\u002F\u002Fwww.jmlr.org\u002Fpapers\u002Fvolume12\u002Fduchi11a\u002Fduchi11a.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:db7a81234a79_97":{"id":"db7a81234a79_97","name":"f6d4","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: neural networks for machine learning, 4(2):26–31, 2012.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:db7a81234a79_97.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:db7a81234a79_97.markups.0":{"type":"A","start":58,"end":122,"href":"https:\u002F\u002Fwww.coursera.org\u002Flearn\u002Fneural-networks\u002Flecture\u002FYQHki\u002Frmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"Tag:optimization":{"id":"optimization","displayTitle":"Optimization","__typename":"Tag"},"Tag:towards-data-science":{"id":"towards-data-science","displayTitle":"Towards Data Science","__typename":"Tag"},"1eca0103fff3":{"topicId":"1eca0103fff3","name":"Machine Learning","__typename":"Topic","slug":"machine-learning"},"ae5d4995e225":{"topicId":"ae5d4995e225","name":"Data Science","__typename":"Topic","slug":"data-science"},"$Post:6be9a291375c.postResponses":{"count":8,"__typename":"PostResponses"},"$Post:6be9a291375c.previewContent":{"subtitle":"Adam is an adaptive learning rate optimization algorithm that’s been designed specifically for training deep neural networks. First…","__typename":"PreviewContent"}}</script><script src="./Adam — latest trends in deep learning optimization._files/manifest.51f1343a.js"></script><script src="./Adam — latest trends in deep learning optimization._files/vendors_main.314590f6.chunk.js"></script><script src="./Adam — latest trends in deep learning optimization._files/main.c8f95410.chunk.js"></script><script src="./Adam — latest trends in deep learning optimization._files/vendors_instrumentation.b4ca681f.chunk.js"></script>
<script src="./Adam — latest trends in deep learning optimization._files/instrumentation.e5b0d1a6.chunk.js"></script>
<script src="./Adam — latest trends in deep learning optimization._files/reporting.83f8bbe2.chunk.js"></script>
<script src="./Adam — latest trends in deep learning optimization._files/vendors_AMPPost_CollectionHomepage_CollectionHomepagePreview_CollectionNewShortformEditor_Collection_37c9fa1e.77e6fe9c.chunk.js"></script>
<script src="./Adam — latest trends in deep learning optimization._files/vendors_AMPPost_DebugCachedPost_Post_SequencePost_Series.0bf77567.chunk.js"></script>
<script src="./Adam — latest trends in deep learning optimization._files/AMPPost_CollectionHomepage_CollectionHomepagePreview_CollectionNewShortformEditor_CollectionPostShor_3fa3f642.15e3ccc0.chunk.js"></script>
<script src="./Adam — latest trends in deep learning optimization._files/AMPPost_CollectionHomepage_CollectionHomepagePreview_DebugCachedPost_PackageBuilder_Post_SequenceLib_32b7ff81.4df3a8c2.chunk.js"></script>
<script src="./Adam — latest trends in deep learning optimization._files/Post.c0b4f8b2.chunk.js"></script><script>window.main();</script></body></html>