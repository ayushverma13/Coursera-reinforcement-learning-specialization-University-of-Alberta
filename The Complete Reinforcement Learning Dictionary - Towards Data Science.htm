<!DOCTYPE html>
<!-- saved from url=(0090)https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/branch-latest.min.js"></script><script async="" src="https://www.google-analytics.com/analytics.js"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><script defer="" src="https://cdn.optimizely.com/js/16180790160.js"></script><title>The Complete Reinforcement Learning Dictionary - Towards Data Science</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2019-11-24T08:28:48.449Z"><meta data-rh="true" name="title" content="The Complete Reinforcement Learning Dictionary - Towards Data Science"><meta data-rh="true" property="og:title" content="The Complete Reinforcement Learning Dictionary"><meta data-rh="true" property="twitter:title" content="The Complete Reinforcement Learning Dictionary"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/e16230b7d24e"><meta data-rh="true" property="al:android:url" content="medium://p/e16230b7d24e"><meta data-rh="true" property="al:ios:url" content="medium://p/e16230b7d24e"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Whenever I begin learning a subject which is new to me, I find the hardest thing to cope with is its new terminology. Every field have many terms and definitions which are completely obscure to an…"><meta data-rh="true" property="og:description" content="The Reinforcement Learning Terminology, A to Z"><meta data-rh="true" property="twitter:description" content="The Reinforcement Learning Terminology, A to Z"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://towardsdatascience.com/@shakedzy"><meta data-rh="true" name="twitter:creator" content="@shakedzy"><meta data-rh="true" name="author" content="Shaked Zychlinski"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="14 min read"><meta data-rh="true" name="parsely-post-id" content="e16230b7d24e"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/m2.css"><link data-rh="true" rel="author" href="https://towardsdatascience.com/@shakedzy"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/e16230b7d24e"><link data-rh="true" rel="icon" href="https://miro.medium.com/fit/c/256/256/1*ChFMdf--f5jbm-AYv6VdYA@2x.png"><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e","dateCreated":"2019-02-23T21:31:51.367Z","datePublished":"2019-02-23T21:31:51.367Z","dateModified":"2019-11-24T08:28:48.726Z","headline":"The Complete Reinforcement Learning Dictionary - Towards Data Science","name":"The Complete Reinforcement Learning Dictionary - Towards Data Science","description":"Whenever I begin learning a subject which is new to me, I find the hardest thing to cope with is its new terminology. Every field have many terms and definitions which are completely obscure to an…","identifier":"e16230b7d24e","keywords":["Lite:true","Tag:Machine Learning","Tag:Reinforcement Learning","Tag:Artificial Intelligence","Tag:Data Science","Tag:Algorithms","Topic:Artificial Intelligence","Topic:Machine Learning","Topic:Data Science","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_UGC_UNENROLLED","LayerCake:3"],"author":{"@type":"Person","name":"Shaked Zychlinski","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@shakedzy"},"creator":["Shaked Zychlinski"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F330\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e","isAccessibleForFree":"False","hasPart":{"@type":"WebPageElement","isAccessibleForFree":"False","cssSelector":".meteredContent"}}</script><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="https://cdn.optimizely.com/js/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="496" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:absolute}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ag{max-width:1192px}.ah{min-width:0}.ai{width:100%}.aj{height:65px}.am{flex:1 0 auto}.an{visibility:hidden}.ao{margin-left:16px}.ap{display:none}.ar{color:rgba(90, 118, 144, 1)}.as{fill:rgba(102, 138, 170, 1)}.at{font-size:inherit}.au{border:inherit}.av{font-family:inherit}.aw{letter-spacing:inherit}.ax{font-weight:inherit}.ay{padding:0}.az{margin:0}.ba:hover{cursor:pointer}.bb:hover{color:rgba(84, 108, 131, 1)}.bc:hover{fill:rgba(90, 118, 144, 1)}.bd:focus{outline:none}.be:disabled{cursor:default}.bf:disabled{color:rgba(140, 93, 255, 0.5)}.bg:disabled{fill:rgba(140, 93, 255, 0.5)}.bh{flex:0 0 auto}.bi{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.bj{font-style:normal}.bk{line-height:20px}.bl{font-size:15.8px}.bm{letter-spacing:0px}.bn{color:rgba(0, 0, 0, 0.54)}.bo{fill:rgba(0, 0, 0, 0.54)}.bp{justify-content:flex-end}.bq{margin-top:16px}.br{margin-bottom:16px}.bs{display:inherit}.bt{max-width:210px}.bu{text-overflow:ellipsis}.bv{overflow:hidden}.bw{white-space:nowrap}.bx{display:inline-block}.by{border:none}.bz{outline:none}.ca{font:inherit}.cb{font-size:16px}.cc{opacity:0}.cd{position:relative}.ce{width:0px}.cf{transition:width 140ms ease-in}.cg{color:inherit}.ch{fill:inherit}.ci:hover{color:rgba(0, 0, 0, 0.9)}.cj:hover{fill:rgba(0, 0, 0, 0.9)}.ck:disabled{color:rgba(0, 0, 0, 0.54)}.cl:disabled{fill:rgba(0, 0, 0, 0.54)}.cm{margin-right:10px}.cq{margin-right:16px}.cr{margin:15px 0}.cs{padding:4px 12px}.ct{color:rgba(0, 0, 0, 0.84)}.cu{background:0}.cv{border-color:rgba(0, 0, 0, 0.54)}.cw:hover{color:rgba(0, 0, 0, 0.97)}.cx:hover{fill:rgba(0, 0, 0, 0.97)}.cy:hover{border-color:rgba(0, 0, 0, 0.84)}.cz:disabled{fill:rgba(0, 0, 0, 0.76)}.da:disabled{border-color:rgba(0, 0, 0, 0.2)}.db:disabled{cursor:inherit}.dc:disabled:hover{color:rgba(0, 0, 0, 0.54)}.dd:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.de:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.df{border-radius:4px}.dg{border-width:1px}.dh{border-style:solid}.di{box-sizing:border-box}.dj{text-decoration:none}.dk{padding-bottom:10px}.dl{padding-top:10px}.dm{border-radius:50%}.dn{height:32px}.do{width:32px}.dp{border-top:none}.dq{background-color:rgba(53, 88, 118, 1)}.dr{height:54px}.ds{margin-right:40px}.dt{height:36px}.du{width:100px}.dv{overflow:auto}.dw{flex:0 1 auto}.dx{list-style-type:none}.dy{line-height:40px}.dz{overflow-x:auto}.ea{align-items:flex-start}.eb{margin-top:20px}.ec{padding-top:20px}.ed{height:80px}.ee{height:20px}.ef{margin-right:15px}.eg{margin-left:15px}.eh:first-child{margin-left:0}.ei{min-width:1px}.ej{background-color:rgba(197, 210, 225, 1)}.ek{font-weight:300}.el{font-size:15px}.em{color:rgba(197, 210, 225, 1)}.en{text-transform:uppercase}.eo{letter-spacing:1px}.ep:hover{color:rgba(251, 255, 255, 1)}.eq:hover{fill:rgba(233, 241, 250, 1)}.er:disabled{color:rgba(150, 171, 191, 1)}.es:disabled{fill:rgba(150, 171, 191, 1)}.et{margin-bottom:0px}.eu{height:119px}.ex{padding-left:24px}.ey{padding-right:24px}.ez{margin-left:auto}.fa{margin-right:auto}.fb{max-width:728px}.fc{flex-direction:column}.fd{top:calc(100vh + 100px)}.fe{bottom:calc(100vh + 100px)}.ff{width:10px}.fg{pointer-events:none}.fh{word-break:break-word}.fi{word-wrap:break-word}.fj:after{display:block}.fk:after{content:""}.fl:after{clear:both}.fm{max-width:680px}.fn{line-height:1.23}.fo{letter-spacing:0}.fp{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.ga{margin-bottom:-0.27em}.gg{line-height:1.394}.gr{margin-bottom:-0.42em}.gx{margin-top:32px}.gy{justify-content:space-between}.hc{height:48px}.hd{width:48px}.he{margin-left:12px}.hf{margin-bottom:2px}.hh{max-height:20px}.hi{display:-webkit-box}.hj{-webkit-line-clamp:1}.hk{-webkit-box-orient:vertical}.hl:hover{text-decoration:underline}.hm{margin-left:8px}.hn{padding:0px 8px}.ho{border-color:rgba(102, 138, 170, 1)}.hp:hover{border-color:rgba(90, 118, 144, 1)}.hq{line-height:18px}.hr{align-items:flex-end}.hz{padding-right:6px}.ia{margin-right:8px}.ib{fill:rgba(0, 0, 0, 0.76)}.ic{margin-right:-6px}.id{max-width:1920px}.ij{clear:both}.ik{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.il{cursor:zoom-in}.im{z-index:auto}.in{transition:opacity 100ms 400ms}.io{height:100%}.ip{will-change:transform}.iq{transform:translateZ(0)}.ir{margin:auto}.is{background-color:rgba(0, 0, 0, 0.05)}.it{padding-bottom:75%}.iu{height:0}.iv{filter:blur(20px)}.iw{transform:scale(1.1)}.ix{visibility:visible}.iy{background:rgba(255, 255, 255, 1)}.iz{line-height:1.58}.ja{letter-spacing:-0.004em}.jb{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.jk{margin-bottom:-0.46em}.jl{letter-spacing:-0.003em}.jo{list-style-type:disc}.jp{margin-left:30px}.jq{padding-left:0px}.jr{font-style:italic}.js{background-repeat:repeat-x}.jt{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.ju{background-size:1px 1px}.jv{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.kb{font-family:medium-content-slab-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.kc{font-size:28px}.kd{color:rgba(0, 0, 0, 0.97)}.ke{margin-top:30px}.kf{text-align:center}.kg:before{content:"..."}.kh:before{letter-spacing:0.6em}.ki:before{text-indent:0.6em}.kj:before{font-style:italic}.kk:before{line-height:1.4}.kl{line-height:1.12}.km{letter-spacing:-0.022em}.kn{font-weight:600}.kw{margin-bottom:-0.28em}.lc{font-weight:700}.ld{max-width:577px}.le{padding-bottom:9.705372616984402%}.lf{max-width:1200px}.lg{padding-bottom:10.666666666666666%}.lh{padding-bottom:17.331022530329292%}.li{padding-bottom:10.333333333333332%}.lj{max-width:1154px}.lk{padding-bottom:11.166666666666668%}.ll{will-change:opacity}.lm{position:fixed}.ln{width:188px}.lo{left:50%}.lp{transform:translateX(406px)}.lq{top:calc(65px + 54px + 14px)}.lt{top:calc(65px + 54px + 40px)}.lv{width:131px}.lw{padding-bottom:28px}.lx{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.ly{font-size:18px}.lz{padding-bottom:20px}.ma{padding-top:2px}.mb{max-height:120px}.mc{-webkit-line-clamp:6}.md{padding-top:28px}.me{margin-bottom:19px}.mf{margin-left:-3px}.ml{outline:0}.mm{border:0}.mn{user-select:none}.mo{cursor:pointer}.mp> svg{pointer-events:none}.mq:active{border-style:none}.mr{-webkit-user-select:none}.ms:focus{fill:rgba(0, 0, 0, 0.54)}.mt:hover{fill:rgba(0, 0, 0, 0.54)}.nb button{text-align:left}.nc{margin-top:40px}.nd{flex-wrap:wrap}.ne{margin-top:25px}.nf{margin-bottom:8px}.ng{border-radius:3px}.nh{padding:5px 10px}.ni{background:rgba(0, 0, 0, 0.05)}.nj{line-height:22px}.nk{margin-top:15px}.nl{flex-direction:row}.nm{max-width:155px}.ns{border:1px solid rgba(0, 0, 0, 0.1)}.nt{height:60px}.nu{width:60px}.oh:hover{border-color:rgba(0, 0, 0, 0.54)}.oi:active{border-style:solid}.oj{z-index:2}.ol{top:1px}.or{padding-right:8px}.os{padding-top:32px}.ot{border-top:1px solid rgba(0, 0, 0, 0.1)}.ou{margin-bottom:25px}.ov{margin-bottom:32px}.ow{min-height:80px}.pb{width:80px}.pc{padding-left:102px}.pe{letter-spacing:0.05em}.pf{margin-bottom:6px}.pg{line-height:36px}.ph{max-width:555px}.pi{max-width:450px}.pj{line-height:24px}.pl{max-width:550px}.pm{padding-top:25px}.pn{color:rgba(0, 0, 0, 0.76)}.po{opacity:1}.pp{padding:20px}.pq{border:1px solid rgba(102, 138, 170, 1)}.pr{margin-top:64px}.ps{background-color:rgba(0, 0, 0, 0.02)}.pt{padding:60px 0}.pu{background-color:rgba(0, 0, 0, 0.9)}.ql{padding-bottom:48px}.qm{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.qn{margin:0 -12px}.qo{margin:0 12px}.qp{flex:1 1 0}.qq{padding-bottom:12px}.qr:hover{color:rgba(255, 255, 255, 0.99)}.qs:hover{fill:rgba(255, 255, 255, 0.99)}.qt:disabled{color:rgba(255, 255, 255, 0.7)}.qu:disabled{fill:rgba(255, 255, 255, 0.7)}.qv{color:rgba(255, 255, 255, 0.98)}.qw{fill:rgba(255, 255, 255, 0.98)}.qx{text-align:inherit}.qy{font-size:21.6px}.qz{letter-spacing:-0.32px}.ra{color:rgba(255, 255, 255, 0.7)}.rb{fill:rgba(255, 255, 255, 0.7)}.rc{text-decoration:underline}.rd{padding-bottom:8px}.re{padding-top:8px}.rf{width:200px}.rq{padding-bottom:24px}.rr{padding-top:24px}.ru{top:-4px}.rv{font-family:medium-marketing-display-font, Georgia, Cambria, "Times New Roman", Times, serif}.rw{font-weight:500}.rx{margin-bottom:4px}.ry{margin-right:9px}.rz{margin-top:4px}.sa{width:130px}.sb{max-height:60px}.sc{-webkit-line-clamp:3}.sd:disabled{color:rgba(3, 168, 124, 0.5)}.se:disabled{fill:rgba(3, 168, 124, 0.5)}.sf{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.af{margin:0 64px}.fy{font-size:40px}.fz{margin-top:0.78em}.gf{line-height:48px}.gp{font-size:24px}.gq{margin-top:0.79em}.gw{line-height:32px}.hy{margin-left:30px}.ii{margin-top:56px}.ji{font-size:21px}.jj{margin-top:2em}.ka{margin-top:1.05em}.ku{font-size:34px}.kv{margin-top:1.25em}.lb{margin-top:0.86em}.mk{margin-right:5px}.na{margin-top:5px}.nr{margin-right:16px}.oq{width:25px}.qi{padding-left:64px}.qj{padding-right:64px}.qk{max-width:1320px}.rp:last-of-type{display:block}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.hx{margin-left:30px}.mj{margin-right:5px}.mz{margin-top:5px}.nq{margin-right:16px}.op{width:25px}.qf{padding-left:64px}.qg{padding-right:64px}.qh{max-width:1080px}.rs{margin:0 24px}.rt{width:780px}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.cp{display:flex}.hw{margin-left:30px}.mi{margin-right:5px}.my{margin-top:5px}.np{margin-right:16px}.oo{width:15px}.qc{padding-left:48px}.qd{padding-right:48px}.qe{max-width:904px}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.ak{height:56px}.al{display:flex}.aq{display:block}.cn{margin-left:10px}.co{margin-right:10px}.ev{margin-bottom:0px}.ew{height:110px}.ha{margin-top:32px}.hb{flex-direction:column-reverse}.hu{margin-bottom:30px}.hv{margin-left:0px}.mh{margin-left:8px}.mw{margin-top:2px}.mx{margin-right:8px}.no{margin-left:16px}.on{width:15px}.ox{margin-bottom:24px}.oy{align-items:center}.oz{width:102px}.pa{position:relative}.pd{padding-left:0}.pk{margin-top:24px}.pv{padding:32px 0}.pz{padding-left:24px}.qa{padding-right:24px}.qb{max-width:728px}.rg{width:140px}.rh{margin-bottom:16px}.ri{margin-top:30px}.rj{width:100%}.rk{flex-direction:row}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.z{margin:0 24px}.fq{font-size:30px}.fr{margin-top:0.72em}.gb{line-height:40px}.gh{font-size:18px}.gi{margin-top:0.79em}.gs{line-height:24px}.gz{margin-top:32px}.hg{margin-bottom:0px}.hs{margin-bottom:30px}.ht{margin-left:0px}.ie{margin-top:40px}.jc{margin-top:1.56em}.jm{line-height:28px}.jw{margin-top:1.34em}.ko{margin-top:0.93em}.kx{margin-top:0.67em}.mg{margin-left:8px}.mu{margin-top:2px}.mv{margin-right:8px}.nn{margin-left:16px}.om{width:15px}.pw{padding-left:24px}.px{padding-right:24px}.py{max-width:552px}.rl:last-of-type{display:none}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ae{margin:0 64px}.fw{font-size:40px}.fx{margin-top:0.78em}.ge{line-height:48px}.gn{font-size:24px}.go{margin-top:0.79em}.gv{line-height:32px}.ih{margin-top:56px}.jg{font-size:21px}.jh{margin-top:2em}.jz{margin-top:1.05em}.ks{font-size:34px}.kt{margin-top:1.25em}.la{margin-top:0.86em}.ro:last-of-type{display:none}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.ac{margin:0 48px}.fu{font-size:40px}.fv{margin-top:0.78em}.gd{line-height:48px}.gl{font-size:24px}.gm{margin-top:0.79em}.gu{line-height:32px}.ig{margin-top:56px}.je{font-size:21px}.jf{margin-top:2em}.jy{margin-top:1.05em}.kq{font-size:34px}.kr{margin-top:1.25em}.kz{margin-top:0.86em}.rn:last-of-type{display:none}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ab{margin:0 24px}.fs{font-size:30px}.ft{margin-top:0.72em}.gc{line-height:40px}.gj{font-size:18px}.gk{margin-top:0.79em}.gt{line-height:24px}.if{margin-top:40px}.jd{margin-top:1.56em}.jn{line-height:28px}.jx{margin-top:1.34em}.kp{margin-top:0.93em}.ky{margin-top:0.67em}.rm:last-of-type{display:none}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="print">.y{display:none}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.lr{transition:opacity 200ms}.nv{transition:border-color 150ms ease}.nw::before{background:
      radial-gradient(circle, rgba(0, 0, 0, 0.84) 60%, transparent 70%)
    }.nx::before{border-radius:50%}.ny::before{content:""}.nz::before{display:block}.oa::before{z-index:0}.ob::before{left:0}.oc::before{height:100%}.od::before{position:absolute}.oe::before{top:0}.of::before{width:100%}.og:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.ok{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (max-width: 1230px)">.ls{display:none}</style><style type="text/css" data-fela-rehydration="496" data-fela-type="RULE" media="all and (max-width: 1198px)">.lu{display:none}</style><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="sh si lm ai sj sk sl"><div class="n p"><div class="sm sn so sp sq sr ah ai"><div class="ir sv n gy"><div class="ai r"><div class="ix" id="li-mobile-small-meter-4-outer-clickable"><div><a href="https://medium.com/membership?source=upgrade_membership---post_counter--e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" rel="noopener"><div class="sw r sx"><div class="ix" id="li-mobile-small-meter-4-header"><h4 class="bi ek sy sz ta tb tc td te tf tg th ct"><div class="n aq"><div class="ix" id="li-small-meter-4-header">You've read all of your free stories this month.&nbsp;</div><div><a href="https://medium.com/membership?source=upgrade_membership---post_counter--e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba bd be ck cl rc" rel="noopener"><div class="ix" id="li-small-meter-4-link-text">Upgrade for unlimited access</div></a></div></div></h4></div></div></a></div></div></div><div class="sw r ti"><div class="ix" id="li-mobile-small-meter-4-close-button"><div class="az r cd v"><span class="bi b bj bk bl bm r bn bo"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" data-testid="close-button"><svg width="19" height="19" viewBox="0 0 19 19"><path d="M13.8 4.6L9.5 8.89 5.21 4.6l-.61.61 4.29 4.3-4.29 4.28.61.62 4.3-4.3 4.28 4.3.62-.62-4.3-4.29 4.3-4.29" fill-rule="evenodd"></path></svg></button></span></div></div></div></div></div></div></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y"><div><div class="r c"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="aj n o ak al"><div class="n o am w"><a href="https://medium.com/?source=post_page-----e16230b7d24e----------------------" aria-label="Homepage" rel="noopener"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a><div class="ix" id="li-general-navbar-open-in-app-button"><div class="ao ap aq"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe16230b7d24e&amp;~feature=LiMobileNavBar&amp;~channel=ShowPostUnderCollection&amp;source=post_page-----e16230b7d24e----------------------" class="ar as at au av aw ax ay az ba bb bc bd be bf bg" rel="noopener nofollow">Open in app</a></div></div></div><div class="r bh w"><span class="bi b bj bk bl bm r bn bo"><div class="n o bp"><div class="n f"><div class="bx" aria-hidden="true"><div class="n"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="0 0 25 25" class="ao cm r cn co"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></button><input class="by bz ca cb bk cc cd ce cf" placeholder="Search Towards Data Science" value=""></div></div></div><div class="ap cp"><a href="https://towardsdatascience.com/search?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="ao cq r cn co"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a></div><a href="https://medium.com/me/list/queue?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="cq r g"><path d="M16 6a2 2 0 0 1 2 2v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-5.67-4.13-5.66 4.13a.5.5 0 0 1-.7-.03.48.48 0 0 1-.13-.29H5V8c0-1.1.9-2 2-2h9zM6 8v12.64l5.16-3.67a.49.49 0 0 1 .68 0L17 20.64V8a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z"></path><path d="M21 5v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-.17-.12V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1H8c0-1.1.9-2 2-2h9a2 2 0 0 1 2 2z"></path></svg></a><div class="cq n co"><div class="bx" aria-hidden="true"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl r"><svg width="25" height="25" viewBox="-293 409 25 25" class="cr r"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></div></div><div class="ix" id="li-post-page-navbar-upsell-button"><div class="cq r g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="cs ct q cu cv cw cx cy ba ck cz da db dc dd de df bi b bj bk bl bm dg dh di bx dj bd" rel="noopener">Upgrade</a></div></div></div><div class="n" aria-hidden="true"><div class="dk dl n o"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><img alt="Ayushverma" class="r dm dn do" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/0_fSKu5zWydc5B9tfM" width="32" height="32"></button></div></div></div></span></div></div></div></div></div><div class="dp r dq aq"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="dr bv n o"><div class="ds r bh"><a href="https://towardsdatascience.com/?source=post_page-----e16230b7d24e----------------------" rel="noopener"><div class="dt du r"><img alt="Towards Data Science" class="" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_mG6i4Bh_LgixUYXJgQpYsg@2x.png" width="100" height="36"></div></a></div><div class="dv r dw"><ul class="dx az dy bw dz n ea g eb ec ed"><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/data-science/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">Data Science</a></span></li><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/machine-learning/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">Machine Learning</a></span></li><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/programming/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">Programming</a></span></li><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/data-visualization/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">Visualization</a></span></li><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/artificial-intelligence/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">AI</a></span></li><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/video/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">Video</a></span></li><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/about-us/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">About</a></span></li><span class="ee ei ej"></span><li class="n o ee ef eg eh"><span class="bi ek el bk em en eo"><a href="https://towardsdatascience.com/contribute/home?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba ep eq bd be er es" rel="noopener">Contribute</a></span></li></ul></div></div></div></div></div></div></nav><div class="et eu r ev ew"></div><article class="meteredContent"><section class="ex ey ez fa ai fb di n fc"></section><span class="r"></span><div><div class="s u fd fe ff fg"></div><section class="fh fi fj fk fl"><div class="n p"><div class="z ab ac ae af fm ah ai"><div><div id="7184" class="fn fo ct bj fp b fq fr fs ft fu fv fw fx fy fz ga"><h1 class="fp b fq gb fs gc fu gd fw ge fy gf ct">The Complete Reinforcement Learning Dictionary</h1></div></div><h2 id="b16a" class="gg fo bn bj bi ek gh gi gs gj gk gt gl gm gu gn go gv gp gq gw gr">The Reinforcement Learning Terminology, A to Z</h2><div class="gx"><div class="n gy gz ha hb"><div class="o n"><div><a href="https://towardsdatascience.com/@shakedzy?source=post_page-----e16230b7d24e----------------------" rel="noopener"><img alt="Shaked Zychlinski" class="r dm hc hd" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_hM8X1ekutC5l8TquiCA45g.jpeg" width="48" height="48"></a></div><div class="he ai r"><div class="n"><div style="flex:1"><span class="bi b bj bk bl bm r ct q"><div class="hf n o hg"><span class="bi ek cb bk bv hh bu hi hj hk ct"><a href="https://towardsdatascience.com/@shakedzy?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener">Shaked Zychlinski</a></span><div class="hm r bh h"><button class="hn cu ar as ho bb bc hp ba df bi b bj hq el bm dg dh di bx dj bd">Follow</button></div></div></span></div></div><span class="bi b bj bk bl bm r bn bo"><span class="bi ek cb bk bv hh bu hi hj hk bn"><div><a class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener" href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e?source=post_page-----e16230b7d24e----------------------">Feb 24, 2019</a> <!-- -->·<!-- --> <!-- -->14<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n hr hs ht hu hv hw hx hy y"><div class="n o"><div class="hz r bh"><a href="https://medium.com/p/e16230b7d24e/share/twitter?source=post_actions_header---------------------------" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="hz r bh"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="hz r bh"><a href="https://medium.com/p/e16230b7d24e/share/facebook?source=post_actions_header---------------------------" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="ia r"><div><div class="ib"><div><div class="bx" role="tooltip" aria-hidden="true" aria-describedby="1" aria-labelledby="1"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="ic r am"><div class="bx" aria-hidden="true"><div class="bx" aria-hidden="true"><div class="r bh"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div></div></div></div><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa id"><div class="ir r cd is"><div class="it iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_Z0JjTZ2DnEK8S-FPgktbNQ.jpeg" width="1920" height="1440" role="presentation"></div><img class="po sg s t u io ai iy" width="1920" height="1440" srcset="https://miro.medium.com/max/552/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 276w, https://miro.medium.com/max/1104/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 552w, https://miro.medium.com/max/1280/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 640w, https://miro.medium.com/max/1400/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_Z0JjTZ2DnEK8S-FPgktbNQ(1).jpeg"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/3840/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg" width="1920" height="1440" srcSet="https://miro.medium.com/max/552/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 276w, https://miro.medium.com/max/1104/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 552w, https://miro.medium.com/max/1280/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 640w, https://miro.medium.com/max/1400/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="1d7a" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">Whenever I begin learning a subject which is new to me, I find the hardest thing to cope with is its new terminology. Every field have many terms and definitions which are completely obscure to an outsider, and can make a newcomer’s first step quite difficult.</p><p id="30b0" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">When I made my first step into the world or Reinforcement Learning, I was quite overwhelmed by the new terms which popped-up every other line, and it always surprised me how behind those complex words stood quite simple and logical ideas. I therefore decided to write them all down in my own words, so I’ll always be able to look them up in case I forget. This is how this dictionary came to be.</p><p id="f7f0" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">This is not an introduction post to Reinforcement Learning, rather it’s a supplementary tool to assist while studying. If you do look to start your path in this field too, I can recommend the following:</p><ul class=""><li id="8c81" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk jo jp jq" data-selectable-paragraph="">If you’re looking for a quick, 10-minutes crash course into RL with code examples, checkout my <em class="jr">Qrash Course</em> series: <a href="https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677" class="cg dj js jt ju jv" target="_blank" rel="noopener">Introduction to RL and Q-Learning</a> and <a href="https://medium.com/@shakedzy/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c" class="cg dj js jt ju jv" target="_blank" rel="noopener">Policy Gradients and Actor-Critics</a>.</li><li id="5f07" class="iz jl ct bj jb b gh jw jm gj jx jn je jy gu jg jz gv ji ka gw jk jo jp jq" data-selectable-paragraph="">I you’re into something deeper, and would like to learn and code several different RL algorithms and gain more intuition, I can recommend <a href="https://medium.com/freecodecamp/an-introduction-to-reinforcement-learning-4339519de419" class="cg dj js jt ju jv" target="_blank" rel="noopener">this series</a> by <a href="https://medium.com/@thomassimonini" class="cg dj js jt ju jv" target="_blank" rel="noopener">Thomas Simonini</a> and <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0" class="cg dj js jt ju jv" target="_blank" rel="noopener">this series</a> by <a href="https://medium.com/@awjuliani" class="cg dj js jt ju jv" target="_blank" rel="noopener">Arthur Juliani</a>.</li><li id="a368" class="iz jl ct bj jb b gh jw jm gj jx jn je jy gu jg jz gv ji ka gw jk jo jp jq" data-selectable-paragraph="">If you’re ready to master RL, I will direct you to the “bible” of Reinforcement Learning — <em class="jr">“Reinforcement Learning, an introduction”</em> by Richard Sutton and Andrew Barto. The second edition (from 2018) is available for free (legally) as a <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">PDF file</a>.</li></ul><p id="8db4" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">I will do my best to try and keep on updating this dictionary. Feel free to let me know if I’ve missed anything important or got something wrong.</p></div></div></section><hr class="kb ek kc kd by ke kf kg kh ki kj kk"><section class="fh fi fj fk fl"><div class="n p"><div class="z ab ac ae af fm ah ai"><h1 id="6195" class="kl km ct bj bi kn fq ko fs kp kq kr ks kt ku kv kw" data-selectable-paragraph="">The Dictionary</h1><p id="c678" class="iz jl ct bj jb b gh kx jm gj ky jn je kz gu jg la gv ji lb gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Action-Value Function:</strong> See <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Value</em></a><em class="jr">.</em></p><p id="8751" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Actions: </strong>Actions are the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a><em class="jr">’s </em>methods which allow it to interact and change its <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4311" class="cg dj js jt ju jv" rel="noopener"><em class="jr">environment</em></a>, and thus transfer between <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">states</em></a>. Every action performed by the Agent yields a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> from the environment. The decision of which action to choose is made by the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a>.</p><p id="8f35" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Actor-Critic: </strong>When attempting to solve a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a><em class="jr"> </em>problem, there are two main methods one can choose from: calculating the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#680c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Value Functions</em></a> or <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Values</em></a> of each state and choosing actions according to those, or directly compute a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a> which defines the probabilities each action should be taken depending on the current state, and act according to it. Actor-Critic algorithms combine the two methods in order to create a more robust method. A great illustrated-comics explanation can be found <a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">here</a>.</p><p id="efa6" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Advantage Function: </strong>Usually denoted as <em class="jr">A(s,a)</em>, the Advantage function is a measure of how much is a certain <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a><em class="jr"> </em>a good or bad decision given a certain <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a> — or more simply, what is the advantage of selecting a certain action from a certain state. It is defined mathematically as:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ez fa ld"><div class="ir r cd is"><div class="le iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_CbxBGj9dxC741D0z0DL2Kg.png" width="577" height="56" role="presentation"></div><img class="po sg s t u io ai iy" width="577" height="56" srcset="https://miro.medium.com/max/552/1*CbxBGj9dxC741D0z0DL2Kg.png 276w, https://miro.medium.com/max/1104/1*CbxBGj9dxC741D0z0DL2Kg.png 552w, https://miro.medium.com/max/1154/1*CbxBGj9dxC741D0z0DL2Kg.png 577w" sizes="577px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_CbxBGj9dxC741D0z0DL2Kg(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/1154/1*CbxBGj9dxC741D0z0DL2Kg.png" width="577" height="56" srcSet="https://miro.medium.com/max/552/1*CbxBGj9dxC741D0z0DL2Kg.png 276w, https://miro.medium.com/max/1104/1*CbxBGj9dxC741D0z0DL2Kg.png 552w, https://miro.medium.com/max/1154/1*CbxBGj9dxC741D0z0DL2Kg.png 577w" sizes="577px" role="presentation"/></noscript></div></div></div></figure><p id="c06e" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">where <em class="jr">r(s,a) </em>is the expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> of action <em class="jr">a</em> from state <em class="jr">s</em>, and <em class="jr">r(s)</em> is the expected reward of the entire state <em class="jr">s</em>, before an action was selected. It can also be viewed as:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ez fa ld"><div class="ir r cd is"><div class="le iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_GuzTDUuNkuEboEhhdILnEQ.png" width="577" height="56" role="presentation"></div><img class="po sg s t u io ai iy" width="577" height="56" srcset="https://miro.medium.com/max/552/1*GuzTDUuNkuEboEhhdILnEQ.png 276w, https://miro.medium.com/max/1104/1*GuzTDUuNkuEboEhhdILnEQ.png 552w, https://miro.medium.com/max/1154/1*GuzTDUuNkuEboEhhdILnEQ.png 577w" sizes="577px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_GuzTDUuNkuEboEhhdILnEQ(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/1154/1*GuzTDUuNkuEboEhhdILnEQ.png" width="577" height="56" srcSet="https://miro.medium.com/max/552/1*GuzTDUuNkuEboEhhdILnEQ.png 276w, https://miro.medium.com/max/1104/1*GuzTDUuNkuEboEhhdILnEQ.png 552w, https://miro.medium.com/max/1154/1*GuzTDUuNkuEboEhhdILnEQ.png 577w" sizes="577px" role="presentation"/></noscript></div></div></div></figure><p id="d10c" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">where <em class="jr">Q(s,a)</em> is the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q Value</em></a> and <em class="jr">V(s)</em> is the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#680c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Value function</em></a><em class="jr">.</em></p><p id="b6a2" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Agent:</strong> The learning and <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">acting</em></a> part of a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> problem, which tries to maximize the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">rewards</em></a> it is given by the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4311" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Environment</em></a>. Putting it simply, the Agent is the model which you try to design.</p><p id="790b" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Bandits: </strong>Formally named “k-Armed Bandits” after the nickname “one-armed bandit” given to <a href="https://en.wikipedia.org/wiki/Slot_machine" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">slot-machines</a>, these are considered to be the simplest type of <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a><em class="jr"> </em>tasks. Bandits have no different <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">states</em></a>, but only one — and the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> taken under consideration is only the immediate one. Hence, bandits can be thought of as having single-state <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episodes</em></a>. Each of the k-arms is considered an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a>, and the objective is to learn the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a> which will maximize the expected reward after each action (or arm-pulling). <br><em class="jr">Contextual Bandits</em> are a slightly more complex task, where each state may be different and affect the outcome of the actions — hence every time the <em class="jr">context</em> is different. Still, the task remains a single-state episodic task, and one context cannot have an influence on others.</p><p id="fbd3" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Bellman Equation: </strong>Formally, Bellman equation defines the relationships between a given <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a> (or state-<a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a> pair) to its successors. While many forms exist, the most common one usually encountered in <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> tasks is the Bellman equation for the optimal <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Value</em></a><em class="jr"> </em>, which is given by:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa lf"><div class="ir r cd is"><div class="lg iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_Re6kADukp4wKFEnGzhImzw.png" width="1200" height="128" role="presentation"></div><img class="po sg s t u io ai iy" width="1200" height="128" srcset="https://miro.medium.com/max/552/1*Re6kADukp4wKFEnGzhImzw.png 276w, https://miro.medium.com/max/1104/1*Re6kADukp4wKFEnGzhImzw.png 552w, https://miro.medium.com/max/1280/1*Re6kADukp4wKFEnGzhImzw.png 640w, https://miro.medium.com/max/1400/1*Re6kADukp4wKFEnGzhImzw.png 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_Re6kADukp4wKFEnGzhImzw(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/2400/1*Re6kADukp4wKFEnGzhImzw.png" width="1200" height="128" srcSet="https://miro.medium.com/max/552/1*Re6kADukp4wKFEnGzhImzw.png 276w, https://miro.medium.com/max/1104/1*Re6kADukp4wKFEnGzhImzw.png 552w, https://miro.medium.com/max/1280/1*Re6kADukp4wKFEnGzhImzw.png 640w, https://miro.medium.com/max/1400/1*Re6kADukp4wKFEnGzhImzw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="0abe" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">or when no uncertainty exists (meaning, probabilities are either 1 or 0):</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa lf"><div class="ir r cd is"><div class="lg iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_HTYLQwcKYerhL9620zALMA.png" width="1200" height="128" role="presentation"></div><img class="po sg s t u io ai iy" width="1200" height="128" srcset="https://miro.medium.com/max/552/1*HTYLQwcKYerhL9620zALMA.png 276w, https://miro.medium.com/max/1104/1*HTYLQwcKYerhL9620zALMA.png 552w, https://miro.medium.com/max/1280/1*HTYLQwcKYerhL9620zALMA.png 640w, https://miro.medium.com/max/1400/1*HTYLQwcKYerhL9620zALMA.png 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_HTYLQwcKYerhL9620zALMA(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/2400/1*HTYLQwcKYerhL9620zALMA.png" width="1200" height="128" srcSet="https://miro.medium.com/max/552/1*HTYLQwcKYerhL9620zALMA.png 276w, https://miro.medium.com/max/1104/1*HTYLQwcKYerhL9620zALMA.png 552w, https://miro.medium.com/max/1280/1*HTYLQwcKYerhL9620zALMA.png 640w, https://miro.medium.com/max/1400/1*HTYLQwcKYerhL9620zALMA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="49d8" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">where the asterisk sign indicates <em class="jr">optimal value</em>. Some algorithms, such as <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#9d8f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Learning</em></a>, are basing their learning procedure over it.</p><p id="2692" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Continuous Tasks: </strong><a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> tasks which are not made of <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episodes</em></a>, but rather last forever. This tasks have no terminal<em class="jr"> </em><a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a><em class="jr">s.</em> For simplicity, they are usually assumed to be made of one never-ending episode.</p><p id="ac07" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Deep Q-Networks (DQN)</strong>: See <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#9d8f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Learning</em></a></p><p id="aa91" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Deep Reinforcement Learning: </strong>The use of a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> algorithm with a deep neural network as an approximator for the learning part. This is usually done in order to cope with problems where the number of possible <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">states</em></a> and <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a> scales fast, and an exact solution in no longer feasible.</p><p id="4ee6" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Discount Factor (γ)</strong>: The discount factor, usually denoted as γ, is a factor multiplying the future expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a>, and varies on the range of [0,1]. It controls the importance of the future rewards versus the immediate ones. The lower the discount factor is, the less important future rewards are, and the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a><em class="jr"> </em>will tend to focus on <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a> which will yield immediate rewards only.</p><p id="4311" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Environment: </strong>Everything which isn’t the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a>; everything the Agent can interact with, either directly or indirectly. The environment changes as the Agent performs <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a>; every such change is considered a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a>-transition. Every action the Agent performs yields a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> received by the Agent.</p><p id="601d" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Episode: </strong>All <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">states</em></a> that come in between an initial-state and a terminal-state; for example: one game of Chess. The <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a><em class="jr">’s</em> goal it to maximize the total <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> it receives during an episode. In situations where there is no terminal-state, we consider an infinite episode. It is important to remember that different episodes are completely independent of one another.</p><p id="15bc" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Episodic Tasks:<em class="jr"> </em></strong><a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> tasks which are made of different <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episodes</em></a> (meaning, each episode has a terminal<em class="jr"> </em><a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a>).</p><p id="a3de" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Expected Return:</strong> Sometimes referred to as “overall reward” and occasionally denoted as <em class="jr">G</em>, is the expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> over an entire <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episode</em></a>.</p><p id="5ecb" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Experience Replay:</strong> As <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> tasks have no pre-generated training sets which they can learn from, the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> must keep records of all the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a><em class="jr">-</em>transitions it encountered so it can learn from them later. The memory-buffer used to store this is often referred to as <em class="jr">Experience Replay</em>. There are several types and architectures of these memory buffers — but some very common ones are the <a href="https://en.wikipedia.org/wiki/Circular_buffer" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">cyclic memory buffers</a> (which makes sure the Agent keeps training over its new behavior rather than things that might no longer be relevant) and <a href="https://en.wikipedia.org/wiki/Reservoir_sampling" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">reservoir-sampling</a>-based memory buffers (which guarantees each state-transition recorded has an even probability to be inserted to the buffer).</p><p id="b903" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Exploitation &amp; Exploration:</strong> <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> tasks have no pre-generated training sets which they can learn from — they create their own <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#5ecb" class="cg dj js jt ju jv" rel="noopener"><em class="jr">experience</em></a> and learn “on the fly”. To be able to do so, the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> needs to try many different <em class="jr">actions</em> in many different <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">states</em></a> in order to try and learn all available possibilities and find the path which will maximize its overall <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a>; this is known as <em class="jr">Exploration</em>, as the Agent explores the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4311" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Environment</em></a>. On the other hand, if all the Agent will do is explore, it will never maximize the overall reward — it must also use the information it learned to do so. This is known as <em class="jr">Exploitation</em>, as the Agent exploits its knowledge to maximize the rewards it receives. <br>The trade-off between the two is one of the greatest challenges of Reinforcement Learning problems, as the two must be balanced in order to allow the Agent to both explore the environment enough, but also exploit what it learned and repeat the most rewarding path it found.</p><p id="15c5" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Greedy Policy, <em class="jr">ε</em>-Greedy Policy:</strong> A greedy <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a> means the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a><em class="jr"> </em>constantly performs the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a> that is believed to yield the highest expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a>. Obviously, such a policy will not allow the Agent to <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b903" class="cg dj js jt ju jv" rel="noopener"><em class="jr">explore</em></a><em class="jr"> </em>at all. In order to still allow some exploration, an <em class="jr">ε-</em>greedy policy is often used instead: a number (named <em class="jr">ε</em>)<em class="jr"> </em>in the range of [0,1] is selected, and prior selecting an action, a random number in the range of [0,1] is selected. if that number is larger than <em class="jr">ε</em>, the greedy action is selected — but if it’s lower, a random action is selected. Note that if <em class="jr">ε</em>=0, the policy becomes the greedy policy, and if <em class="jr">ε</em>=1, always explore.</p><p id="42c0" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">k-Armed Bandits:</strong> See <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#790b" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Bandits</em></a><em class="jr">.</em></p><p id="4a1d" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Markov Decision Process (MDP): </strong>The Markov Property means that each <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a><em class="jr"> </em>is dependent solely on its preceding state, the selected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a> taken from that state and the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> received immediately after that action was executed. Mathematically, it means: <em class="jr">s’ = s’(s,a,r)</em>, where <em class="jr">s’</em> is the future state, <em class="jr">s</em> is its preceding state and <em class="jr">a</em> and <em class="jr">r </em>are the action and reward. No prior knowledge of what happened before <em class="jr">s</em> is needed — the Markov Property assumes that <em class="jr">s </em>holds all the relevant information within it. A Markov Decision Process is decision process based on these assumptions.</p><p id="6d88" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Model-Based &amp; Model-Free: </strong>Model-based and model-free are two different approaches an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> can choose when trying to optimize its <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a>. This is best explained using an example: assume you’re trying to learn how to play <a href="https://en.wikipedia.org/wiki/Blackjack" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">Blackjack</a>. You can do so in two ways: one, you calculate in advance, before the game begins, the winning probabilities of all <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">states</em></a> and all the state-transition probabilities given all the possible <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a>, and then simply act according to you calculations. The second option is to simply play without any prior knowledge, and gain information using “trial-and-error”. Note that using the first approach, you’re basically <em class="jr">modeling</em> your <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4311" class="cg dj js jt ju jv" rel="noopener"><em class="jr">environment</em></a>, while the second approach requires no information about the environment. This is exactly the difference between model-based and model-free; the first method is model-based, while the latter is model-free.</p><p id="f9e0" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Monte Carlo (MC):</strong> Monte Carlo methods are algorithms which use repeated random sampling in order to achieve a result. They are used quite often in <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> algorithms to obtain expected values; for example — calculating a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a> <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#680c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Value function</em></a> by returning to the same state over and over again, and averaging over the actual cumulative <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> received each time.</p><p id="44d1" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">On-Policy &amp; Off-Policy:</strong> Every <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a><em class="jr"> </em>algorithm must follow some <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a> in order to decide which <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a> to perform at each <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a>. Still, the learning procedure of the algorithm doesn’t have to take into account that policy while learning. Algorithms which concern about the policy which yielded past state-action decisions are referred to as <em class="jr">on-policy</em> algorithms, while those ignoring it are known as <em class="jr">off-policy</em>. <br>A well known off-policy algorithm is <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#9d8f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Learning</em></a>, as its update rule uses the action which will yield the highest <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Value</em></a>, while the actual policy used might restrict that action or choose another. The on-policy variation of Q-Learning is known as <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#da43" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Sarsa</em></a>, where the update rule uses the action chosen by the followed policy.</p><p id="04db" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">One-Armed Bandits: </strong>See <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#790b" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Bandits</em></a><em class="jr">.</em></p><p id="532b" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">One-Step TD:</strong> See <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#d0d7" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Temporal Difference</em></a><em class="jr">.</em></p><p id="a76c" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Policy (π):</strong> The policy, denoted as <em class="jr">π</em> (or sometimes <em class="jr">π(a|s)</em>), is a mapping from some <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a><em class="jr"> s</em> to the probabilities of selecting each possible <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a><em class="jr"> </em>given that state. For example, a <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#15c5" class="cg dj js jt ju jv" rel="noopener"><em class="jr">greedy policy</em></a> outputs for every state the action with the highest expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Value</em></a>.</p><p id="9d8f" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Q-Learning: </strong>Q-Learning is an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#44d1" class="cg dj js jt ju jv" rel="noopener"><em class="jr">off-policy</em></a><em class="jr"> </em><a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a> algorithm, considered as one of the very basic ones. In its most simplified form, it uses a table to store all <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Values</em></a> of all possible <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a><em class="jr">-</em><a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a> pairs possible. It updates this table using the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fbd3" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Bellman equation</em></a>, while action selection is usually made with an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#15c5" class="cg dj js jt ju jv" rel="noopener"><em class="jr">ε-greedy policy</em></a><em class="jr">. <br></em>In its simplest form (no uncertainties in <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a>-transitions and expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">rewards</em></a>), the update rule of Q-Learning is:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa lf"><div class="ir r cd is"><div class="lg iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_l8ZP4tTFqDGyezwJ8jR8eA.png" width="1200" height="128" role="presentation"></div><img class="po sg s t u io ai iy" width="1200" height="128" srcset="https://miro.medium.com/max/552/1*l8ZP4tTFqDGyezwJ8jR8eA.png 276w, https://miro.medium.com/max/1104/1*l8ZP4tTFqDGyezwJ8jR8eA.png 552w, https://miro.medium.com/max/1280/1*l8ZP4tTFqDGyezwJ8jR8eA.png 640w, https://miro.medium.com/max/1400/1*l8ZP4tTFqDGyezwJ8jR8eA.png 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_l8ZP4tTFqDGyezwJ8jR8eA(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/2400/1*l8ZP4tTFqDGyezwJ8jR8eA.png" width="1200" height="128" srcSet="https://miro.medium.com/max/552/1*l8ZP4tTFqDGyezwJ8jR8eA.png 276w, https://miro.medium.com/max/1104/1*l8ZP4tTFqDGyezwJ8jR8eA.png 552w, https://miro.medium.com/max/1280/1*l8ZP4tTFqDGyezwJ8jR8eA.png 640w, https://miro.medium.com/max/1400/1*l8ZP4tTFqDGyezwJ8jR8eA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="a9f2" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">A more complex version of it, though far more popular, is the <em class="jr">Deep Q-Network</em> variant (which is sometimes even referred to simply as <em class="jr">Deep Q-Learning </em>or just <em class="jr">Q-Learning</em>). This variant replaces the state-action table with a neural network in order to cope with large-scale tasks, where the number of possible state-action pairs can be enormous. You can find a tutorial for this algorithm in <a href="https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677" class="cg dj js jt ju jv" target="_blank" rel="noopener">this blogpost</a>.</p><p id="f366" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Q Value (Q Function): </strong>Usually denoted as <em class="jr">Q(s,a) </em>(sometimes with a π subscript, and sometimes as <em class="jr">Q(s,a; θ)</em> in <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#aa91" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Deep RL</em></a>), Q Value is a measure of the overall expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> assuming the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> is in <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a> <em class="jr">s</em> and performs <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a> <em class="jr">a</em>, and then continues playing until the end of the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episode</em></a><em class="jr"> </em>following some <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a> π. Its name is an abbreviation of the word “Quality”, and it is defined mathematically as:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ez fa ld"><div class="ir r cd is"><div class="lh iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_OQqePNtxQV177JeQR1O16g.png" width="577" height="100" role="presentation"></div><img class="po sg s t u io ai iy" width="577" height="100" srcset="https://miro.medium.com/max/552/1*OQqePNtxQV177JeQR1O16g.png 276w, https://miro.medium.com/max/1104/1*OQqePNtxQV177JeQR1O16g.png 552w, https://miro.medium.com/max/1154/1*OQqePNtxQV177JeQR1O16g.png 577w" sizes="577px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_OQqePNtxQV177JeQR1O16g(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/1154/1*OQqePNtxQV177JeQR1O16g.png" width="577" height="100" srcSet="https://miro.medium.com/max/552/1*OQqePNtxQV177JeQR1O16g.png 276w, https://miro.medium.com/max/1104/1*OQqePNtxQV177JeQR1O16g.png 552w, https://miro.medium.com/max/1154/1*OQqePNtxQV177JeQR1O16g.png 577w" sizes="577px" role="presentation"/></noscript></div></div></div></figure><p id="c8d9" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">where <em class="jr">N</em> is the number of states from state <em class="jr">s</em> till the terminal state, γ is the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4ee6" class="cg dj js jt ju jv" rel="noopener"><em class="jr">discount factor</em></a> and r⁰ is the immediate reward received after performing action <em class="jr">a</em> in state <em class="jr">s</em>.</p><p id="41f4" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">REINFORCE Algorithms: </strong>REINFORCE algorithms are a family of <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fc9f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Reinforcement Learning</em></a><em class="jr"> </em>algorithms<strong class="jb lc"> </strong>which update their <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a><em class="jr"> </em>parameters<em class="jr"> </em>according to the gradient of the policy with respect to the policy-parameters [<a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">paper</a>]. The name is typically written using capital letters only, as it’s originally an acronym for the original algorithms group design: “<strong class="jb lc">RE</strong>ward <strong class="jb lc">I</strong>ncrement = <strong class="jb lc">N</strong>onnegative <strong class="jb lc">F</strong>actor <em class="jr">x</em> <strong class="jb lc">O</strong>ffset <strong class="jb lc">R</strong>einforcement <em class="jr">x </em><strong class="jb lc">C</strong>haracteristic <strong class="jb lc">E</strong>ligibility” [<a href="https://link.springer.com/content/pdf/10.1007%2FBF00992696.pdf" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">source</a>]</p><p id="fc9f" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Reinforcement Learning (RL):</strong> Reinforcement Learning is, like Supervised Learning and Unsupervised Learning, one the main areas of Machine Learning and Artificial Intelligence. It is concerned with the learning process of an arbitrary being, formally known as an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a>, in the world surrounding it, known as the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4311" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Environment</em></a>. The Agent seeks to maximize the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">rewards</em></a> it receives from the Environment, and performs different <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a> in order to learn how the Environment responds and gain more rewards. One of the greatest challenges of RL tasks is to associate actions with postponed rewards — which are rewards received by the Agent long after the reward-generating action was made. It is therefore heavily used to solve different kind of games, from Tic-Tac-Toe, Chess, Atari 2600 and all the way to Go and StarCraft.</p><p id="6a6f" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Reward:</strong> A numerical value received by the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> from the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4311" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Environment</em></a> as a direct response to the Agent’s <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a>. The Agent’s goal is to maximize the overall reward it receives during an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episode</em></a>, and so rewards are the motivation the Agent needs in order to act in a desired behavior. All actions yield rewards, which can be roughly divided to three types: <em class="jr">positive rewards</em> which emphasize a desired action, <em class="jr">negative rewards</em> which emphasize an action the Agent should stray away from, and <em class="jr">zero</em>, which means the Agent didn’t do anything special or unique.</p><p id="da43" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Sarsa:</strong> The Sarsa algorithm is pretty much the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#9d8f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Learning</em></a> algorithm with a slight modification in order to make it an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#44d1" class="cg dj js jt ju jv" rel="noopener"><em class="jr">on-policy</em></a> algorithm. The Q-Learning update rule is based on the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#fbd3" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Bellman equation</em></a> for the optimal <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q-Value</em></a>, and so in the case on no uncertainties in <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a>-transitions and expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">rewards</em></a>, the Q-Learning update rule is:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa lf"><div class="ir r cd is"><div class="lg iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_l8ZP4tTFqDGyezwJ8jR8eA.png" width="1200" height="128" role="presentation"></div><img class="po sg s t u io ai iy" width="1200" height="128" srcset="https://miro.medium.com/max/552/1*l8ZP4tTFqDGyezwJ8jR8eA.png 276w, https://miro.medium.com/max/1104/1*l8ZP4tTFqDGyezwJ8jR8eA.png 552w, https://miro.medium.com/max/1280/1*l8ZP4tTFqDGyezwJ8jR8eA.png 640w, https://miro.medium.com/max/1400/1*l8ZP4tTFqDGyezwJ8jR8eA.png 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_l8ZP4tTFqDGyezwJ8jR8eA(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/2400/1*l8ZP4tTFqDGyezwJ8jR8eA.png" width="1200" height="128" srcSet="https://miro.medium.com/max/552/1*l8ZP4tTFqDGyezwJ8jR8eA.png 276w, https://miro.medium.com/max/1104/1*l8ZP4tTFqDGyezwJ8jR8eA.png 552w, https://miro.medium.com/max/1280/1*l8ZP4tTFqDGyezwJ8jR8eA.png 640w, https://miro.medium.com/max/1400/1*l8ZP4tTFqDGyezwJ8jR8eA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="b7e4" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">In order to transform this into an on-policy algorithm, the last term is modified:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa lf"><div class="ir r cd is"><div class="li iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_hdnBhX5TYGLSj7xGAwe6Xw.png" width="1200" height="124" role="presentation"></div><img class="po sg s t u io ai iy" width="1200" height="124" srcset="https://miro.medium.com/max/552/1*hdnBhX5TYGLSj7xGAwe6Xw.png 276w, https://miro.medium.com/max/1104/1*hdnBhX5TYGLSj7xGAwe6Xw.png 552w, https://miro.medium.com/max/1280/1*hdnBhX5TYGLSj7xGAwe6Xw.png 640w, https://miro.medium.com/max/1400/1*hdnBhX5TYGLSj7xGAwe6Xw.png 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_hdnBhX5TYGLSj7xGAwe6Xw(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/2400/1*hdnBhX5TYGLSj7xGAwe6Xw.png" width="1200" height="124" srcSet="https://miro.medium.com/max/552/1*hdnBhX5TYGLSj7xGAwe6Xw.png 276w, https://miro.medium.com/max/1104/1*hdnBhX5TYGLSj7xGAwe6Xw.png 552w, https://miro.medium.com/max/1280/1*hdnBhX5TYGLSj7xGAwe6Xw.png 640w, https://miro.medium.com/max/1400/1*hdnBhX5TYGLSj7xGAwe6Xw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="90a9" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">when here, both <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a> <em class="jr">a</em> and <em class="jr">a’</em> are chosen by the same <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a>. The name of the algorithm is derived from its update rule, which is based on (<em class="jr">s,a,r,s’,a’</em>), all coming from the same policy.</p><p id="c274" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">State:</strong> Every scenario the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> encounters in the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4311" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Environment</em></a> is formally called a <em class="jr">state. </em>The Agent transitions between different states by performing <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">actions</em></a>. It is also worth mentioning the <em class="jr">terminal states</em>, which mark the end of an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episode</em></a><em class="jr">.</em> There are no possible states after a terminal state has been reached, and a new episode begins. Quite often, a terminal state is represented as a special state where all actions transition to the same terminal state with <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> 0.</p><p id="40ff" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">State-Value Function:</strong> See <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#680c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Value Function</em></a><em class="jr">.</em></p><p id="d0d7" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Temporal-Difference (TD): </strong>Temporal Difference is a learning method which combines both <a href="https://en.wikipedia.org/wiki/Dynamic_programming" class="cg dj js jt ju jv" target="_blank" rel="noopener nofollow">Dynamic Programming</a> and <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f9e0" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Monte Carlo</em></a> principles; it learns “on the fly” similarly to Monte Carlo, yet updates its estimates like Dynamic Programming. One of the simplest Temporal Difference algorithms it known as <em class="jr">one-step TD</em> or <em class="jr">TD(0)</em>. It updates the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#680c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Value Function</em></a> according to the following update rule:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa lj"><div class="ir r cd is"><div class="le iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_jLeVw2pIpOfknG5V8ySOhQ.png" width="1154" height="112" role="presentation"></div><img class="po sg s t u io ai iy" width="1154" height="112" srcset="https://miro.medium.com/max/552/1*jLeVw2pIpOfknG5V8ySOhQ.png 276w, https://miro.medium.com/max/1104/1*jLeVw2pIpOfknG5V8ySOhQ.png 552w, https://miro.medium.com/max/1280/1*jLeVw2pIpOfknG5V8ySOhQ.png 640w, https://miro.medium.com/max/1400/1*jLeVw2pIpOfknG5V8ySOhQ.png 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_jLeVw2pIpOfknG5V8ySOhQ(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/2308/1*jLeVw2pIpOfknG5V8ySOhQ.png" width="1154" height="112" srcSet="https://miro.medium.com/max/552/1*jLeVw2pIpOfknG5V8ySOhQ.png 276w, https://miro.medium.com/max/1104/1*jLeVw2pIpOfknG5V8ySOhQ.png 552w, https://miro.medium.com/max/1280/1*jLeVw2pIpOfknG5V8ySOhQ.png 640w, https://miro.medium.com/max/1400/1*jLeVw2pIpOfknG5V8ySOhQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="cd60" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">where <em class="jr">V</em> is the Value Function, <em class="jr">s</em> is the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a>, <em class="jr">r</em> is the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a>, <em class="jr">γ</em> is the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#4ee6" class="cg dj js jt ju jv" rel="noopener"><em class="jr">discount factor</em></a>, <em class="jr">α </em>is a learning rate, <em class="jr">t</em> is the time-step and the ‘=’ sign is used as an update operator and not equality. The term found in the squared brackets is known as the <em class="jr">temporal difference error.</em></p><p id="4371" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Terminal State:</strong> See <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">State</em></a><em class="jr">.</em></p><p id="930e" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Upper Confident Bound (UCB): </strong>UCB is an <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b903" class="cg dj js jt ju jv" rel="noopener"><em class="jr">exploration</em></a> method which tries to ensure that each <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#8751" class="cg dj js jt ju jv" rel="noopener"><em class="jr">action</em></a> is well explored. Consider an exploration <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a><em class="jr"> </em>which is completely random — meaning, each possible action has the same chance of being selected. There is a chance that some actions will be explored much more than others. The less an action is selected, the less confident the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> can be about its expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a>, and the its <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b903" class="cg dj js jt ju jv" rel="noopener"><em class="jr">exploitation</em></a> phase might be harmed. Exploration by UCB takes into account the number of times each action was selected, and gives extra weight to those less-explored. Formalizing this mathematically, the selected action is picked by:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ik il cd im ai"><div class="ez fa lf"><div class="ir r cd is"><div class="lk iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_5haNfed96VexQNtBOw2tYg.png" width="1200" height="134" role="presentation"></div><img class="po sg s t u io ai iy" width="1200" height="134" srcset="https://miro.medium.com/max/552/1*5haNfed96VexQNtBOw2tYg.png 276w, https://miro.medium.com/max/1104/1*5haNfed96VexQNtBOw2tYg.png 552w, https://miro.medium.com/max/1280/1*5haNfed96VexQNtBOw2tYg.png 640w, https://miro.medium.com/max/1400/1*5haNfed96VexQNtBOw2tYg.png 700w" sizes="700px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_5haNfed96VexQNtBOw2tYg(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/2400/1*5haNfed96VexQNtBOw2tYg.png" width="1200" height="134" srcSet="https://miro.medium.com/max/552/1*5haNfed96VexQNtBOw2tYg.png 276w, https://miro.medium.com/max/1104/1*5haNfed96VexQNtBOw2tYg.png 552w, https://miro.medium.com/max/1280/1*5haNfed96VexQNtBOw2tYg.png 640w, https://miro.medium.com/max/1400/1*5haNfed96VexQNtBOw2tYg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="8f30" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">where <em class="jr">R(a)</em> is the expected overall reward of action <em class="jr">a</em>, <em class="jr">t</em> is the number of steps taken (how many actions were selected overall), <em class="jr">N(a)</em> is the number of times action <em class="jr">a</em> was selected and <em class="jr">c</em> is a configureable hyperparameter. This method is also referred to sometimes as “exploration through optimism”, as it gives less-explored actions a higher value, encouraging the model to select them.</p><p id="680c" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph=""><strong class="jb lc">Value Function:</strong> Usually denoted as <em class="jr">V(s) </em>(sometimes with a π subscript), the Value function is a measure of the overall expected <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#6a6f" class="cg dj js jt ju jv" rel="noopener"><em class="jr">reward</em></a> assuming the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#b6a2" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Agent</em></a> is in <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#c274" class="cg dj js jt ju jv" rel="noopener"><em class="jr">state</em></a> <em class="jr">s</em> and then continues playing until the end of the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#601d" class="cg dj js jt ju jv" rel="noopener"><em class="jr">episode</em></a><em class="jr"> </em>following some <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#a76c" class="cg dj js jt ju jv" rel="noopener"><em class="jr">policy</em></a> π. It is defined mathematically as:</p><figure class="ie if ig ih ii ij ez fa paragraph-image"><div class="ez fa ld"><div class="ir r cd is"><div class="lh iu r"><div class="cc in s t u io ai bv ip iq"><img class="s t u io ai iv iw an tj" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_UOfJo9BjoQfNcC2x0hV_Zw.png" width="577" height="100" role="presentation"></div><img class="po sg s t u io ai iy" width="577" height="100" srcset="https://miro.medium.com/max/552/1*UOfJo9BjoQfNcC2x0hV_Zw.png 276w, https://miro.medium.com/max/1104/1*UOfJo9BjoQfNcC2x0hV_Zw.png 552w, https://miro.medium.com/max/1154/1*UOfJo9BjoQfNcC2x0hV_Zw.png 577w" sizes="577px" role="presentation" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_UOfJo9BjoQfNcC2x0hV_Zw(1).png"><noscript><img class="s t u io ai" src="https://miro.medium.com/max/1154/1*UOfJo9BjoQfNcC2x0hV_Zw.png" width="577" height="100" srcSet="https://miro.medium.com/max/552/1*UOfJo9BjoQfNcC2x0hV_Zw.png 276w, https://miro.medium.com/max/1104/1*UOfJo9BjoQfNcC2x0hV_Zw.png 552w, https://miro.medium.com/max/1154/1*UOfJo9BjoQfNcC2x0hV_Zw.png 577w" sizes="577px" role="presentation"/></noscript></div></div></div></figure><p id="8354" class="iz jl ct bj jb b gh jc jm gj jd jn je jf gu jg jh gv ji jj gw jk fh" data-selectable-paragraph="">While it does seem similar to the definition of <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#f366" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Q Value</em></a>, there is an implicit — yet important — difference: for <em class="jr">n=0</em>, the reward r⁰ of <em class="jr">V(s)</em> is the expected reward from just being in state <em class="jr">s</em>, <em class="jr">before</em> any action was played, while in Q Value, r⁰ is the expected reward <em class="jr">after</em> a certain action was played. This difference also yields the <a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e#efa6" class="cg dj js jt ju jv" rel="noopener"><em class="jr">Advantage function</em></a>.</p></div></div></section></div></article><div class="cc fg ll lm ai lt lr lu" data-test-id="post-sidebar"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="lv n fc"><div class="fg"><div class="lw lx r"><a href="https://towardsdatascience.com/?source=post_sidebar--------------------------post_sidebar-" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" rel="noopener"><h2 class="bi kn ly bk ct">Towards Data Science</h2></a><div class="lz ma r"><h4 class="bi ek cb bk bv mb bu hi mc hk bn">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="bx" aria-hidden="true"><button class="cs cu ar as ho bb bc hp ba df bi b bj bk bl bm dg dh di bx dj bd">Follow</button></div></div><div class="md me mf n"><div class="n o"><div class="r cd mg mh mi mj mk"><div class=""><button class="ay ml mm mn mo mp mq mr ib ms mt"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r mu mv mw mx my mz na"><div class="nb"><h4 class="bi ek cb bk bn"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl">943 </button></h4></div></div></div></div><div class="me r"></div><div><div class="ib"><div><div class="bx" role="tooltip" aria-hidden="true" aria-describedby="2" aria-labelledby="2"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="cc fg ll lm ln lo lp lq lr ls"><div class="ln r"><div class="rq rr ai r cd c f rs rt"><div class="r s ru v"><div class="az r cd v"><span class="bi b bj bk bl bm r bn bo"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" data-testid="close-button"><svg width="19" height="19" viewBox="0 0 19 19"><path d="M13.8 4.6L9.5 8.89 5.21 4.6l-.61.61 4.29 4.3-4.29 4.28.61.62 4.3-4.3 4.28 4.3.62-.62-4.3-4.29 4.3-4.29" fill-rule="evenodd"></path></svg></button></span></div></div><div class="rf r"><h2 class="rv rw ly pj ct">Your journey starts here.</h2></div><div class="n fc gy"><div class="rl rm rn ro rp"><a rel="noopener" href="https://towardsdatascience.com/machine-learning-engineers-will-not-exist-in-10-years-c9cbbf4472f3?source=read_next--------------------------post_sidebar-25"><div class="eb n fc p"><div class="rx r"><span class="bi ek el bk bn en pe"><h4 class="bi ek el bk ct"><div class="bw bu bv">Machine Learning</div></h4></span></div><div class="n nl"><img alt="" class="ry rz" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/0_2hfu1X4qX7n9tW7v" width="50" height="50" role="presentation"><div class="sa n fc"><h4 class="bi ek el bk bv sb bu hi sc hk ct">Machine Learning Engineers Will Not Exist In 10 Years.</h4></div></div></div></a></div><div class="rl rm rn ro rp"><a href="https://forge.medium.com/prepare-for-the-ultimate-gaslighting-6a8ce3f0a0e0?source=read_next--------------------------post_sidebar-25" rel="noopener"><div class="eb n fc p"><div class="rx r"><span class="bi ek el bk bn en pe"><h4 class="bi ek el bk ct"><div class="bw bu bv">Forge</div></h4></span></div><div class="n nl"><img alt="" class="ry rz" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_Ry6QAUccmwPfLLmf9n3qxA.jpeg" width="50" height="50" role="presentation"><div class="sa n fc"><h4 class="bi ek el bk bv sb bu hi sc hk ct">Prepare for the Ultimate Gaslighting*</h4></div></div></div></a></div><div class="rl rm rn ro rp"><a href="https://medium.com/@jurgenthoelen/belgian-dutch-study-why-in-times-of-covid-19-you-can-not-walk-run-bike-close-to-each-other-a5df19c77d08?source=read_next--------------------------post_sidebar-25" rel="noopener"><div class="eb n fc p"><div class="rx r"><span class="bi ek el bk bn en pe"><h4 class="bi ek el bk ct"><div class="bw bu bv"></div></h4></span></div><div class="n nl"><img alt="" class="ry rz" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_xR4JuvsEr8WFfBJM1qjyvg.jpeg" width="50" height="50" role="presentation"><div class="sa n fc"><h4 class="bi ek el bk bv sb bu hi sc hk ct">Belgian-Dutch Study: Why in times of COVID-19 you should not walk/run/bike close behind each other.</h4></div></div></div></a></div><div class="rl rm rn ro rp"><a href="https://medium.com/paperswithcode/a-home-for-results-in-ml-e25681c598dc?source=read_next--------------------------post_sidebar-25" rel="noopener"><div class="eb n fc p"><div class="rx r"><span class="bi ek el bk bn en pe"><h4 class="bi ek el bk ct"><div class="bw bu bv">Machine Learning</div></h4></span></div><div class="n nl"><img alt="" class="ry rz" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_fRt5m-OV7_M_OZMTEgtKJw.png" width="50" height="50" role="presentation"><div class="sa n fc"><h4 class="bi ek el bk bv sb bu hi sc hk ct">A Home For Results in ML</h4></div></div></div></a></div></div></div></div></div><div><div class="nc ij n fc p"><div class="n p"><div class="z ab ac ae af fm ah ai"><div class="n nd"></div><div class="n o nd"></div><div class="ne r"><ul class="ay az"><li class="bx dx ia nf"><a href="https://towardsdatascience.com/tagged/machine-learning" class="ng nh dj bn r ni nj a b el">Machine Learning</a></li><li class="bx dx ia nf"><a href="https://towardsdatascience.com/tagged/reinforcement-learning" class="ng nh dj bn r ni nj a b el">Reinforcement Learning</a></li><li class="bx dx ia nf"><a href="https://towardsdatascience.com/tagged/artificial-intelligence" class="ng nh dj bn r ni nj a b el">Artificial Intelligence</a></li><li class="bx dx ia nf"><a href="https://towardsdatascience.com/tagged/data-science" class="ng nh dj bn r ni nj a b el">Data Science</a></li><li class="bx dx ia nf"><a href="https://towardsdatascience.com/tagged/algorithms" class="ng nh dj bn r ni nj a b el">Algorithms</a></li></ul></div><div class="nk n gy y"><div class="n nl"><div class="nm r"><div class="n o"><div class="r cd nn no np nq nr"><div class=""><div class="c ns dm n o nt cd nu nv nw nx ny nz oa ob oc od oe of og oh"><button class="ay ml mm mn mo mp oi mr o iy dm n p oj u io s t ai ib ms mt ok"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="r mu mv mw mx my mz na"><div class="cd ol nb"><h4 class="bi ek cb bk ct"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl">943 claps</button></h4></div></div></div></div><div class="r om on oo op oq"></div></div><div class="n o"><div class="hz r bh"><a href="https://medium.com/p/e16230b7d24e/share/twitter?source=post_actions_footer---------------------------" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="hz r bh"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="hz r bh"><a href="https://medium.com/p/e16230b7d24e/share/facebook?source=post_actions_footer---------------------------" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="or r bh"><div><div class="ib"><div><div class="bx" role="tooltip" aria-hidden="true" aria-describedby="3" aria-labelledby="3"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="bx" aria-hidden="true"><div class="bx" aria-hidden="true"><div class="r bh"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div><div class="os ot ou ne r y"><div class="ov ow r cd"><span class="r ox al oy"><div class="r s oz pa"><a href="https://towardsdatascience.com/@shakedzy?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Shaked Zychlinski" class="r dm ed pb" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_hM8X1ekutC5l8TquiCA45g(1).jpeg" width="80" height="80"></a></div><span class="r"><div class="pc r pd"><p class="bi ek el bk bn en pe">Written by</p></div><div class="pc pf n pd"><div class="ai n o gy"><h2 class="bi kn kc pg ct"><a href="https://towardsdatascience.com/@shakedzy?source=follow_footer--------------------------follow_footer-" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" rel="noopener">Shaked Zychlinski</a></h2><div class="r g"><button class="cs cu ar as ho bb bc hp ba df bi b bj bk bl bm dg dh di bx dj bd">Follow</button></div></div></div></span></span><div class="pc ph r pd aq"><div class="pi r"><h4 class="bi ek ly pj bn">Algorithm Engineer at Taboola. Lives in Tel Aviv, Israel. Actual beard may vary. See me on shakedzy.xyz</h4></div><div class="ap pk aq"><button class="cs cu ar as ho bb bc hp ba df bi b bj bk bl bm dg dh di bx dj bd">Follow</button></div></div></div><div class="os r"></div><div class="ov ow r cd"><span class="r ox al oy"><div class="r s oz pa"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Towards Data Science" class="df pb ed" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_hVxgUA6kP-PgL5TJjuyePg.png" width="80" height="80"></a></div><span class="r"><div class="pc pf n pd"><div class="ai n o gy"><h2 class="bi kn kc pg ct"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" rel="noopener">Towards Data Science</a></h2><div class="r g"><div class="bx" aria-hidden="true"><button class="cs cu ar as ho bb bc hp ba df bi b bj bk bl bm dg dh di bx dj bd">Follow</button></div></div></div></div></span></span><div class="pc pl r pd aq"><div class="pi r"><h4 class="bi ek ly pj bn">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="ap pk aq"><div class="bx" aria-hidden="true"><button class="cs cu ar as ho bb bc hp ba df bi b bj bk bl bm dg dh di bx dj bd">Follow</button></div></div></div></div></div><div class="pm ot r y"><a href="https://medium.com/p/e16230b7d24e/responses/show?source=follow_footer--------------------------follow_footer-" class="cg ch at au av aw ax ay az ba ci cj bd be ck cl" rel="noopener"><span class="pn po mo"><div class="pp pq df r kf aq"><span class="ar">See responses (6)</span></div></span></a></div></div></div><div class="pr r ps y"><div class="n p"><div class="z ab ac ae af ag ah ai"><div class="tl r tm"><div class="rd lx ov r"><h2 class="bi kn tn to ct">More From Medium</h2></div><div class="ea n nl nd tp tq tr ts tt tu tv tw tx ty tz ua ub uc ud"><div class="ue uf ug uh ui uj uk ul um un uo up uq ur us ut uu uv uw ux uy"><div class="ai io"><div class="r uz"><div class="va vb tp tq tr vc vd ts tt tu ve vf tv tw tx vg vh ty tz ua vi vj ub uc ud n nd"><div class="ue uf ug uh ui uj vk vl um un vm vn uq ur vo vp uu uv vq vr uy"><div class="vs r vt f"><h4 class="bi ek cb bk bn">More from Towards Data Science</h4></div><div class="br r vu tm"><a class="cg ch at au av aw ax ay az ba ci cj bd be ck cl r" rel="noopener" href="https://towardsdatascience.com/bye-bye-python-hello-julia-9230bff0df62?source=post_recirc---------0------------------"></a></div></div><div class="ue uf ug uh ui uj vk vl um un vm vn uq ur vo vp uu uv vq vr uy"><div class="br r"><div class="rx ap h vv"><h4 class="bi ek cb bk bn">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/bye-bye-python-hello-julia-9230bff0df62?source=post_recirc---------0------------------"><h3 class="ct q fp rw bj vw vx vy">Bye-bye Python. Hello Julia!</h3></a></div><div class="n o gy"><div class="cm r dw"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="bi b bj bk bl bm r ct q"><div class="et n o hg"><span class="bi ek cb bk bv hh bu hi hj hk ct"><a href="https://towardsdatascience.com/@rheamoutafis?source=post_recirc---------0------------------" class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener">Rhea Moutafis</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------0------------------" class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="bi b bj bk bl bm r bn bo"><span class="bi ek cb bk bv hh bu hi hj hk bn"><div><a class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener" href="https://towardsdatascience.com/bye-bye-python-hello-julia-9230bff0df62?source=post_recirc---------0------------------">May 2</a> · 8 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="r cd mg mh mi mj mk"><div class=""><button class="ay ml mm mn mo mp mq sf ib ms mt"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r mu mv mw mx my mz na"><div class="nb"><h4 class="bi ek cb bk bn">9.4K</h4></div></div></div><div class="vz he cm ee wa r"></div><div class="ib"><div><div class="bx" role="tooltip" aria-hidden="true" aria-describedby="20" aria-labelledby="20"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ue uf ug uh ui uj uk ul um un uo up uq ur us ut uu uv uw ux uy"><div class="ai io"><div class="r uz"><div class="va vb tp tq tr vc vd ts tt tu ve vf tv tw tx vg vh ty tz ua vi vj ub uc ud n nd"><div class="ue uf ug uh ui uj vk vl um un vm vn uq ur vo vp uu uv vq vr uy"><div class="vs r vt f"><h4 class="bi ek cb bk bn">More from Towards Data Science</h4></div><div class="br r vu tm"><a class="cg ch at au av aw ax ay az ba ci cj bd be ck cl r" rel="noopener" href="https://towardsdatascience.com/dont-become-a-data-scientist-ee4769899025?source=post_recirc---------1------------------"></a></div></div><div class="ue uf ug uh ui uj vk vl um un vm vn uq ur vo vp uu uv vq vr uy"><div class="br r"><div class="rx ap h vv"><h4 class="bi ek cb bk bn">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/dont-become-a-data-scientist-ee4769899025?source=post_recirc---------1------------------"><h3 class="ct q fp rw bj vw vx vy">Don’t Become a Data Scientist</h3></a></div><div class="n o gy"><div class="cm r dw"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="bi b bj bk bl bm r ct q"><div class="et n o hg"><span class="bi ek cb bk bv hh bu hi hj hk ct"><a href="https://towardsdatascience.com/@chris.the.data.scientist?source=post_recirc---------1------------------" class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener">Chris</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------1------------------" class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="bi b bj bk bl bm r bn bo"><span class="bi ek cb bk bv hh bu hi hj hk bn"><div><a class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener" href="https://towardsdatascience.com/dont-become-a-data-scientist-ee4769899025?source=post_recirc---------1------------------">May 4</a> · 6 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="r cd mg mh mi mj mk"><div class=""><button class="ay ml mm mn mo mp mq sf ib ms mt"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r mu mv mw mx my mz na"><div class="nb"><h4 class="bi ek cb bk bn">6.1K</h4></div></div></div><div class="vz he cm ee wa r"></div><div class="ib"><div><div class="bx" role="tooltip" aria-hidden="true" aria-describedby="21" aria-labelledby="21"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="ue uf ug uh ui uj uk ul um un uo up uq ur us ut uu uv uw ux uy"><div class="ai io"><div class="r uz"><div class="va vb tp tq tr vc vd ts tt tu ve vf tv tw tx vg vh ty tz ua vi vj ub uc ud n nd"><div class="ue uf ug uh ui uj vk vl um un vm vn uq ur vo vp uu uv vq vr uy"><div class="vs r vt f"><h4 class="bi ek cb bk bn">More from Towards Data Science</h4></div><div class="br r vu tm"><a class="cg ch at au av aw ax ay az ba ci cj bd be ck cl r" rel="noopener" href="https://towardsdatascience.com/forget-about-python-learn-cobol-and-become-a-crisis-hero-7f15e75ff377?source=post_recirc---------2------------------"></a></div></div><div class="ue uf ug uh ui uj vk vl um un vm vn uq ur vo vp uu uv vq vr uy"><div class="br r"><div class="rx ap h vv"><h4 class="bi ek cb bk bn">More from Towards Data Science</h4></div><a rel="noopener" href="https://towardsdatascience.com/forget-about-python-learn-cobol-and-become-a-crisis-hero-7f15e75ff377?source=post_recirc---------2------------------"><h3 class="ct q fp rw bj vw vx vy">Forget about Python. Learn COBOL and become a crisis hero</h3></a></div><div class="n o gy"><div class="cm r dw"><div class="o n"><div></div><div class="ai r"><div class="n"><div style="flex: 1 1 0%;"><span class="bi b bj bk bl bm r ct q"><div class="et n o hg"><span class="bi ek cb bk bv hh bu hi hj hk ct"><a href="https://towardsdatascience.com/@rheamoutafis?source=post_recirc---------2------------------" class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener">Rhea Moutafis</a><span> in <a href="https://towardsdatascience.com/?source=post_recirc---------2------------------" class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener">Towards Data Science</a></span></span></div></span></div></div><span class="bi b bj bk bl bm r bn bo"><span class="bi ek cb bk bv hh bu hi hj hk bn"><div><a class="cg ch at au av aw ax ay az ba hl bd be ck cl" rel="noopener" href="https://towardsdatascience.com/forget-about-python-learn-cobol-and-become-a-crisis-hero-7f15e75ff377?source=post_recirc---------2------------------">May 6</a> · 5 min read<span style="padding-left: 4px;"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top: -2px;"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div></div><div class="n o"><div class="n o"><div class="r cd mg mh mi mj mk"><div class=""><button class="ay ml mm mn mo mp mq sf ib ms mt"><svg width="25" height="25" viewBox="0 0 25 25"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="r mu mv mw mx my mz na"><div class="nb"><h4 class="bi ek cb bk bn">1.7K</h4></div></div></div><div class="vz he cm ee wa r"></div><div class="ib"><div><div class="bx" role="tooltip" aria-hidden="true" aria-describedby="22" aria-labelledby="22"><button class="cg ch at au av aw ax ay az ba ci cj bd be ck cl"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="pt r pu pv"><section class="ez fa ai di r pw px py pz qa qb qc qd qe qf qg qh qi qj qk"><div class="ql qm ov n gy g"><div class="qn n gy"><div class="qo r qp"><div class="qq r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba qr qs bd be qt qu" rel="noopener"><h4 class="qv qw qx bi kn bj pj qy qz r">Discover <!-- -->Medium</h4></a></div><span class="bi b bj bk bl bm r ra rb">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba bd be qt qu rc" rel="noopener">Watch</a></span></div><div class="qo r qp"><div class="rd r"><a href="https://medium.com/topics?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba qr qs bd be qt qu" rel="noopener"><h4 class="qv qw qx bi kn bj pj qy qz r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="bi b bj bk bl bm r ra rb">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba bd be qt qu rc" rel="noopener">Explore</a></span></div><div class="qo r qp"><div class="qq r"><a href="https://medium.com/membership?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba qr qs bd be qt qu" rel="noopener"><h4 class="qv qw qx bi kn bj pj qy qz r">Become a member</h4></a></div><span class="bi b bj bk bl bm r ra rb">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba bd be qt qu rc" rel="noopener">Upgrade</a></span></div></div></div><div class="n fc"><div class="n o gy"><a href="https://medium.com/?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba qr qs bd be qt qu" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="qw"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="bi b bj bk bl bm r ra rb"><div class="re rf n gy rg al"><h4 class="bi ek ly pj qv"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba hl bd be qt qu" rel="noopener">About</a></h4><h4 class="bi ek ly pj qv"><a href="https://help.medium.com/?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba hl bd be qt qu" rel="noopener">Help</a></h4><h4 class="bi ek ly pj qv"><a href="https://medium.com/policy/9db0094a1e0f?source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba hl bd be qt qu" rel="noopener">Legal</a></h4></div></span></div><div class="ap rh ri al"><h4 class="bi ek ly pj ra">Get the Medium app</h4></div><div class="ap rh rj al rk"><div class="cq r"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba qr qs bd be qt qu" rel="noopener nofollow"><img alt="A button that says &#39;Download on the App Store&#39;, and if clicked it will lead you to the iOS App store" class="" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_M2FVPPidy2x386MRAE-EeA.png" width="135" height="41"></a></div><div class="r"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----e16230b7d24e----------------------" class="cg ch at au av aw ax ay az ba qr qs bd be qt qu" rel="noopener nofollow"><img alt="A button that says &#39;Get it on, Google Play&#39;, and if clicked it will lead you to the Google Play store" class="" src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/1_HyH8oIcJvXp7xzu5oF6dTg.png" width="135" height="41"></a></div></div></div></section></div></div></div><script>window.__BUILD_ID__ = "master-20200508-210621-418b6e18bf"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200508-210621-418b6e18bf","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200508-210621-418b6e18bf"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"master-20200508-210621-418b6e18bf","commit":"418b6e18bfeddc0fc985f6283e165504de9e4df5"}},"datacenter":"us"},"sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"internalLinksPostIds":["00","01"],"webpMiroImageIds":["1*y7gegIZOYlsnhWFJwIyDJw","1*ByGRQD1zlYXGS4YBYAoLVA","1*orNowUCqCER-BwaAXOZx0A","1*itOsotYFripvvRKY1itrVQ","1*b_LB1ifqWQ2x3JG_m3MJsg","1*AD7jcqVRun0Hhwmg0-Vqfg","1*0kJoJveqoxkYXEmdM2FZ3A","1*pTq8R2lALVUytg_k4y5CpQ","1*1_tSnwIHb_oPsU9vucJijQ","1*EBN0PWXjvaF2gRdk9fCvzA","1*Uxc2_wlnoVQNMQUhQaLVZg","1*ABOw4ARUQ90kwKfXqeVdXA","1*Ok2A1h7LmAtYjWVG9c8IMA","1*Kw1AUMFyy3AGJ1BbTdeyWg","1*2RZldaiJQXadc5zjscYncg","1*hPIJUpxe2QMvOMNcTlnOlQ","1*MZyvxFpPUvfBUfoNvFhRzg","1*2ROzqt2hXcYs6BuKjG2_nQ","1*NO9eMccT-vPrY8nylJ4PVw","2*6cf2Ep3P-r1vCrc-6Bc-vA","1*hVxgUA6kP-PgL5TJjuyePg","1*VGtACZSU6AxT3ugiNr-WGg","0*8dBf1Vy9mkDdcuwQ","1*5ciI2lDFX8sJanIJa6ppnA","1*Hqtfw2Juvf6Zb9uGimLLMg","1*suPSqLiNrJPCUbtdUwLnGw","1*dIANAeHtMxPlVO9awEN0Jw","1*N2KcM3GCLymsKxnSBXyG_g","1*fgNVzsUlPl9tA8ladAT-KA","1*h-La0GVOrPo6SrFpNWQLtw","1*3IPJZVYg-95RkV8H4DRjvA","1*DgnF3PmTVG14Oz_-8BWX_g","1*x_SKDZtUCcWMH9FB092srw","1*oQ4U4pCo7OUPMXXphwqzTg","1*fjC3jxxcmOwXLwqxraNHfw","1*DXy0NEVftDaLKDVG8dS3YQ","1*KbxEajPgdT9GhcWWGG8JmQ","1*e6oTrX0jQU0lPM_0Tt-oYw","1*oQ4U4pCo7OUPMXXphwqzTg"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"performanceTags":[],"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"covidCollectionId":"8a9336e5bb4","sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":["pandemic","epidemic","coronavirus","covid19","co-vid-19","containment","self-care","flatten-the-curve","public-health","virus","public-health-crisis","quarantine","self-quarantine","zika","corona","disease-prevention","wuhan","chinavirus","outbreak","influenza","socialdistancing","social-distance","flu","vaccines","healthcare","medicine","conspiracy-theories","conspiracy","virality","epidemia","pandemia","salud","corona-e-virus","coronavirus-covid19","covid-19","covid-19-symptoms","covid-19-crisis","covid-19-testing","covid-19-treatment","coronavirus-update","coronavirus-diaries"],"COVID_APPLICABLE_TOPIC_NAMES":["coronavirus"],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":["coronavirus","health"],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4"},"debug":{"requestId":"42012c88-ce83-4b02-83f7-9cf993884019","originalSpanCarrier":{"ot-tracer-spanid":"616f38e16e60adc0","ot-tracer-traceid":"5dbd06bc50f32a78","ot-tracer-sampled":"true"}},"session":{"user":{"id":"85164a79d2b9"},"xsrf":"78pjHp99c3ni","isSpoofed":false},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"referrerSource":"-----e16230b7d24e---------------------post_regwall-","currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle","susiModal":{"step":null,"operation":"register"},"postRead":false,"responsesOpen":false},"client":{"isBot":false,"isCustomDomain":true,"isEu":false,"isNativeMedium":false,"inAppBrowserName":"","supportsWebp":true},"multiVote":{"clapsPerPost":{}},"tracing":{}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"bane_add_user","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.9":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.10":{"name":"coronavirus_topic_recirc","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"covid_19_cdc_banner","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_android_cdc_banner","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_apple_sign_in","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_cdc_banner_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_cleansweep_cachev2_reads","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_cleansweep_double_writes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_confirm_sign_in","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_curation_priority_queue_experiment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_digest_feature_logging","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_digest_tagline","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.39":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_ev_mission_email_v3","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_ev_mission_trial_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_filter_expire_processor","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_footer_app_buttons","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_free_corona_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_global_susi_modal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_highlander_member_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.53":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_li_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_lite_about_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.64":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_lo_open_in_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_login_code_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_meter_payment_link","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_minimal_meter_v2","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_miro_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_ml_rank_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_more_on_coronavirus","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_mute","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"enable_olsen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"enable_open_in_app_regwall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.92":{"name":"enable_post_page_nav_stickiness_removal","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.94":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.95":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.96":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.96.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.96.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.97":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.97.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.97.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.98":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.98.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.98.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.99":{"name":"enable_sepia_to_olsen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.99.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.99.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.100":{"name":"enable_starspace","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.100.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.100.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.101":{"name":"enable_starspace_digest_app","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.101.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.101.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.102":{"name":"enable_starspace_ranker_starspace","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.102.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.102.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.103":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.103.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.103.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.104":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.104.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.104.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.105":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.105.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.105.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.106":{"name":"enable_topic_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.106.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.106.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.107":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.107.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.107.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.108":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.108.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.108.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.109":{"name":"enable_utc_fix_on_partner_program_dashboard","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.109.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.109.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.110":{"name":"exclude_curated_in_popular_topic","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.110.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.110.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.111":{"name":"featured_fc_and_ydr","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.111.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.111.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.112":{"name":"filter_low_scoring_users","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.112.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.112.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.113":{"name":"glyph_embed_commands","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.113.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.113.valueType":{"__typename":"VariantFlagString","value":"control"},"ROOT_QUERY.variantFlags.114":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.114.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.114.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.115":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.115.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.115.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.116":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.116.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.116.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.117":{"name":"limit_post_referrers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.117.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.117.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.118":{"name":"make_nav_sticky","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.118.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.118.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.119":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.119.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.119.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.120":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.120.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.120.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.121":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.121.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.121.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.122":{"name":"remove_post_post_similarity","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.122.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.122.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.123":{"name":"rex_refactor_2","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.123.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.123.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.124":{"name":"share_post_linkedin","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.124.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.124.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.125":{"name":"sign_up_with_email_button","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.125.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.125.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.126":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.126.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.126.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.127":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.127.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.127.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.128":{"name":"skip_sign_in_recaptcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.128.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.128.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.129":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.129.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.129.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.130":{"name":"xgboost_auto_suspend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.130.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.130.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.96","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.97","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.98","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.99","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.100","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.101","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.102","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.103","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.104","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.105","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.106","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.107","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.108","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.109","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.110","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.111","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.112","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.113","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.114","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.115","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.116","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.117","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.118","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.119","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.120","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.121","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.122","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.123","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.124","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.125","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.126","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.127","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.128","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.129","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.130","typename":"VariantFlag"}],"viewer":{"type":"id","generated":false,"id":"User:85164a79d2b9","typename":"User"},"meterPost({\"postId\":\"e16230b7d24e\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"e16230b7d24e\"})":{"type":"id","generated":false,"id":"Post:e16230b7d24e","typename":"Post"},"post({\"id\":\"e16230b7d24e\"})":{"type":"id","generated":false,"id":"Post:e16230b7d24e","typename":"Post"}},"User:85164a79d2b9":{"id":"85164a79d2b9","username":"ayushverma1321","name":"Ayushverma","imageId":"0*fSKu5zWydc5B9tfM","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"ayushverma1321@gmail.com","unverifiedEmail":"","createdAt":1589189777130,"isEligibleToViewNewResponses":false,"isMembershipTrialEligible":true,"__typename":"User"},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":["9350e1523031","fb3a0a44f30e","ffb9a89d57d0","e16230b7d24e"]},"maxUnlockCount":4,"unlocksRemaining":0},"Post:e16230b7d24e":{"__typename":"Post","id":"e16230b7d24e","creator":{"type":"id","generated":false,"id":"User:43218078e688","typename":"User"},"isLocked":true,"lockedSource":"LOCKED_POST_SOURCE_UGC_UNENROLLED","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"sequence":null,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fthe-complete-reinforcement-learning-dictionary-e16230b7d24e","canonicalUrl":"","content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}})":{"type":"id","generated":true,"id":"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}})","typename":"PostContent"},"firstPublishedAt":1550957511367,"isPublished":true,"layerCake":3,"primaryTopic":{"type":"id","generated":false,"id":"machine-learning","typename":"Topic"},"title":"The Complete Reinforcement Learning Dictionary","latestPublishedVersion":"154a1f387c6d","visibility":"LOCKED","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":13.093396226415093,"readingList":"READING_LIST_NONE","allowResponses":true,"clapCount":943,"viewerClapCount":null,"voterCount":205,"recommenders":[],"license":"ALL_RIGHTS_RESERVED","tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:reinforcement-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:artificial-intelligence","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:data-science","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:algorithms","typename":"Tag"}],"topics":[{"type":"id","generated":false,"id":"1af65db9c2f8","typename":"Topic"},{"type":"id","generated":false,"id":"1eca0103fff3","typename":"Topic"},{"type":"id","generated":false,"id":"ae5d4995e225","typename":"Topic"}],"postResponses":{"type":"id","generated":true,"id":"$Post:e16230b7d24e.postResponses","typename":"PostResponses"},"responsesCount":6,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":1550957508647,"isDistributionAlertDismissed":false,"audioVersionUrl":"","socialTitle":"","socialDek":"","metaDescription":"Whenever I begin learning a subject which is new to me, I find the hardest thing to cope with is its new terminology. Every field have many terms and definitions which are completely obscure to an…","latestPublishedAt":1574584128449,"previewContent":{"type":"id","generated":true,"id":"$Post:e16230b7d24e.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg","typename":"ImageMetadata"},"isShortform":false,"seoTitle":"","updatedAt":1574584128726,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isSuspended":false,"readNext":[{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.0","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.1","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.2","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.3","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.4","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.5","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.6","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.7","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.8","typename":"ReadNextItem"},{"type":"id","generated":true,"id":"Post:e16230b7d24e.readNext.9","typename":"ReadNextItem"}]},"User:43218078e688":{"id":"43218078e688","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Shaked Zychlinski","isFollowing":false,"username":"shakedzy","bio":"Algorithm Engineer at Taboola. Lives in Tel Aviv, Israel. Actual beard may vary. See me on shakedzy.xyz","imageId":"1*hM8X1ekutC5l8TquiCA45g.jpeg","mediumMemberAt":0,"isBlocking":false,"isMuting":false,"isPartnerProgramEnrolled":false,"twitterScreenName":"shakedzy"},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","isAuroraVisible":false,"domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","customStyleSheet":null,"colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","favicon":{"type":"id","generated":false,"id":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png","typename":"ImageMetadata"},"name":"Towards Data Science","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png","typename":"ImageMetadata"},"isAuroraEligible":false,"isEnrolledInHightower":false,"isNewsletterV3Enabled":true,"newsletterV3":{"type":"id","generated":false,"id":"NewsletterV3:d6fe9076899","typename":"NewsletterV3"},"creator":{"type":"id","generated":false,"id":"User:895063a310f4","typename":"User"},"viewerIsEditor":false,"navItems":[{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.6","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.7","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"viewerIsMuting":false,"description":"A Medium publication sharing concepts, ideas, and codes.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":true,"isUserSubscribedToCollectionEmails":false,"ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"tagline":"A Medium publication sharing concepts, ideas, and codes."},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","originalWidth":337,"originalHeight":122,"__typename":"ImageMetadata"},"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png":{"id":"1*hVxgUA6kP-PgL5TJjuyePg.png","__typename":"ImageMetadata"},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","slug":"monthly-edition","__typename":"NewsletterV3","isSubscribed":false,"showPromo":false,"name":"Monthly Edition","description":"Well-written and informative articles that you’ll be excited to read. Find our best content here, including tutorials, hands-on real-world examples, research, and cutting-edge techniques. It’s the best way to learn new things and stay up to date with Towards Data Science. "},"User:895063a310f4":{"id":"895063a310f4","__typename":"User"},"Collection:7f60cf5620c9.navItems.0":{"title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.1":{"title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.2":{"title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.3":{"title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.4":{"title":"AI","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fartificial-intelligence\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.5":{"title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.6":{"title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.7":{"title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF355876","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF355876","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF4D6C88","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF637F99","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF7791A8","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF8CA2B7","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF9FB3C6","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFB2C3D4","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFC5D2E1","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFD7E2EE","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFE9F1FA","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFBFFFF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum","typename":"ColorSpectrum"},"defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"}},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFEDF4FC","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFE9F2FD","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFE6F1FD","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFE2EFFD","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFDFEEFD","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFDBECFE","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFD7EBFE","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFD4E9FE","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFD0E7FF","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFCCE6FF","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFC8E4FF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF668AAA","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF61809D","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF5A7690","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF546C83","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF4D6275","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF455768","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF3D4C5A","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF34414C","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF2B353E","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF21282F","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF161B1F","point":1,"__typename":"ColorPoint"},"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}}).bodyModel","typename":"RichText"}},"machine-learning":{"name":"Machine Learning","slug":"machine-learning","__typename":"Topic","isFollowing":null},"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}}).bodyModel.sections.0":{"name":"db38","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}}).bodyModel.sections.1":{"name":"d40c","startIndex":10,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}}).bodyModel.sections.0","typename":"Section"},{"type":"id","generated":true,"id":"$Post:e16230b7d24e.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fmedium.com\u002Fm\u002Fcallback\u002Fgoogle\",\"source\":\"-----e16230b7d24e---------------------post_regwall-\"}}).bodyModel.sections.1","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_61","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_62","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_63","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_64","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_65","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_66","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_67","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_68","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_69","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:154a1f387c6d_70","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:154a1f387c6d_0":{"id":"154a1f387c6d_0","name":"7184","type":"H3","href":null,"layout":null,"metadata":null,"text":"The Complete Reinforcement Learning Dictionary","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_1":{"id":"154a1f387c6d_1","name":"b16a","type":"H4","href":null,"layout":null,"metadata":null,"text":"The Reinforcement Learning Terminology, A to Z","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_2":{"id":"154a1f387c6d_2","name":"929d","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg":{"id":"1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg","originalHeight":1440,"originalWidth":1920,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_3":{"id":"154a1f387c6d_3","name":"1d7a","type":"P","href":null,"layout":null,"metadata":null,"text":"Whenever I begin learning a subject which is new to me, I find the hardest thing to cope with is its new terminology. Every field have many terms and definitions which are completely obscure to an outsider, and can make a newcomer’s first step quite difficult.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_4":{"id":"154a1f387c6d_4","name":"30b0","type":"P","href":null,"layout":null,"metadata":null,"text":"When I made my first step into the world or Reinforcement Learning, I was quite overwhelmed by the new terms which popped-up every other line, and it always surprised me how behind those complex words stood quite simple and logical ideas. I therefore decided to write them all down in my own words, so I’ll always be able to look them up in case I forget. This is how this dictionary came to be.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_5":{"id":"154a1f387c6d_5","name":"f7f0","type":"P","href":null,"layout":null,"metadata":null,"text":"This is not an introduction post to Reinforcement Learning, rather it’s a supplementary tool to assist while studying. If you do look to start your path in this field too, I can recommend the following:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_6":{"id":"154a1f387c6d_6","name":"8c81","type":"ULI","href":null,"layout":null,"metadata":null,"text":"If you’re looking for a quick, 10-minutes crash course into RL with code examples, checkout my Qrash Course series: Introduction to RL and Q-Learning and Policy Gradients and Actor-Critics.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_6.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_6.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_6.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_6.markups.0":{"type":"A","start":116,"end":149,"href":"https:\u002F\u002Fmedium.com\u002F@shakedzy\u002Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_6.markups.1":{"type":"A","start":154,"end":188,"href":"https:\u002F\u002Fmedium.com\u002F@shakedzy\u002Fqrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_6.markups.2":{"type":"EM","start":95,"end":107,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_7":{"id":"154a1f387c6d_7","name":"5f07","type":"ULI","href":null,"layout":null,"metadata":null,"text":"I you’re into something deeper, and would like to learn and code several different RL algorithms and gain more intuition, I can recommend this series by Thomas Simonini and this series by Arthur Juliani.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_7.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_7.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_7.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_7.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_7.markups.0":{"type":"A","start":138,"end":149,"href":"https:\u002F\u002Fmedium.com\u002Ffreecodecamp\u002Fan-introduction-to-reinforcement-learning-4339519de419","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_7.markups.1":{"type":"A","start":153,"end":168,"href":"https:\u002F\u002Fmedium.com\u002F@thomassimonini","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_7.markups.2":{"type":"A","start":173,"end":184,"href":"https:\u002F\u002Fmedium.com\u002Femergent-future\u002Fsimple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_7.markups.3":{"type":"A","start":188,"end":202,"href":"https:\u002F\u002Fmedium.com\u002F@awjuliani","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_8":{"id":"154a1f387c6d_8","name":"a368","type":"ULI","href":null,"layout":null,"metadata":null,"text":"If you’re ready to master RL, I will direct you to the “bible” of Reinforcement Learning — “Reinforcement Learning, an introduction” by Richard Sutton and Andrew Barto. The second edition (from 2018) is available for free (legally) as a PDF file.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_8.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_8.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_8.markups.0":{"type":"A","start":237,"end":245,"href":"https:\u002F\u002Fweb.stanford.edu\u002Fclass\u002Fpsych209\u002FReadings\u002FSuttonBartoIPRLBook2ndEd.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_8.markups.1":{"type":"EM","start":91,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_9":{"id":"154a1f387c6d_9","name":"8db4","type":"P","href":null,"layout":null,"metadata":null,"text":"I will do my best to try and keep on updating this dictionary. Feel free to let me know if I’ve missed anything important or got something wrong.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_10":{"id":"154a1f387c6d_10","name":"6195","type":"H3","href":null,"layout":null,"metadata":null,"text":"The Dictionary","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_11":{"id":"154a1f387c6d_11","name":"c678","type":"P","href":null,"layout":null,"metadata":null,"text":"Action-Value Function: See Q-Value.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_11.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_11.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_11.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_11.markups.0":{"type":"A","start":27,"end":34,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_11.markups.1":{"type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_11.markups.2":{"type":"EM","start":27,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12":{"id":"154a1f387c6d_12","name":"8751","type":"P","href":null,"layout":null,"metadata":null,"text":"Actions: Actions are the Agent’s methods which allow it to interact and change its environment, and thus transfer between states. Every action performed by the Agent yields a reward from the environment. The decision of which action to choose is made by the policy.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_12.markups.10","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_12.markups.0":{"type":"A","start":25,"end":30,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.1":{"type":"A","start":83,"end":94,"href":"#4311","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.2":{"type":"A","start":122,"end":128,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.3":{"type":"A","start":175,"end":181,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.4":{"type":"A","start":258,"end":264,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.5":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.6":{"type":"EM","start":25,"end":33,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.7":{"type":"EM","start":83,"end":94,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.8":{"type":"EM","start":122,"end":128,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.9":{"type":"EM","start":175,"end":181,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_12.markups.10":{"type":"EM","start":258,"end":264,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13":{"id":"154a1f387c6d_13","name":"8f35","type":"P","href":null,"layout":null,"metadata":null,"text":"Actor-Critic: When attempting to solve a Reinforcement Learning problem, there are two main methods one can choose from: calculating the Value Functions or Q-Values of each state and choosing actions according to those, or directly compute a policy which defines the probabilities each action should be taken depending on the current state, and act according to it. Actor-Critic algorithms combine the two methods in order to create a more robust method. A great illustrated-comics explanation can be found here.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_13.markups.9","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_13.markups.0":{"type":"A","start":41,"end":63,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.1":{"type":"A","start":137,"end":152,"href":"#680c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.2":{"type":"A","start":156,"end":164,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.3":{"type":"A","start":242,"end":248,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.4":{"type":"A","start":507,"end":511,"href":"https:\u002F\u002Fhackernoon.com\u002Fintuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.5":{"type":"STRONG","start":0,"end":14,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.6":{"type":"EM","start":41,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.7":{"type":"EM","start":137,"end":152,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.8":{"type":"EM","start":156,"end":164,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_13.markups.9":{"type":"EM","start":242,"end":248,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_14":{"id":"154a1f387c6d_14","name":"efa6","type":"P","href":null,"layout":null,"metadata":null,"text":"Advantage Function: Usually denoted as A(s,a), the Advantage function is a measure of how much is a certain action a good or bad decision given a certain state — or more simply, what is the advantage of selecting a certain action from a certain state. It is defined mathematically as:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_14.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_14.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_14.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_14.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_14.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_14.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_14.markups.0":{"type":"A","start":108,"end":114,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_14.markups.1":{"type":"A","start":154,"end":159,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_14.markups.2":{"type":"STRONG","start":0,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_14.markups.3":{"type":"EM","start":39,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_14.markups.4":{"type":"EM","start":108,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_14.markups.5":{"type":"EM","start":154,"end":159,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_15":{"id":"154a1f387c6d_15","name":"5ccd","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*CbxBGj9dxC741D0z0DL2Kg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*CbxBGj9dxC741D0z0DL2Kg.png":{"id":"1*CbxBGj9dxC741D0z0DL2Kg.png","originalHeight":56,"originalWidth":577,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_16":{"id":"154a1f387c6d_16","name":"c06e","type":"P","href":null,"layout":null,"metadata":null,"text":"where r(s,a) is the expected reward of action a from state s, and r(s) is the expected reward of the entire state s, before an action was selected. It can also be viewed as:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_16.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_16.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_16.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_16.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_16.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_16.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_16.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_16.markups.0":{"type":"A","start":29,"end":35,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_16.markups.1":{"type":"EM","start":6,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_16.markups.2":{"type":"EM","start":29,"end":35,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_16.markups.3":{"type":"EM","start":46,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_16.markups.4":{"type":"EM","start":59,"end":60,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_16.markups.5":{"type":"EM","start":66,"end":70,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_16.markups.6":{"type":"EM","start":114,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_17":{"id":"154a1f387c6d_17","name":"10a8","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*GuzTDUuNkuEboEhhdILnEQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*GuzTDUuNkuEboEhhdILnEQ.png":{"id":"1*GuzTDUuNkuEboEhhdILnEQ.png","originalHeight":56,"originalWidth":577,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_18":{"id":"154a1f387c6d_18","name":"d10c","type":"P","href":null,"layout":null,"metadata":null,"text":"where Q(s,a) is the Q Value and V(s) is the Value function.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_18.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_18.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_18.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_18.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_18.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_18.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_18.markups.0":{"type":"A","start":20,"end":27,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_18.markups.1":{"type":"A","start":44,"end":58,"href":"#680c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_18.markups.2":{"type":"EM","start":6,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_18.markups.3":{"type":"EM","start":20,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_18.markups.4":{"type":"EM","start":32,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_18.markups.5":{"type":"EM","start":44,"end":59,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19":{"id":"154a1f387c6d_19","name":"b6a2","type":"P","href":null,"layout":null,"metadata":null,"text":"Agent: The learning and acting part of a Reinforcement Learning problem, which tries to maximize the rewards it is given by the Environment. Putting it simply, the Agent is the model which you try to design.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_19.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_19.markups.0":{"type":"A","start":24,"end":30,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.1":{"type":"A","start":41,"end":63,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.2":{"type":"A","start":101,"end":108,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.3":{"type":"A","start":128,"end":139,"href":"#4311","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.4":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.5":{"type":"EM","start":24,"end":30,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.6":{"type":"EM","start":41,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.7":{"type":"EM","start":101,"end":108,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_19.markups.8":{"type":"EM","start":128,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20":{"id":"154a1f387c6d_20","name":"790b","type":"P","href":null,"layout":null,"metadata":null,"text":"Bandits: Formally named “k-Armed Bandits” after the nickname “one-armed bandit” given to slot-machines, these are considered to be the simplest type of Reinforcement Learning tasks. Bandits have no different states, but only one — and the reward taken under consideration is only the immediate one. Hence, bandits can be thought of as having single-state episodes. Each of the k-arms is considered an action, and the objective is to learn the policy which will maximize the expected reward after each action (or arm-pulling). \nContextual Bandits are a slightly more complex task, where each state may be different and affect the outcome of the actions — hence every time the context is different. Still, the task remains a single-state episodic task, and one context cannot have an influence on others.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_20.markups.15","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_20.markups.0":{"type":"A","start":89,"end":102,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FSlot_machine","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.1":{"type":"A","start":152,"end":174,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.2":{"type":"A","start":208,"end":214,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.3":{"type":"A","start":239,"end":245,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.4":{"type":"A","start":355,"end":363,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.5":{"type":"A","start":401,"end":407,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.6":{"type":"A","start":443,"end":449,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.7":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.8":{"type":"EM","start":152,"end":175,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.9":{"type":"EM","start":208,"end":214,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.10":{"type":"EM","start":239,"end":245,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.11":{"type":"EM","start":355,"end":363,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.12":{"type":"EM","start":401,"end":407,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.13":{"type":"EM","start":443,"end":449,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.14":{"type":"EM","start":527,"end":545,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_20.markups.15":{"type":"EM","start":675,"end":682,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21":{"id":"154a1f387c6d_21","name":"fbd3","type":"P","href":null,"layout":null,"metadata":null,"text":"Bellman Equation: Formally, Bellman equation defines the relationships between a given state (or state-action pair) to its successors. While many forms exist, the most common one usually encountered in Reinforcement Learning tasks is the Bellman equation for the optimal Q-Value , which is given by:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_21.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_21.markups.0":{"type":"A","start":87,"end":92,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.1":{"type":"A","start":103,"end":109,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.2":{"type":"A","start":202,"end":224,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.3":{"type":"A","start":271,"end":278,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.4":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.5":{"type":"EM","start":87,"end":92,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.6":{"type":"EM","start":103,"end":109,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.7":{"type":"EM","start":202,"end":224,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_21.markups.8":{"type":"EM","start":271,"end":279,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_22":{"id":"154a1f387c6d_22","name":"0b2f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Re6kADukp4wKFEnGzhImzw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Re6kADukp4wKFEnGzhImzw.png":{"id":"1*Re6kADukp4wKFEnGzhImzw.png","originalHeight":128,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_23":{"id":"154a1f387c6d_23","name":"0abe","type":"P","href":null,"layout":null,"metadata":null,"text":"or when no uncertainty exists (meaning, probabilities are either 1 or 0):","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_24":{"id":"154a1f387c6d_24","name":"e173","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*HTYLQwcKYerhL9620zALMA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*HTYLQwcKYerhL9620zALMA.png":{"id":"1*HTYLQwcKYerhL9620zALMA.png","originalHeight":128,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_25":{"id":"154a1f387c6d_25","name":"49d8","type":"P","href":null,"layout":null,"metadata":null,"text":"where the asterisk sign indicates optimal value. Some algorithms, such as Q-Learning, are basing their learning procedure over it.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_25.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_25.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_25.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_25.markups.0":{"type":"A","start":74,"end":84,"href":"#9d8f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_25.markups.1":{"type":"EM","start":34,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_25.markups.2":{"type":"EM","start":74,"end":84,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_26":{"id":"154a1f387c6d_26","name":"2692","type":"P","href":null,"layout":null,"metadata":null,"text":"Continuous Tasks: Reinforcement Learning tasks which are not made of episodes, but rather last forever. This tasks have no terminal states. For simplicity, they are usually assumed to be made of one never-ending episode.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_26.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_26.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_26.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_26.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_26.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_26.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_26.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_26.markups.0":{"type":"A","start":18,"end":40,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_26.markups.1":{"type":"A","start":69,"end":77,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_26.markups.2":{"type":"A","start":132,"end":137,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_26.markups.3":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_26.markups.4":{"type":"EM","start":18,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_26.markups.5":{"type":"EM","start":69,"end":77,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_26.markups.6":{"type":"EM","start":131,"end":139,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_27":{"id":"154a1f387c6d_27","name":"ac07","type":"P","href":null,"layout":null,"metadata":null,"text":"Deep Q-Networks (DQN): See Q-Learning","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_27.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_27.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_27.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_27.markups.0":{"type":"A","start":27,"end":37,"href":"#9d8f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_27.markups.1":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_27.markups.2":{"type":"EM","start":27,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_28":{"id":"154a1f387c6d_28","name":"aa91","type":"P","href":null,"layout":null,"metadata":null,"text":"Deep Reinforcement Learning: The use of a Reinforcement Learning algorithm with a deep neural network as an approximator for the learning part. This is usually done in order to cope with problems where the number of possible states and actions scales fast, and an exact solution in no longer feasible.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_28.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_28.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_28.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_28.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_28.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_28.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_28.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_28.markups.0":{"type":"A","start":42,"end":64,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_28.markups.1":{"type":"A","start":225,"end":231,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_28.markups.2":{"type":"A","start":236,"end":243,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_28.markups.3":{"type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_28.markups.4":{"type":"EM","start":42,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_28.markups.5":{"type":"EM","start":225,"end":231,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_28.markups.6":{"type":"EM","start":236,"end":243,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_29":{"id":"154a1f387c6d_29","name":"4ee6","type":"P","href":null,"layout":null,"metadata":null,"text":"Discount Factor (γ): The discount factor, usually denoted as γ, is a factor multiplying the future expected reward, and varies on the range of [0,1]. It controls the importance of the future rewards versus the immediate ones. The lower the discount factor is, the less important future rewards are, and the Agent will tend to focus on actions which will yield immediate rewards only.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_29.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_29.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_29.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_29.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_29.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_29.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_29.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_29.markups.0":{"type":"A","start":108,"end":114,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_29.markups.1":{"type":"A","start":307,"end":312,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_29.markups.2":{"type":"A","start":335,"end":342,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_29.markups.3":{"type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_29.markups.4":{"type":"EM","start":108,"end":114,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_29.markups.5":{"type":"EM","start":307,"end":313,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_29.markups.6":{"type":"EM","start":335,"end":342,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30":{"id":"154a1f387c6d_30","name":"4311","type":"P","href":null,"layout":null,"metadata":null,"text":"Environment: Everything which isn’t the Agent; everything the Agent can interact with, either directly or indirectly. The environment changes as the Agent performs actions; every such change is considered a state-transition. Every action the Agent performs yields a reward received by the Agent.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_30.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_30.markups.0":{"type":"A","start":40,"end":45,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.1":{"type":"A","start":164,"end":171,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.2":{"type":"A","start":207,"end":212,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.3":{"type":"A","start":266,"end":272,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.4":{"type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.5":{"type":"EM","start":40,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.6":{"type":"EM","start":164,"end":171,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.7":{"type":"EM","start":207,"end":212,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_30.markups.8":{"type":"EM","start":266,"end":272,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_31":{"id":"154a1f387c6d_31","name":"601d","type":"P","href":null,"layout":null,"metadata":null,"text":"Episode: All states that come in between an initial-state and a terminal-state; for example: one game of Chess. The Agent’s goal it to maximize the total reward it receives during an episode. In situations where there is no terminal-state, we consider an infinite episode. It is important to remember that different episodes are completely independent of one another.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_31.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_31.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_31.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_31.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_31.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_31.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_31.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_31.markups.0":{"type":"A","start":13,"end":19,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_31.markups.1":{"type":"A","start":116,"end":121,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_31.markups.2":{"type":"A","start":154,"end":160,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_31.markups.3":{"type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_31.markups.4":{"type":"EM","start":13,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_31.markups.5":{"type":"EM","start":116,"end":123,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_31.markups.6":{"type":"EM","start":154,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_32":{"id":"154a1f387c6d_32","name":"15bc","type":"P","href":null,"layout":null,"metadata":null,"text":"Episodic Tasks: Reinforcement Learning tasks which are made of different episodes (meaning, each episode has a terminal state).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_32.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_32.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_32.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_32.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_32.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_32.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_32.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_32.markups.0":{"type":"A","start":16,"end":38,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_32.markups.1":{"type":"A","start":73,"end":81,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_32.markups.2":{"type":"A","start":120,"end":125,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_32.markups.3":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_32.markups.4":{"type":"EM","start":15,"end":38,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_32.markups.5":{"type":"EM","start":73,"end":81,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_32.markups.6":{"type":"EM","start":119,"end":125,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_33":{"id":"154a1f387c6d_33","name":"a3de","type":"P","href":null,"layout":null,"metadata":null,"text":"Expected Return: Sometimes referred to as “overall reward” and occasionally denoted as G, is the expected reward over an entire episode.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_33.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_33.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_33.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_33.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_33.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_33.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_33.markups.0":{"type":"A","start":106,"end":112,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_33.markups.1":{"type":"A","start":128,"end":135,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_33.markups.2":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_33.markups.3":{"type":"EM","start":87,"end":88,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_33.markups.4":{"type":"EM","start":106,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_33.markups.5":{"type":"EM","start":128,"end":135,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34":{"id":"154a1f387c6d_34","name":"5ecb","type":"P","href":null,"layout":null,"metadata":null,"text":"Experience Replay: As Reinforcement Learning tasks have no pre-generated training sets which they can learn from, the Agent must keep records of all the state-transitions it encountered so it can learn from them later. The memory-buffer used to store this is often referred to as Experience Replay. There are several types and architectures of these memory buffers — but some very common ones are the cyclic memory buffers (which makes sure the Agent keeps training over its new behavior rather than things that might no longer be relevant) and reservoir-sampling-based memory buffers (which guarantees each state-transition recorded has an even probability to be inserted to the buffer).","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_34.markups.9","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_34.markups.0":{"type":"A","start":22,"end":44,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.1":{"type":"A","start":118,"end":123,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.2":{"type":"A","start":153,"end":158,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.3":{"type":"A","start":401,"end":422,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCircular_buffer","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.4":{"type":"A","start":545,"end":563,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FReservoir_sampling","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.5":{"type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.6":{"type":"EM","start":22,"end":44,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.7":{"type":"EM","start":118,"end":123,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.8":{"type":"EM","start":153,"end":159,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_34.markups.9":{"type":"EM","start":280,"end":297,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35":{"id":"154a1f387c6d_35","name":"b903","type":"P","href":null,"layout":null,"metadata":null,"text":"Exploitation & Exploration: Reinforcement Learning tasks have no pre-generated training sets which they can learn from — they create their own experience and learn “on the fly”. To be able to do so, the Agent needs to try many different actions in many different states in order to try and learn all available possibilities and find the path which will maximize its overall reward; this is known as Exploration, as the Agent explores the Environment. On the other hand, if all the Agent will do is explore, it will never maximize the overall reward — it must also use the information it learned to do so. This is known as Exploitation, as the Agent exploits its knowledge to maximize the rewards it receives. \nThe trade-off between the two is one of the greatest challenges of Reinforcement Learning problems, as the two must be balanced in order to allow the Agent to both explore the environment enough, but also exploit what it learned and repeat the most rewarding path it found.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_35.markups.15","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_35.markups.0":{"type":"A","start":28,"end":50,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.1":{"type":"A","start":143,"end":153,"href":"#5ecb","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.2":{"type":"A","start":203,"end":208,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.3":{"type":"A","start":263,"end":269,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.4":{"type":"A","start":374,"end":380,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.5":{"type":"A","start":438,"end":449,"href":"#4311","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.6":{"type":"STRONG","start":0,"end":27,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.7":{"type":"EM","start":28,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.8":{"type":"EM","start":143,"end":153,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.9":{"type":"EM","start":203,"end":208,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.10":{"type":"EM","start":237,"end":244,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.11":{"type":"EM","start":263,"end":269,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.12":{"type":"EM","start":374,"end":380,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.13":{"type":"EM","start":399,"end":410,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.14":{"type":"EM","start":438,"end":449,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_35.markups.15":{"type":"EM","start":622,"end":634,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36":{"id":"154a1f387c6d_36","name":"15c5","type":"P","href":null,"layout":null,"metadata":null,"text":"Greedy Policy, ε-Greedy Policy: A greedy policy means the Agent constantly performs the action that is believed to yield the highest expected reward. Obviously, such a policy will not allow the Agent to explore at all. In order to still allow some exploration, an ε-greedy policy is often used instead: a number (named ε) in the range of [0,1] is selected, and prior selecting an action, a random number in the range of [0,1] is selected. if that number is larger than ε, the greedy action is selected — but if it’s lower, a random action is selected. Note that if ε=0, the policy becomes the greedy policy, and if ε=1, always explore.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.16","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_36.markups.17","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_36.markups.0":{"type":"A","start":41,"end":47,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.1":{"type":"A","start":58,"end":63,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.2":{"type":"A","start":88,"end":94,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.3":{"type":"A","start":142,"end":148,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.4":{"type":"A","start":203,"end":210,"href":"#b903","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.5":{"type":"STRONG","start":0,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.6":{"type":"EM","start":15,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.7":{"type":"EM","start":41,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.8":{"type":"EM","start":58,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.9":{"type":"EM","start":88,"end":94,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.10":{"type":"EM","start":142,"end":148,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.11":{"type":"EM","start":203,"end":211,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.12":{"type":"EM","start":264,"end":266,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.13":{"type":"EM","start":319,"end":320,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.14":{"type":"EM","start":321,"end":322,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.15":{"type":"EM","start":469,"end":470,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.16":{"type":"EM","start":565,"end":566,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_36.markups.17":{"type":"EM","start":615,"end":616,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_37":{"id":"154a1f387c6d_37","name":"42c0","type":"P","href":null,"layout":null,"metadata":null,"text":"k-Armed Bandits: See Bandits.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_37.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_37.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_37.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_37.markups.0":{"type":"A","start":21,"end":28,"href":"#790b","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_37.markups.1":{"type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_37.markups.2":{"type":"EM","start":21,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38":{"id":"154a1f387c6d_38","name":"4a1d","type":"P","href":null,"layout":null,"metadata":null,"text":"Markov Decision Process (MDP): The Markov Property means that each state is dependent solely on its preceding state, the selected action taken from that state and the reward received immediately after that action was executed. Mathematically, it means: s’ = s’(s,a,r), where s’ is the future state, s is its preceding state and a and r are the action and reward. No prior knowledge of what happened before s is needed — the Markov Property assumes that s holds all the relevant information within it. A Markov Decision Process is decision process based on these assumptions.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_38.markups.13","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_38.markups.0":{"type":"A","start":67,"end":72,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.1":{"type":"A","start":130,"end":136,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.2":{"type":"A","start":167,"end":173,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.3":{"type":"STRONG","start":0,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.4":{"type":"EM","start":67,"end":73,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.5":{"type":"EM","start":130,"end":136,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.6":{"type":"EM","start":167,"end":173,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.7":{"type":"EM","start":253,"end":267,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.8":{"type":"EM","start":275,"end":277,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.9":{"type":"EM","start":299,"end":300,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.10":{"type":"EM","start":328,"end":329,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.11":{"type":"EM","start":334,"end":336,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.12":{"type":"EM","start":406,"end":407,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_38.markups.13":{"type":"EM","start":453,"end":455,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39":{"id":"154a1f387c6d_39","name":"6d88","type":"P","href":null,"layout":null,"metadata":null,"text":"Model-Based & Model-Free: Model-based and model-free are two different approaches an Agent can choose when trying to optimize its policy. This is best explained using an example: assume you’re trying to learn how to play Blackjack. You can do so in two ways: one, you calculate in advance, before the game begins, the winning probabilities of all states and all the state-transition probabilities given all the possible actions, and then simply act according to you calculations. The second option is to simply play without any prior knowledge, and gain information using “trial-and-error”. Note that using the first approach, you’re basically modeling your environment, while the second approach requires no information about the environment. This is exactly the difference between model-based and model-free; the first method is model-based, while the latter is model-free.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_39.markups.12","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_39.markups.0":{"type":"A","start":85,"end":90,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.1":{"type":"A","start":130,"end":136,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.2":{"type":"A","start":221,"end":230,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBlackjack","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.3":{"type":"A","start":347,"end":353,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.4":{"type":"A","start":420,"end":427,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.5":{"type":"A","start":658,"end":669,"href":"#4311","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.6":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.7":{"type":"EM","start":85,"end":90,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.8":{"type":"EM","start":130,"end":136,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.9":{"type":"EM","start":347,"end":353,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.10":{"type":"EM","start":420,"end":427,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.11":{"type":"EM","start":644,"end":652,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_39.markups.12":{"type":"EM","start":658,"end":669,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40":{"id":"154a1f387c6d_40","name":"f9e0","type":"P","href":null,"layout":null,"metadata":null,"text":"Monte Carlo (MC): Monte Carlo methods are algorithms which use repeated random sampling in order to achieve a result. They are used quite often in Reinforcement Learning algorithms to obtain expected values; for example — calculating a state Value function by returning to the same state over and over again, and averaging over the actual cumulative reward received each time.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_40.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_40.markups.0":{"type":"A","start":147,"end":169,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.1":{"type":"A","start":236,"end":241,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.2":{"type":"A","start":242,"end":256,"href":"#680c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.3":{"type":"A","start":350,"end":356,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.4":{"type":"STRONG","start":0,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.5":{"type":"EM","start":147,"end":169,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.6":{"type":"EM","start":236,"end":241,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.7":{"type":"EM","start":242,"end":256,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_40.markups.8":{"type":"EM","start":350,"end":356,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41":{"id":"154a1f387c6d_41","name":"44d1","type":"P","href":null,"layout":null,"metadata":null,"text":"On-Policy & Off-Policy: Every Reinforcement Learning algorithm must follow some policy in order to decide which actions to perform at each state. Still, the learning procedure of the algorithm doesn’t have to take into account that policy while learning. Algorithms which concern about the policy which yielded past state-action decisions are referred to as on-policy algorithms, while those ignoring it are known as off-policy. \nA well known off-policy algorithm is Q-Learning, as its update rule uses the action which will yield the highest Q-Value, while the actual policy used might restrict that action or choose another. The on-policy variation of Q-Learning is known as Sarsa, where the update rule uses the action chosen by the followed policy.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_41.markups.16","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_41.markups.0":{"type":"A","start":30,"end":52,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.1":{"type":"A","start":80,"end":86,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.2":{"type":"A","start":112,"end":119,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.3":{"type":"A","start":139,"end":144,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.4":{"type":"A","start":467,"end":477,"href":"#9d8f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.5":{"type":"A","start":543,"end":550,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.6":{"type":"A","start":677,"end":682,"href":"#da43","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.7":{"type":"STRONG","start":0,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.8":{"type":"EM","start":30,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.9":{"type":"EM","start":80,"end":86,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.10":{"type":"EM","start":112,"end":119,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.11":{"type":"EM","start":139,"end":144,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.12":{"type":"EM","start":358,"end":367,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.13":{"type":"EM","start":417,"end":427,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.14":{"type":"EM","start":467,"end":477,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.15":{"type":"EM","start":543,"end":550,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_41.markups.16":{"type":"EM","start":677,"end":682,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_42":{"id":"154a1f387c6d_42","name":"04db","type":"P","href":null,"layout":null,"metadata":null,"text":"One-Armed Bandits: See Bandits.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_42.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_42.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_42.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_42.markups.0":{"type":"A","start":23,"end":30,"href":"#790b","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_42.markups.1":{"type":"STRONG","start":0,"end":19,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_42.markups.2":{"type":"EM","start":23,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_43":{"id":"154a1f387c6d_43","name":"532b","type":"P","href":null,"layout":null,"metadata":null,"text":"One-Step TD: See Temporal Difference.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_43.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_43.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_43.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_43.markups.0":{"type":"A","start":17,"end":36,"href":"#d0d7","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_43.markups.1":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_43.markups.2":{"type":"EM","start":17,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44":{"id":"154a1f387c6d_44","name":"a76c","type":"P","href":null,"layout":null,"metadata":null,"text":"Policy (π): The policy, denoted as π (or sometimes π(a|s)), is a mapping from some state s to the probabilities of selecting each possible action given that state. For example, a greedy policy outputs for every state the action with the highest expected Q-Value.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_44.markups.10","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_44.markups.0":{"type":"A","start":83,"end":88,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.1":{"type":"A","start":139,"end":145,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.2":{"type":"A","start":179,"end":192,"href":"#15c5","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.3":{"type":"A","start":254,"end":261,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.4":{"type":"STRONG","start":0,"end":11,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.5":{"type":"EM","start":35,"end":36,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.6":{"type":"EM","start":51,"end":57,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.7":{"type":"EM","start":83,"end":90,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.8":{"type":"EM","start":139,"end":146,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.9":{"type":"EM","start":179,"end":192,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_44.markups.10":{"type":"EM","start":254,"end":261,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45":{"id":"154a1f387c6d_45","name":"9d8f","type":"P","href":null,"layout":null,"metadata":null,"text":"Q-Learning: Q-Learning is an off-policy Reinforcement Learning algorithm, considered as one of the very basic ones. In its most simplified form, it uses a table to store all Q-Values of all possible state-action pairs possible. It updates this table using the Bellman equation, while action selection is usually made with an ε-greedy policy. \nIn its simplest form (no uncertainties in state-transitions and expected rewards), the update rule of Q-Learning is:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_45.markups.16","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_45.markups.0":{"type":"A","start":29,"end":39,"href":"#44d1","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.1":{"type":"A","start":40,"end":62,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.2":{"type":"A","start":174,"end":182,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.3":{"type":"A","start":199,"end":204,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.4":{"type":"A","start":205,"end":211,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.5":{"type":"A","start":260,"end":276,"href":"#fbd3","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.6":{"type":"A","start":325,"end":340,"href":"#15c5","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.7":{"type":"A","start":385,"end":390,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.8":{"type":"A","start":416,"end":423,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.9":{"type":"STRONG","start":0,"end":12,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.10":{"type":"EM","start":29,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.11":{"type":"EM","start":174,"end":182,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.12":{"type":"EM","start":199,"end":211,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.13":{"type":"EM","start":260,"end":276,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.14":{"type":"EM","start":325,"end":343,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.15":{"type":"EM","start":385,"end":390,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_45.markups.16":{"type":"EM","start":416,"end":423,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_46":{"id":"154a1f387c6d_46","name":"d4a7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*l8ZP4tTFqDGyezwJ8jR8eA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*l8ZP4tTFqDGyezwJ8jR8eA.png":{"id":"1*l8ZP4tTFqDGyezwJ8jR8eA.png","originalHeight":128,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_47":{"id":"154a1f387c6d_47","name":"a9f2","type":"P","href":null,"layout":null,"metadata":null,"text":"A more complex version of it, though far more popular, is the Deep Q-Network variant (which is sometimes even referred to simply as Deep Q-Learning or just Q-Learning). This variant replaces the state-action table with a neural network in order to cope with large-scale tasks, where the number of possible state-action pairs can be enormous. You can find a tutorial for this algorithm in this blogpost.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_47.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_47.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_47.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_47.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_47.markups.0":{"type":"A","start":388,"end":401,"href":"https:\u002F\u002Fmedium.com\u002F@shakedzy\u002Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_47.markups.1":{"type":"EM","start":62,"end":76,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_47.markups.2":{"type":"EM","start":132,"end":148,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_47.markups.3":{"type":"EM","start":156,"end":166,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48":{"id":"154a1f387c6d_48","name":"f366","type":"P","href":null,"layout":null,"metadata":null,"text":"Q Value (Q Function): Usually denoted as Q(s,a) (sometimes with a π subscript, and sometimes as Q(s,a; θ) in Deep RL), Q Value is a measure of the overall expected reward assuming the Agent is in state s and performs action a, and then continues playing until the end of the episode following some policy π. Its name is an abbreviation of the word “Quality”, and it is defined mathematically as:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.16","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.17","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_48.markups.18","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_48.markups.0":{"type":"A","start":109,"end":116,"href":"#aa91","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.1":{"type":"A","start":164,"end":170,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.2":{"type":"A","start":184,"end":189,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.3":{"type":"A","start":196,"end":201,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.4":{"type":"A","start":217,"end":223,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.5":{"type":"A","start":275,"end":282,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.6":{"type":"A","start":298,"end":304,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.7":{"type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.8":{"type":"EM","start":41,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.9":{"type":"EM","start":96,"end":105,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.10":{"type":"EM","start":109,"end":116,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.11":{"type":"EM","start":164,"end":170,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.12":{"type":"EM","start":184,"end":189,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.13":{"type":"EM","start":196,"end":201,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.14":{"type":"EM","start":202,"end":203,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.15":{"type":"EM","start":217,"end":223,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.16":{"type":"EM","start":224,"end":225,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.17":{"type":"EM","start":275,"end":283,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_48.markups.18":{"type":"EM","start":298,"end":304,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_49":{"id":"154a1f387c6d_49","name":"6c81","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*OQqePNtxQV177JeQR1O16g.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*OQqePNtxQV177JeQR1O16g.png":{"id":"1*OQqePNtxQV177JeQR1O16g.png","originalHeight":100,"originalWidth":577,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_50":{"id":"154a1f387c6d_50","name":"c8d9","type":"P","href":null,"layout":null,"metadata":null,"text":"where N is the number of states from state s till the terminal state, γ is the discount factor and r⁰ is the immediate reward received after performing action a in state s.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_50.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_50.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_50.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_50.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_50.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_50.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_50.markups.0":{"type":"A","start":79,"end":94,"href":"#4ee6","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_50.markups.1":{"type":"EM","start":6,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_50.markups.2":{"type":"EM","start":43,"end":44,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_50.markups.3":{"type":"EM","start":79,"end":94,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_50.markups.4":{"type":"EM","start":159,"end":160,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_50.markups.5":{"type":"EM","start":170,"end":171,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51":{"id":"154a1f387c6d_51","name":"41f4","type":"P","href":null,"layout":null,"metadata":null,"text":"REINFORCE Algorithms: REINFORCE algorithms are a family of Reinforcement Learning algorithms which update their policy parameters according to the gradient of the policy with respect to the policy-parameters [paper]. The name is typically written using capital letters only, as it’s originally an acronym for the original algorithms group design: “REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility” [source]","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.12","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.13","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.14","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.15","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.16","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.17","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_51.markups.18","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_51.markups.0":{"type":"A","start":59,"end":81,"href":"#fc9f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.1":{"type":"A","start":112,"end":118,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.2":{"type":"A","start":209,"end":214,"href":"https:\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.3":{"type":"A","start":440,"end":446,"href":"https:\u002F\u002Flink.springer.com\u002Fcontent\u002Fpdf\u002F10.1007%2FBF00992696.pdf","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.4":{"type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.5":{"type":"STRONG","start":92,"end":93,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.6":{"type":"STRONG","start":348,"end":350,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.7":{"type":"STRONG","start":355,"end":356,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.8":{"type":"STRONG","start":367,"end":368,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.9":{"type":"STRONG","start":379,"end":380,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.10":{"type":"STRONG","start":388,"end":389,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.11":{"type":"STRONG","start":395,"end":396,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.12":{"type":"STRONG","start":411,"end":412,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.13":{"type":"STRONG","start":426,"end":427,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.14":{"type":"EM","start":59,"end":82,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.15":{"type":"EM","start":112,"end":119,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.16":{"type":"EM","start":129,"end":130,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.17":{"type":"EM","start":386,"end":387,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_51.markups.18":{"type":"EM","start":409,"end":411,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52":{"id":"154a1f387c6d_52","name":"fc9f","type":"P","href":null,"layout":null,"metadata":null,"text":"Reinforcement Learning (RL): Reinforcement Learning is, like Supervised Learning and Unsupervised Learning, one the main areas of Machine Learning and Artificial Intelligence. It is concerned with the learning process of an arbitrary being, formally known as an Agent, in the world surrounding it, known as the Environment. The Agent seeks to maximize the rewards it receives from the Environment, and performs different actions in order to learn how the Environment responds and gain more rewards. One of the greatest challenges of RL tasks is to associate actions with postponed rewards — which are rewards received by the Agent long after the reward-generating action was made. It is therefore heavily used to solve different kind of games, from Tic-Tac-Toe, Chess, Atari 2600 and all the way to Go and StarCraft.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_52.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_52.markups.0":{"type":"A","start":262,"end":267,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.1":{"type":"A","start":311,"end":322,"href":"#4311","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.2":{"type":"A","start":356,"end":363,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.3":{"type":"A","start":421,"end":428,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.4":{"type":"STRONG","start":0,"end":28,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.5":{"type":"EM","start":262,"end":267,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.6":{"type":"EM","start":311,"end":322,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.7":{"type":"EM","start":356,"end":363,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_52.markups.8":{"type":"EM","start":421,"end":428,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53":{"id":"154a1f387c6d_53","name":"6a6f","type":"P","href":null,"layout":null,"metadata":null,"text":"Reward: A numerical value received by the Agent from the Environment as a direct response to the Agent’s actions. The Agent’s goal is to maximize the overall reward it receives during an episode, and so rewards are the motivation the Agent needs in order to act in a desired behavior. All actions yield rewards, which can be roughly divided to three types: positive rewards which emphasize a desired action, negative rewards which emphasize an action the Agent should stray away from, and zero, which means the Agent didn’t do anything special or unique.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_53.markups.11","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_53.markups.0":{"type":"A","start":42,"end":47,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.1":{"type":"A","start":57,"end":68,"href":"#4311","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.2":{"type":"A","start":105,"end":112,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.3":{"type":"A","start":187,"end":194,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.4":{"type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.5":{"type":"EM","start":42,"end":47,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.6":{"type":"EM","start":57,"end":68,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.7":{"type":"EM","start":105,"end":112,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.8":{"type":"EM","start":187,"end":194,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.9":{"type":"EM","start":357,"end":373,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.10":{"type":"EM","start":408,"end":424,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_53.markups.11":{"type":"EM","start":489,"end":493,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54":{"id":"154a1f387c6d_54","name":"da43","type":"P","href":null,"layout":null,"metadata":null,"text":"Sarsa: The Sarsa algorithm is pretty much the Q-Learning algorithm with a slight modification in order to make it an on-policy algorithm. The Q-Learning update rule is based on the Bellman equation for the optimal Q-Value, and so in the case on no uncertainties in state-transitions and expected rewards, the Q-Learning update rule is:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_54.markups.12","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_54.markups.0":{"type":"A","start":46,"end":56,"href":"#9d8f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.1":{"type":"A","start":117,"end":126,"href":"#44d1","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.2":{"type":"A","start":181,"end":197,"href":"#fbd3","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.3":{"type":"A","start":214,"end":221,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.4":{"type":"A","start":265,"end":270,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.5":{"type":"A","start":296,"end":303,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.6":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.7":{"type":"EM","start":46,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.8":{"type":"EM","start":117,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.9":{"type":"EM","start":181,"end":197,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.10":{"type":"EM","start":214,"end":221,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.11":{"type":"EM","start":265,"end":270,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_54.markups.12":{"type":"EM","start":296,"end":303,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_55":{"id":"154a1f387c6d_55","name":"a237","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*l8ZP4tTFqDGyezwJ8jR8eA.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_56":{"id":"154a1f387c6d_56","name":"b7e4","type":"P","href":null,"layout":null,"metadata":null,"text":"In order to transform this into an on-policy algorithm, the last term is modified:","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_57":{"id":"154a1f387c6d_57","name":"2ca7","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*hdnBhX5TYGLSj7xGAwe6Xw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*hdnBhX5TYGLSj7xGAwe6Xw.png":{"id":"1*hdnBhX5TYGLSj7xGAwe6Xw.png","originalHeight":124,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_58":{"id":"154a1f387c6d_58","name":"90a9","type":"P","href":null,"layout":null,"metadata":null,"text":"when here, both actions a and a’ are chosen by the same policy. The name of the algorithm is derived from its update rule, which is based on (s,a,r,s’,a’), all coming from the same policy.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_58.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_58.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_58.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_58.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_58.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_58.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_58.markups.6","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_58.markups.0":{"type":"A","start":16,"end":23,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_58.markups.1":{"type":"A","start":56,"end":62,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_58.markups.2":{"type":"EM","start":16,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_58.markups.3":{"type":"EM","start":24,"end":25,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_58.markups.4":{"type":"EM","start":30,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_58.markups.5":{"type":"EM","start":56,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_58.markups.6":{"type":"EM","start":142,"end":153,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59":{"id":"154a1f387c6d_59","name":"c274","type":"P","href":null,"layout":null,"metadata":null,"text":"State: Every scenario the Agent encounters in the Environment is formally called a state. The Agent transitions between different states by performing actions. It is also worth mentioning the terminal states, which mark the end of an episode. There are no possible states after a terminal state has been reached, and a new episode begins. Quite often, a terminal state is represented as a special state where all actions transition to the same terminal state with reward 0.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_59.markups.12","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_59.markups.0":{"type":"A","start":26,"end":31,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.1":{"type":"A","start":50,"end":61,"href":"#4311","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.2":{"type":"A","start":151,"end":158,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.3":{"type":"A","start":234,"end":241,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.4":{"type":"A","start":464,"end":470,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.5":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.6":{"type":"EM","start":26,"end":31,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.7":{"type":"EM","start":50,"end":61,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.8":{"type":"EM","start":83,"end":90,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.9":{"type":"EM","start":151,"end":158,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.10":{"type":"EM","start":192,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.11":{"type":"EM","start":234,"end":242,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_59.markups.12":{"type":"EM","start":464,"end":470,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_60":{"id":"154a1f387c6d_60","name":"40ff","type":"P","href":null,"layout":null,"metadata":null,"text":"State-Value Function: See Value Function.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_60.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_60.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_60.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_60.markups.0":{"type":"A","start":26,"end":40,"href":"#680c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_60.markups.1":{"type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_60.markups.2":{"type":"EM","start":26,"end":41,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61":{"id":"154a1f387c6d_61","name":"d0d7","type":"P","href":null,"layout":null,"metadata":null,"text":"Temporal-Difference (TD): Temporal Difference is a learning method which combines both Dynamic Programming and Monte Carlo principles; it learns “on the fly” similarly to Monte Carlo, yet updates its estimates like Dynamic Programming. One of the simplest Temporal Difference algorithms it known as one-step TD or TD(0). It updates the Value Function according to the following update rule:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_61.markups.7","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_61.markups.0":{"type":"A","start":87,"end":106,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDynamic_programming","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61.markups.1":{"type":"A","start":111,"end":122,"href":"#f9e0","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61.markups.2":{"type":"A","start":336,"end":350,"href":"#680c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61.markups.3":{"type":"STRONG","start":0,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61.markups.4":{"type":"EM","start":111,"end":122,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61.markups.5":{"type":"EM","start":299,"end":310,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61.markups.6":{"type":"EM","start":314,"end":319,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_61.markups.7":{"type":"EM","start":336,"end":350,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_62":{"id":"154a1f387c6d_62","name":"ec6f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*jLeVw2pIpOfknG5V8ySOhQ.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*jLeVw2pIpOfknG5V8ySOhQ.png":{"id":"1*jLeVw2pIpOfknG5V8ySOhQ.png","originalHeight":112,"originalWidth":1154,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_63":{"id":"154a1f387c6d_63","name":"cd60","type":"P","href":null,"layout":null,"metadata":null,"text":"where V is the Value Function, s is the state, r is the reward, γ is the discount factor, α is a learning rate, t is the time-step and the ‘=’ sign is used as an update operator and not equality. The term found in the squared brackets is known as the temporal difference error.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_63.markups.12","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_63.markups.0":{"type":"A","start":40,"end":45,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.1":{"type":"A","start":56,"end":62,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.2":{"type":"A","start":73,"end":88,"href":"#4ee6","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.3":{"type":"EM","start":6,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.4":{"type":"EM","start":31,"end":32,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.5":{"type":"EM","start":40,"end":45,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.6":{"type":"EM","start":47,"end":48,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.7":{"type":"EM","start":56,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.8":{"type":"EM","start":64,"end":65,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.9":{"type":"EM","start":73,"end":88,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.10":{"type":"EM","start":90,"end":92,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.11":{"type":"EM","start":112,"end":113,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_63.markups.12":{"type":"EM","start":251,"end":277,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_64":{"id":"154a1f387c6d_64","name":"4371","type":"P","href":null,"layout":null,"metadata":null,"text":"Terminal State: See State.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_64.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_64.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_64.markups.2","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_64.markups.0":{"type":"A","start":20,"end":25,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_64.markups.1":{"type":"STRONG","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_64.markups.2":{"type":"EM","start":20,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65":{"id":"154a1f387c6d_65","name":"930e","type":"P","href":null,"layout":null,"metadata":null,"text":"Upper Confident Bound (UCB): UCB is an exploration method which tries to ensure that each action is well explored. Consider an exploration policy which is completely random — meaning, each possible action has the same chance of being selected. There is a chance that some actions will be explored much more than others. The less an action is selected, the less confident the Agent can be about its expected reward, and the its exploitation phase might be harmed. Exploration by UCB takes into account the number of times each action was selected, and gives extra weight to those less-explored. Formalizing this mathematically, the selected action is picked by:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_65.markups.12","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_65.markups.0":{"type":"A","start":39,"end":50,"href":"#b903","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.1":{"type":"A","start":90,"end":96,"href":"#8751","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.2":{"type":"A","start":139,"end":145,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.3":{"type":"A","start":375,"end":380,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.4":{"type":"A","start":407,"end":413,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.5":{"type":"A","start":427,"end":439,"href":"#b903","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.6":{"type":"STRONG","start":0,"end":29,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.7":{"type":"EM","start":39,"end":50,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.8":{"type":"EM","start":90,"end":96,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.9":{"type":"EM","start":139,"end":146,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.10":{"type":"EM","start":375,"end":380,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.11":{"type":"EM","start":407,"end":413,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_65.markups.12":{"type":"EM","start":427,"end":439,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_66":{"id":"154a1f387c6d_66","name":"d9f8","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*5haNfed96VexQNtBOw2tYg.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*5haNfed96VexQNtBOw2tYg.png":{"id":"1*5haNfed96VexQNtBOw2tYg.png","originalHeight":134,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_67":{"id":"154a1f387c6d_67","name":"8f30","type":"P","href":null,"layout":null,"metadata":null,"text":"where R(a) is the expected overall reward of action a, t is the number of steps taken (how many actions were selected overall), N(a) is the number of times action a was selected and c is a configureable hyperparameter. This method is also referred to sometimes as “exploration through optimism”, as it gives less-explored actions a higher value, encouraging the model to select them.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_67.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_67.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_67.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_67.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_67.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_67.markups.5","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_67.markups.0":{"type":"EM","start":6,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_67.markups.1":{"type":"EM","start":52,"end":53,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_67.markups.2":{"type":"EM","start":55,"end":56,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_67.markups.3":{"type":"EM","start":128,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_67.markups.4":{"type":"EM","start":163,"end":164,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_67.markups.5":{"type":"EM","start":182,"end":183,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68":{"id":"154a1f387c6d_68","name":"680c","type":"P","href":null,"layout":null,"metadata":null,"text":"Value Function: Usually denoted as V(s) (sometimes with a π subscript), the Value function is a measure of the overall expected reward assuming the Agent is in state s and then continues playing until the end of the episode following some policy π. It is defined mathematically as:","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.8","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.9","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.10","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.11","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_68.markups.12","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_68.markups.0":{"type":"A","start":128,"end":134,"href":"#6a6f","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.1":{"type":"A","start":148,"end":153,"href":"#b6a2","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.2":{"type":"A","start":160,"end":165,"href":"#c274","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.3":{"type":"A","start":216,"end":223,"href":"#601d","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.4":{"type":"A","start":239,"end":245,"href":"#a76c","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.5":{"type":"STRONG","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.6":{"type":"EM","start":35,"end":40,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.7":{"type":"EM","start":128,"end":134,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.8":{"type":"EM","start":148,"end":153,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.9":{"type":"EM","start":160,"end":165,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.10":{"type":"EM","start":166,"end":167,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.11":{"type":"EM","start":216,"end":224,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_68.markups.12":{"type":"EM","start":239,"end":245,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_69":{"id":"154a1f387c6d_69","name":"9b9f","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*UOfJo9BjoQfNcC2x0hV_Zw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*UOfJo9BjoQfNcC2x0hV_Zw.png":{"id":"1*UOfJo9BjoQfNcC2x0hV_Zw.png","originalHeight":100,"originalWidth":577,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:154a1f387c6d_70":{"id":"154a1f387c6d_70","name":"8354","type":"P","href":null,"layout":null,"metadata":null,"text":"While it does seem similar to the definition of Q Value, there is an implicit — yet important — difference: for n=0, the reward r⁰ of V(s) is the expected reward from just being in state s, before any action was played, while in Q Value, r⁰ is the expected reward after a certain action was played. This difference also yields the Advantage function.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.3","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.4","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.5","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.6","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.7","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:154a1f387c6d_70.markups.8","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:154a1f387c6d_70.markups.0":{"type":"A","start":48,"end":55,"href":"#f366","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.1":{"type":"A","start":331,"end":349,"href":"#efa6","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.2":{"type":"EM","start":48,"end":55,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.3":{"type":"EM","start":112,"end":115,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.4":{"type":"EM","start":134,"end":138,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.5":{"type":"EM","start":187,"end":188,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.6":{"type":"EM","start":190,"end":196,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.7":{"type":"EM","start":264,"end":269,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:154a1f387c6d_70.markups.8":{"type":"EM","start":331,"end":349,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:reinforcement-learning":{"id":"reinforcement-learning","displayTitle":"Reinforcement Learning","__typename":"Tag"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","displayTitle":"Artificial Intelligence","__typename":"Tag"},"Tag:data-science":{"id":"data-science","displayTitle":"Data Science","__typename":"Tag"},"Tag:algorithms":{"id":"algorithms","displayTitle":"Algorithms","__typename":"Tag"},"1af65db9c2f8":{"topicId":"1af65db9c2f8","name":"Artificial Intelligence","__typename":"Topic","slug":"artificial-intelligence"},"1eca0103fff3":{"topicId":"1eca0103fff3","name":"Machine Learning","__typename":"Topic","slug":"machine-learning"},"ae5d4995e225":{"topicId":"ae5d4995e225","name":"Data Science","__typename":"Topic","slug":"data-science"},"$Post:e16230b7d24e.postResponses":{"count":7,"__typename":"PostResponses","responsesConnection({\"paging\":{\"limit\":10}})":{"type":"id","generated":true,"id":"$Post:e16230b7d24e.postResponses.responsesConnection({\"paging\":{\"limit\":10}})","typename":"StreamConnection"}},"$Post:e16230b7d24e.previewContent":{"subtitle":"The Reinforcement Learning Terminology, A to Z","__typename":"PreviewContent"},"$Post:e16230b7d24e.postResponses.responsesConnection({\"paging\":{\"limit\":10}})":{"pagingInfo":null,"stream":[],"__typename":"StreamConnection"},"Post:e16230b7d24e.readNext.0":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:c9cbbf4472f3","typename":"Post"},"__typename":"ReadNextItem"},"Post:c9cbbf4472f3":{"id":"c9cbbf4472f3","title":"Machine Learning Engineers Will Not Exist In 10 Years.","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning-engineers-will-not-exist-in-10-years-c9cbbf4472f3","primaryTopic":{"type":"id","generated":false,"id":"machine-learning","typename":"Topic"},"collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:0*2hfu1X4qX7n9tW7v","typename":"ImageMetadata"},"__typename":"Post"},"ImageMetadata:0*2hfu1X4qX7n9tW7v":{"id":"0*2hfu1X4qX7n9tW7v","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.1":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:6a8ce3f0a0e0","typename":"Post"},"__typename":"ReadNextItem"},"Post:6a8ce3f0a0e0":{"id":"6a8ce3f0a0e0","title":"Prepare for the Ultimate Gaslighting*","mediumUrl":"https:\u002F\u002Fforge.medium.com\u002Fprepare-for-the-ultimate-gaslighting-6a8ce3f0a0e0","primaryTopic":{"type":"id","generated":false,"id":"society","typename":"Topic"},"collection":{"type":"id","generated":false,"id":"Collection:3f6ecf56618","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*Ry6QAUccmwPfLLmf9n3qxA.jpeg","typename":"ImageMetadata"},"__typename":"Post"},"society":{"name":"Society","slug":"society","__typename":"Topic"},"Collection:3f6ecf56618":{"id":"3f6ecf56618","name":"Forge","__typename":"Collection"},"ImageMetadata:1*Ry6QAUccmwPfLLmf9n3qxA.jpeg":{"id":"1*Ry6QAUccmwPfLLmf9n3qxA.jpeg","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.2":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:a5df19c77d08","typename":"Post"},"__typename":"ReadNextItem"},"Post:a5df19c77d08":{"id":"a5df19c77d08","title":"Belgian-Dutch Study: Why in times of COVID-19 you should not walk\u002Frun\u002Fbike close behind each other.","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@jurgenthoelen\u002Fbelgian-dutch-study-why-in-times-of-covid-19-you-can-not-walk-run-bike-close-to-each-other-a5df19c77d08","primaryTopic":{"type":"id","generated":false,"id":"coronavirus","typename":"Topic"},"collection":null,"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*xR4JuvsEr8WFfBJM1qjyvg.jpeg","typename":"ImageMetadata"},"__typename":"Post"},"coronavirus":{"name":"Coronavirus","slug":"coronavirus","__typename":"Topic"},"ImageMetadata:1*xR4JuvsEr8WFfBJM1qjyvg.jpeg":{"id":"1*xR4JuvsEr8WFfBJM1qjyvg.jpeg","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.3":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:e25681c598dc","typename":"Post"},"__typename":"ReadNextItem"},"Post:e25681c598dc":{"id":"e25681c598dc","title":"A Home For Results in ML","mediumUrl":"https:\u002F\u002Fmedium.com\u002Fpaperswithcode\u002Fa-home-for-results-in-ml-e25681c598dc","primaryTopic":{"type":"id","generated":false,"id":"machine-learning","typename":"Topic"},"collection":{"type":"id","generated":false,"id":"Collection:e16b712f0ece","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*fRt5m-OV7_M_OZMTEgtKJw.png","typename":"ImageMetadata"},"__typename":"Post"},"Collection:e16b712f0ece":{"id":"e16b712f0ece","name":"PapersWithCode","__typename":"Collection"},"ImageMetadata:1*fRt5m-OV7_M_OZMTEgtKJw.png":{"id":"1*fRt5m-OV7_M_OZMTEgtKJw.png","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.4":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:3d90808b9598","typename":"Post"},"__typename":"ReadNextItem"},"Post:3d90808b9598":{"id":"3d90808b9598","title":"RIP correlation. Introducing the Predictive Power Score","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Frip-correlation-introducing-the-predictive-power-score-3d90808b9598","primaryTopic":{"type":"id","generated":false,"id":"data-science","typename":"Topic"},"collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*XOC9DQ6rJMdNE4-_fUo6kw.png","typename":"ImageMetadata"},"__typename":"Post"},"data-science":{"name":"Data Science","slug":"data-science","__typename":"Topic"},"ImageMetadata:1*XOC9DQ6rJMdNE4-_fUo6kw.png":{"id":"1*XOC9DQ6rJMdNE4-_fUo6kw.png","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.5":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:ee4769899025","typename":"Post"},"__typename":"ReadNextItem"},"Post:ee4769899025":{"id":"ee4769899025","title":"Don’t Become a Data Scientist","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fdont-become-a-data-scientist-ee4769899025","primaryTopic":{"type":"id","generated":false,"id":"data-science","typename":"Topic"},"collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*OyqZHUrw_aybBSkw6KZwPQ.jpeg","typename":"ImageMetadata"},"__typename":"Post"},"ImageMetadata:1*OyqZHUrw_aybBSkw6KZwPQ.jpeg":{"id":"1*OyqZHUrw_aybBSkw6KZwPQ.jpeg","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.6":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:88c748a4cd25","typename":"Post"},"__typename":"ReadNextItem"},"Post:88c748a4cd25":{"id":"88c748a4cd25","title":"What’s happened to the data science job market in the past month","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fwhats-happened-to-the-data-science-job-market-in-the-past-month-88c748a4cd25","primaryTopic":null,"collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*UmYW2QVc8JHrDHk0NkGqrg.png","typename":"ImageMetadata"},"__typename":"Post"},"ImageMetadata:1*UmYW2QVc8JHrDHk0NkGqrg.png":{"id":"1*UmYW2QVc8JHrDHk0NkGqrg.png","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.7":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:66d13a8ab1f1","typename":"Post"},"__typename":"ReadNextItem"},"Post:66d13a8ab1f1":{"id":"66d13a8ab1f1","title":"Probably the Best Resource to Learn Deep Learning in 2020","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fprobably-the-best-resource-to-learn-deep-learning-in-2020-66d13a8ab1f1","primaryTopic":{"type":"id","generated":false,"id":"artificial-intelligence","typename":"Topic"},"collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:0*nMJDXq-BWkTvI11E","typename":"ImageMetadata"},"__typename":"Post"},"artificial-intelligence":{"name":"Artificial Intelligence","slug":"artificial-intelligence","__typename":"Topic"},"ImageMetadata:0*nMJDXq-BWkTvI11E":{"id":"0*nMJDXq-BWkTvI11E","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.8":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:9230bff0df62","typename":"Post"},"__typename":"ReadNextItem"},"Post:9230bff0df62":{"id":"9230bff0df62","title":"Bye-bye Python. Hello Julia!","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fbye-bye-python-hello-julia-9230bff0df62","primaryTopic":{"type":"id","generated":false,"id":"programming","typename":"Topic"},"collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*jJCYqdN8a0xJ3iYa9PFT-g.jpeg","typename":"ImageMetadata"},"__typename":"Post"},"programming":{"name":"Programming","slug":"programming","__typename":"Topic"},"ImageMetadata:1*jJCYqdN8a0xJ3iYa9PFT-g.jpeg":{"id":"1*jJCYqdN8a0xJ3iYa9PFT-g.jpeg","alt":"Woman with hat covering her face in front of sunset","focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"},"Post:e16230b7d24e.readNext.9":{"reason":73,"post":{"type":"id","generated":false,"id":"Post:f4d3d9cd99ca","typename":"Post"},"__typename":"ReadNextItem"},"Post:f4d3d9cd99ca":{"id":"f4d3d9cd99ca","title":"Coronavirus: Why You Must Act Now","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@tomaspueyo\u002Fcoronavirus-act-today-or-people-will-die-f4d3d9cd99ca","primaryTopic":{"type":"id","generated":false,"id":"coronavirus","typename":"Topic"},"collection":null,"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*4kOJv8hmd5VFPcBL1mywsw.png","typename":"ImageMetadata"},"__typename":"Post"},"ImageMetadata:1*4kOJv8hmd5VFPcBL1mywsw.png":{"id":"1*4kOJv8hmd5VFPcBL1mywsw.png","alt":null,"focusPercentX":null,"focusPercentY":null,"__typename":"ImageMetadata"}}</script><script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/manifest.de06aeef.js"></script><script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/vendors_main.09ac4b3e.chunk.js"></script><script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/main.45f3040a.chunk.js"></script><script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/vendors_instrumentation.d7114bfc.chunk.js"></script>
<script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/instrumentation.d66f82e4.chunk.js"></script>
<script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/reporting.49324504.chunk.js"></script>
<script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/vendors_AMPPost_CollectionHomepage_CollectionHomepagePreview_CollectionNewShortformEditor_Collection_37c9fa1e.77e6fe9c.chunk.js"></script>
<script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/vendors_AMPPost_DebugCachedPost_Post_SequencePost_Series.0bf77567.chunk.js"></script>
<script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/AMPPost_CollectionHomepage_CollectionHomepagePreview_CollectionNewShortformEditor_CollectionPostShor_3fa3f642.74a270cb.chunk.js"></script>
<script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/AMPPost_CollectionHomepage_CollectionHomepagePreview_DebugCachedPost_PackageBuilder_Post_PostSetting_8e568ca5.fbf0aa50.chunk.js"></script>
<script src="./The Complete Reinforcement Learning Dictionary - Towards Data Science_files/Post.a0c9fb4e.chunk.js"></script><script>window.main();</script></body></html>